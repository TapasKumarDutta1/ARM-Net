{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FANet",
      "provenance": [],
      "authorship_tag": "ABX9TyOzczP7d/mP0TyPdIE9k0XU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluefeather88/FANet/blob/main/FANet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zucfq7hdVeUg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import mixed_precision\n",
        "import zipfile\n",
        "import datetime, os\n",
        "import h5py\n",
        "from tensorflow.keras.optimizers import *\n",
        "import cv2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import glob, os\n",
        "from matplotlib import pyplot as plt\n",
        "import h5py\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import gc\n",
        "from tensorflow.keras.applications import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "5-HpxhnaVhRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = F\"/content/gdrive/My Drive/check.npy\" \n",
        "df=np.load(path,allow_pickle=True)\n",
        "df=df.item()"
      ],
      "metadata": {
        "id": "ljm1yORtVhPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "\n",
        "\n",
        "#change targets\n",
        "def change(img):\n",
        "    resized = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA )\n",
        "    return resized\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#get train and test splits\n",
        "def get_trn_tst(df,tst_fold):\n",
        "  idx=np.asarray(df['fold'])\n",
        "  y=np.asarray(df['label'])\n",
        "  y-=1\n",
        "  img=np.asarray(df['image'])\n",
        "  img1=[]\n",
        "  for i in range(len(img)):\n",
        "        img1.append(change(img[i]))\n",
        "  img1=np.asarray(img1)\n",
        "  del([img])\n",
        "  gc.collect()\n",
        "  trn_y=np.asarray(y[(idx!=tst_fold)])\n",
        "  trn_img=np.asarray(img1[(idx!=tst_fold)])\n",
        "  tst_y=np.asarray(y[(idx==tst_fold)])\n",
        "  tst_img=img1[idx==tst_fold]\n",
        "  trn_img=np.repeat(trn_img.reshape((trn_img.shape[0],224,224,1)),3,axis=3)\n",
        "  tst_img=np.repeat(tst_img.reshape((tst_img.shape[0],224,224,1)),3,axis=3)\n",
        "  return (trn_img.copy(),trn_y.copy()),(tst_img.copy(),tst_y.copy())\n",
        "import zipfile\n",
        "import h5py\n",
        "from tensorflow.keras.optimizers import *\n",
        "import cv2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import glob, os\n",
        "from matplotlib import pyplot as plt\n",
        "import h5py\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import gc\n",
        "from tensorflow.keras.applications import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow import keras\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "class LayerNormalization(Layer):\n",
        "    def __init__(self, eps=1e-6, **kwargs):\n",
        "        self.eps = eps\n",
        "        super(LayerNormalization, self).__init__(**kwargs)\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
        "                                     initializer=Ones(), trainable=True)\n",
        "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
        "                                    initializer=Zeros(), trainable=True)\n",
        "        super(LayerNormalization, self).build(input_shape)\n",
        "    def call(self, x):\n",
        "        mean = K.mean(x, axis=-1, keepdims=True)\n",
        "        std = K.std(x, axis=-1, keepdims=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "class abc(Layer):\n",
        "    def __init__(self,inr,size,mo,up,org,**kwargs):\n",
        "        super(abc, self).__init__(**kwargs)\n",
        "        self.inr=inr\n",
        "        self.mo=mo\n",
        "        self.up=up\n",
        "        self.org=org\n",
        "        self.size=size\n",
        "    def get_config(self):\n",
        "        base_config = super(abc, self).get_config()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(abc, self).build(input_shape)\n",
        "        self.cv1 = Conv2D(self.inr,1)\n",
        "        self.cv2 = Conv2D(self.inr,1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.dns1 = Conv2D(self.org,1,activation='relu')\n",
        "        self.dns2 = Conv2D(self.org,1,activation='sigmoid')\n",
        "        \n",
        "        \n",
        "        self.cv3 = Conv2D(1,1)\n",
        "        self.up = UpSampling2D(interpolation='bilinear',size=(self.up,self.up))\n",
        "        self.dns1=Dense(1)\n",
        "    def call(self, img,y):\n",
        "        y = self.cv1(y)\n",
        "        x = self.cv2(img)\n",
        "        y = self.up(y)\n",
        "        \n",
        "        y = Add()([y,x])\n",
        "        y=GlobalAveragePooling2D()(y)\n",
        "        y = Reshape((1,1,self.inr))(y)\n",
        "        x = self.dns1(y)\n",
        "        x = self.dns2(x)\n",
        "        z = tf.math.multiply(img,x)\n",
        "        \n",
        "        x = ReLU()(z)\n",
        "        x = K.max(x,axis=-1)\n",
        "        x = Reshape((self.size,self.size,1))(x)\n",
        "        \n",
        "        map = softmax(x,axis=[2,3])\n",
        "\n",
        "\n",
        "        return tf.math.multiply(z,map)\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.applications import *\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.applications import *\n",
        "def load_model():   \n",
        "  \n",
        "  K.clear_session() \n",
        "  mod=densenet.DenseNet121(include_top=True, weights='imagenet')\n",
        "  d = mod.get_layer('conv5_block16_concat').output\n",
        "  d = Conv2D(512,1)(d)\n",
        "\n",
        "  a = mod.get_layer('conv3_block12_concat').output\n",
        "  a = Conv2D(256,1)(a)\n",
        "  a = abc(inr=16,mo=256,up=4,size=28,org=256)(a,d)\n",
        "\n",
        "  b = mod.get_layer('conv4_block24_concat').output\n",
        "  b = Conv2D(512,1)(b)\n",
        "  b = abc(inr=16,mo=256,up=2,size=14,org=512)(b,d)\n",
        "\n",
        "  b = LayerNormalization()(b)\n",
        "  b = Reshape((14,14,512))(b)\n",
        "  b = UpSampling2D(interpolation='bilinear',size=(2,2))(b)\n",
        "  d = UpSampling2D(interpolation='bilinear',size=(4,4))(d)\n",
        "    \n",
        "  conc=Concatenate(axis=-1)([a,b,d])\n",
        "  conc = GlobalMaxPooling2D()(conc)\n",
        "\n",
        "  conc = Dense(3, activation=\"softmax\")(conc) \n",
        "  \n",
        "  mod=Model(inputs=mod.input,outputs=conc)\n",
        "  return mod"
      ],
      "metadata": {
        "id": "-Y_iC1x4VhMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import time\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "\n",
        "\n",
        "#change targets\n",
        "def change(img):\n",
        "    resized = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA )\n",
        "    return resized\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "def rotate_image(image, angle):\n",
        "  image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "  result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
        "  return result\n",
        "def Hflip( images):\n",
        "\t\tseq = iaa.Sequential([iaa.Fliplr(1.0)])\n",
        "\t\treturn seq.augment_images(images)\n",
        "def Vflip( images):\n",
        "\t\tseq = iaa.Sequential([iaa.Flipud(1.0)])\n",
        "\t\treturn seq.augment_images(images)\n",
        "def noise(images):\n",
        "    ls=[]\n",
        "    for i in images:\n",
        "        x = np.random.normal(loc=0, scale=0.05, size=(299,299,3))\n",
        "        ls.append(i+x)\n",
        "    return ls\n",
        "def rotate(images):\n",
        "    ls=[]\n",
        "    for angle in range(-15,20,5):\n",
        "        for image in images:\n",
        "            ls.append(rotate_image(image,angle))\n",
        "    return ls\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "  def __init__(self, images, labels, batch_size=64, image_dimensions = (96 ,96 ,3), shuffle=False, augment=False):\n",
        "    self.labels       = labels              # array of labels\n",
        "    self.images = images        # array of image paths\n",
        "    self.batch_size   = batch_size          # batch size\n",
        "    self.on_epoch_end()\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.floor(self.labels.shape[0] / self.batch_size))\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.indexes = np.arange(self.labels.shape[0])\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\t\t# selects indices of data for next batch\n",
        "    indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "    # select data and load images\n",
        "    labels = self.labels.loc[indexes]\n",
        "    img = [self.images[k].astype(np.float32) for k in indexes]\n",
        "    imgH=Hflip(img)\n",
        "    imgV=Vflip(img)\n",
        "    imgR=rotate(img)\n",
        "    images=[]\n",
        "    images.extend(imgH)\n",
        "    images.extend(imgV)\n",
        "    images.extend(imgR)\n",
        "    lbl=labels.copy()\n",
        "    labels=pd.DataFrame()\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    labels=pd.concat([labels,lbl],0)\n",
        "    #images = np.array([preprocess_input(img) for img in images])\n",
        "    return np.asarray(images), np.asarray(labels.values)"
      ],
      "metadata": {
        "id": "Nu7y_NLNVhJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  best_accuracy_last={}\n",
        "  final_accuracy_last={}\n",
        "  history_last={}\n",
        "  answers_last={}\n",
        "  predictions_last={}\n",
        "  predictions_last_best={}\n",
        "  times_last={}"
      ],
      "metadata": {
        "id": "l3oYLnRiVhGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def upd(dk,data):\n",
        "    if dk==0:\n",
        "        dk=data\n",
        "    else:\n",
        "        for ky in data.keys():\n",
        "            dk[ky].extend(data[ky])\n",
        "    return dk\n",
        "  index=1\n",
        "  epoch=50\n",
        "  pre_acc=0\n",
        "  best=0\n",
        "  fold='fold_'+str(index)\n",
        "  trn,tst=get_trn_tst(df,index)\n",
        "  history_last[fold]=0\n",
        "\n",
        "\n",
        "\n",
        "  plt.imshow(trn[0][0])\n",
        "  plt.show()\n",
        "  plt.imshow(tst[0][0])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  trn_x,trn_y=unison_shuffled_copies(trn[0],trn[1])\n",
        "  tst_x,tst_y=unison_shuffled_copies(tst[0],tst[1])\n",
        "\n",
        "\n",
        "\n",
        "  model=load_model()\n",
        "\n",
        "\n",
        "  \n",
        "  #compiling the model\n",
        "  train_data = DataGenerator(trn_x,pd.get_dummies(trn_y), batch_size=1, augment=True)\n",
        "  ln=len(trn_y)\n",
        "  # del([trn_x,trn_y,trn,tst])\n",
        "  # gc.collect()\n",
        "  #fitting the model\n",
        "  #timing\n",
        "  start=time.time()\n",
        "  print('training')\n",
        "  model.compile(optimizer=Adam(1e-2,decay=1e-3), \n",
        "                     loss='categorical_crossentropy', \n",
        "                     metrics=['accuracy'])\n",
        "  hist=model.fit_generator(train_data,epochs=1,verbose=1,steps_per_epoch=ln//1)\n",
        "\n",
        "  for i in model.layers:\n",
        "        i.trainable=True\n",
        "  model.compile(optimizer=Adam(2e-4,decay=1e-3), \n",
        "                     loss='categorical_crossentropy', \n",
        "                     metrics=['accuracy'])\n",
        "  hist=model.fit_generator(train_data,epochs=50,verbose=1,steps_per_epoch=ln//1)\n",
        "  history_last[fold]=upd(history_last[fold],hist.history)\n",
        "  del([train_data,trn_x,trn_y,trn,df])\n",
        "  gc.collect()\n",
        "  end=time.time()\n",
        "  times_last[fold]=end-start\n",
        "\n",
        "  #getting the prediction \n",
        "  pre=model.predict(tst_x)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  #select the maximum position\n",
        "  pre=np.argmax(pre,1)\n",
        "  predictions_last[fold]=pre\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  #getting the accuracy\n",
        "  new_acc=accuracy_score(pre,tst_y)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  #storing the predictions\n",
        "  final_accuracy_last[fold]=new_acc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #storing the answers\n",
        "  answers_last[fold]=tst_y\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  #freeing memory\n",
        "  del([tst_y,tst_x])\n",
        "  gc.collect()"
      ],
      "metadata": {
        "id": "yWU7Sb6RVhDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_last[fold]['loss'])"
      ],
      "metadata": {
        "id": "XrM34cR7VhA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_acc"
      ],
      "metadata": {
        "id": "MLi688RhVg-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  print(new_acc)\n",
        "  index=str(index)\n",
        "  type='gated'\n",
        "  model1='squeezenet_'\n",
        "  path='/content/gdrive/My Drive/'\n",
        "  index=str(index)\n",
        "  np.save(path+\"/best_accuracy_all_fold_\"+index+\"_\"+model1+\"_\"+type+\".npy\",best_accuracy_last)\n",
        "  np.save(path+'/final_accuracy_all_fold'+index+\"_\"+model1+\"_\"+type+\".npy\",final_accuracy_last)\n",
        "  np.save(path+'/history_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",history_last)\n",
        "  np.save(path+'/answers_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",answers_last)\n",
        "  np.save(path+'/predictions_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",predictions_last)\n",
        "  np.save(path+'/predictions_all_best_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",predictions_last_best)\n",
        "  np.save(path+'/times_all_fold_'+index+\"_\"+model1+\"_\"+type+\".npy\",times_last)\n",
        "  model.save_weights(path+type+index+'.hdf5')"
      ],
      "metadata": {
        "id": "NtkNBlOYVg74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "W31MnTAVVg5Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}