{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "check.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/gan_segmentation_FE/blob/main/check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb7748c-11a7-49b9-8113-3186a21154d7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/gdrive/MyDrive/total.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBsut26sqm0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74caeb2c-5499-4414-b624-9d1e1c04a5a9"
      },
      "source": [
        "!pip install pydicom"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/15/df16546bc59bfca390cf072d473fb2c8acd4231636f64356593a63137e55/pydicom-2.1.2-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 8.2MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4WISpeBMVXn"
      },
      "source": [
        "import glob\n",
        "import pydicom\n",
        "import gc\n",
        "import numpy as np\n",
        "for path in glob.glob('/content/3Dircadb*.zip'):\n",
        "  with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69BKoxueoVFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cefa898-24bb-4e2f-f2cc-04ef02a66d3c"
      },
      "source": [
        "import glob\n",
        "import pydicom\n",
        "import gc\n",
        "import numpy as np\n",
        "for path in glob.glob('/content/3Dircadb*/PATIENT_DICOM.zip'):\n",
        "  with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/'+path.split('/')[2])\n",
        "  image=[]\n",
        "  for i in range(len(glob.glob('/content/'+path.split('/')[-2]+'/PATIENT_DICOM/*'))):\n",
        "      image.append(pydicom.dcmread('/content/'+path.split('/')[-2]+'/PATIENT_DICOM/image_'+str(i)).pixel_array)\n",
        "  img=np.asarray(image)\n",
        "  img[img < -200] = -200\n",
        "  img[img > 250] = 250\n",
        "  print(img.shape)\n",
        "  np.save(path.split('/')[2]+'_input.npy',img)\n",
        "  del([img,image])\n",
        "  gc.collect()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(125, 512, 512)\n",
            "(124, 512, 512)\n",
            "(151, 512, 512)\n",
            "(122, 512, 512)\n",
            "(135, 512, 512)\n",
            "(260, 512, 512)\n",
            "(172, 512, 512)\n",
            "(111, 512, 512)\n",
            "(129, 512, 512)\n",
            "(124, 512, 512)\n",
            "(91, 512, 512)\n",
            "(155, 512, 512)\n",
            "(74, 512, 512)\n",
            "(225, 512, 512)\n",
            "(219, 512, 512)\n",
            "(119, 512, 512)\n",
            "(113, 512, 512)\n",
            "(167, 512, 512)\n",
            "(200, 512, 512)\n",
            "(132, 512, 512)\n",
            "(139, 512, 512)\n",
            "(122, 512, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oWV42d88j-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60010c17-668e-4b59-c4b4-c08613a91a87"
      },
      "source": [
        "import glob\n",
        "import pydicom\n",
        "import gc\n",
        "import numpy as np\n",
        "for path in glob.glob('/content/3Dircadb*/MASKS_DICOM.zip'):\n",
        "  with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/'+path.split('/')[2])\n",
        "  image=[]\n",
        "  for i in range(len(glob.glob('/content/'+path.split('/')[-2]+'/MASKS_DICOM/liver/*'))):\n",
        "      image.append(pydicom.dcmread('/content/'+path.split('/')[-2]+'/MASKS_DICOM/liver/image_'+str(i)).pixel_array)\n",
        "  img=np.asarray(image)\n",
        "  if img.shape[0]==0:\n",
        "    print(path)\n",
        "  else:\n",
        "    print(img.shape)\n",
        "    np.save(path.split('/')[2]+'_liver.npy',img)\n",
        "  del([img,image])\n",
        "  gc.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(125, 512, 512)\n",
            "(124, 512, 512)\n",
            "(151, 512, 512)\n",
            "(122, 512, 512)\n",
            "(135, 512, 512)\n",
            "(260, 512, 512)\n",
            "(172, 512, 512)\n",
            "(111, 512, 512)\n",
            "(129, 512, 512)\n",
            "(124, 512, 512)\n",
            "(91, 512, 512)\n",
            "(155, 512, 512)\n",
            "(74, 512, 512)\n",
            "(225, 512, 512)\n",
            "(219, 512, 512)\n",
            "(119, 512, 512)\n",
            "(113, 512, 512)\n",
            "(167, 512, 512)\n",
            "(200, 512, 512)\n",
            "(132, 512, 512)\n",
            "(139, 512, 512)\n",
            "(122, 512, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUv5cSFpuK1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d21b40-c185-4488-d583-b11fd101e0bf"
      },
      "source": [
        "import glob\n",
        "import pydicom\n",
        "import gc\n",
        "import numpy as np\n",
        "for path in glob.glob('/content/3Dircadb*/MASKS_DICOM.zip'):\n",
        "  with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/'+path.split('/')[2])\n",
        "  image=[]\n",
        "  for i in range(len(glob.glob('/content/'+path.split('/')[-2]+'/MASKS_DICOM/livertumor/*'))):\n",
        "      image.append(pydicom.dcmread('/content/'+path.split('/')[-2]+'/MASKS_DICOM/livertumor/image_'+str(i)).pixel_array)\n",
        "  img=np.asarray(image)\n",
        "  if img.shape[0]==0:\n",
        "    print(path)\n",
        "  else:\n",
        "    print(img.shape)\n",
        "    np.save(path.split('/')[2]+'_livertumor.npy',img)\n",
        "  del([img,image])\n",
        "  gc.collect()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(125, 512, 512)\n",
            "/content/3Dircadb1.8/MASKS_DICOM.zip\n",
            "/content/3Dircadb1.7/MASKS_DICOM.zip\n",
            "(122, 512, 512)\n",
            "(135, 512, 512)\n",
            "(260, 512, 512)\n",
            "(172, 512, 512)\n",
            "(111, 512, 512)\n",
            "/content/3Dircadb1.1/MASKS_DICOM.zip\n",
            "/content/3Dircadb1.19/MASKS_DICOM.zip\n",
            "(91, 512, 512)\n",
            "(155, 512, 512)\n",
            "(74, 512, 512)\n",
            "/content/3Dircadb1.20/MASKS_DICOM.zip\n",
            "/content/3Dircadb2.2/MASKS_DICOM.zip\n",
            "/content/3Dircadb1.17/MASKS_DICOM.zip\n",
            "/content/3Dircadb1.14/MASKS_DICOM.zip\n",
            "/content/3Dircadb2.1/MASKS_DICOM.zip\n",
            "(200, 512, 512)\n",
            "/content/3Dircadb1.11/MASKS_DICOM.zip\n",
            "/content/3Dircadb1.5/MASKS_DICOM.zip\n",
            "(122, 512, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0PuY2ltp9TB"
      },
      "source": [
        "#custom layers\n",
        "\n",
        "from keras.layers import Layer, InputSpec\n",
        "try:\n",
        "    from keras import initializations\n",
        "except ImportError:\n",
        "    from keras import initializers as initializations\n",
        "import keras.backend as K\n",
        "\n",
        "class Scale(Layer):\n",
        "    '''Custom Layer for DenseNet used for BatchNormalization.\n",
        "    \n",
        "    Learns a set of weights and biases used for scaling the input data.\n",
        "    the output consists simply in an element-wise multiplication of the input\n",
        "    and a sum of a set of constants:\n",
        "        out = in * gamma + beta,\n",
        "    where 'gamma' and 'beta' are the weights and biases larned.\n",
        "    # Arguments\n",
        "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
        "            if your input tensor has shape (samples, channels, rows, cols),\n",
        "            set axis to 1 to normalize per feature map (channels axis).\n",
        "        momentum: momentum in the computation of the\n",
        "            exponential average of the mean and standard deviation\n",
        "            of the data, for feature-wise normalization.\n",
        "        weights: Initialization weights.\n",
        "            List of 2 Numpy arrays, with shapes:\n",
        "            `[(input_shape,), (input_shape,)]`\n",
        "        beta_init: name of initialization function for shift parameter\n",
        "            (see [initializations](../initializations.md)), or alternatively,\n",
        "            Theano/TensorFlow function to use for weights initialization.\n",
        "            This parameter is only relevant if you don't pass a `weights` argument.\n",
        "        gamma_init: name of initialization function for scale parameter (see\n",
        "            [initializations](../initializations.md)), or alternatively,\n",
        "            Theano/TensorFlow function to use for weights initialization.\n",
        "            This parameter is only relevant if you don't pass a `weights` argument.\n",
        "    '''\n",
        "    def __init__(self, weights=None, axis=-1, momentum = 0.9, beta_init='zero', gamma_init='one', **kwargs):\n",
        "        self.momentum = momentum\n",
        "        self.axis = axis\n",
        "        self.beta_init = initializations.get(beta_init)\n",
        "        self.gamma_init = initializations.get(gamma_init)\n",
        "        self.initial_weights = weights\n",
        "        super(Scale, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        shape = (int(input_shape[self.axis]),)\n",
        "\n",
        "        # Tensorflow >= 1.0.0 compatibility\n",
        "        self.gamma = K.variable(self.gamma_init(shape), name='{}_gamma'.format(self.name))\n",
        "        self.beta = K.variable(self.beta_init(shape), name='{}_beta'.format(self.name))\n",
        "        #self.gamma = self.gamma_init(shape, name='{}_gamma'.format(self.name))\n",
        "        #self.beta = self.beta_init(shape, name='{}_beta'.format(self.name))\n",
        "        self._trainable_weights = [self.gamma, self.beta]\n",
        "\n",
        "        if self.initial_weights is not None:\n",
        "            self.set_weights(self.initial_weights)\n",
        "            del self.initial_weights\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        input_shape = self.input_spec[0].shape\n",
        "        broadcast_shape = [1] * len(input_shape)\n",
        "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
        "\n",
        "        out = K.reshape(self.gamma, broadcast_shape) * x + K.reshape(self.beta, broadcast_shape)\n",
        "        return out\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\"momentum\": self.momentum, \"axis\": self.axis}\n",
        "        base_config = super(Scale, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO2MaKLj8DlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af839765-9c80-4287-cbe2-443e9af132d7"
      },
      "source": [
        "pip install medpy"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting medpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/70/c1fd5dd60242eee81774696ea7ba4caafac2bad8f028bba94b1af83777d7/MedPy-0.4.0.tar.gz (151kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 19.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 22.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 51kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 71kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 81kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 92kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 102kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 112kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 122kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 133kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 143kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from medpy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from medpy) (1.19.5)\n",
            "Collecting SimpleITK>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/6b/85df5eb3a8059b23a53a9f224476e75473f9bcc0a8583ed1a9c34619f372/SimpleITK-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (47.4MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4MB 109kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: medpy\n",
            "  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for medpy: filename=MedPy-0.4.0-cp37-cp37m-linux_x86_64.whl size=754472 sha256=aa6400431bfac3f2c5818d717cf35623817ec2081baf2ff9627f459bc2c5e628\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/c9/9c/2c6281c7a72b9fb1570862a4f028af7ce38405008354fbf870\n",
            "Successfully built medpy\n",
            "Installing collected packages: SimpleITK, medpy\n",
            "Successfully installed SimpleITK-2.0.2 medpy-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOdQm2sOWzCv"
      },
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "# sys.path.insert(0,'/home/xmli/livertumor_xmli/Keras-2.0.8')\n",
        "# sys.path.insert(0,'/home/xmli/livertumor_xmli/mylib')\n",
        "# sys.path.insert(0,'/research/pheng/xmli/livertumor/Keras-2.0.8')\n",
        "# sys.path.insert(0,'/research/pheng/xmli/livertumor/mylib')\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "import random\n",
        "from medpy.io import load\n",
        "import numpy as np\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, ZeroPadding2D, concatenate, add\n",
        "from keras.layers.core import Dropout, Activation\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.layers.pooling import AveragePooling2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import keras.backend as K\n",
        "import os\n",
        "import time\n",
        "from skimage.transform import resize\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "path = './result_train_denseU167_fast_new/'\n",
        "batch_size = 1\n",
        "img_deps = 512\n",
        "img_rows = 512\n",
        "img_cols = 3\n",
        "std = 37\n",
        "thread_num = 14\n",
        "txtfile = 'myTrainingDataTxt'\n",
        "mean = 48\n",
        "\n",
        "liverlist = [32,34,38,41,47,87,89,91,105,106,114,115,119]\n",
        "DataList = [\"/home/xmli/gpu7_xmli/\"]\n",
        "def load_seq_crop_data_masktumor_try(Parameter_List):\n",
        "    img = Parameter_List[0]\n",
        "    tumor = Parameter_List[1]\n",
        "    lines = Parameter_List[2]\n",
        "    numid = Parameter_List[3]\n",
        "    minindex = Parameter_List[4]\n",
        "    maxindex = Parameter_List[5]\n",
        "    #  randomly scale\n",
        "    scale = np.random.uniform(0.8,1.2)\n",
        "    deps = int(img_deps * scale)\n",
        "    rows = int(img_rows * scale)\n",
        "    cols = 3\n",
        "\n",
        "    sed = np.random.randint(1,numid)\n",
        "    cen = lines[sed-1]\n",
        "    cen = np.fromstring(cen, dtype=int, sep=' ')\n",
        "    # print (cen)\n",
        "    a = min(max(minindex[0] + deps/2, cen[0]), maxindex[0]- deps/2-1)\n",
        "    b = min(max(minindex[1] + rows/2, cen[1]), maxindex[1]- rows/2-1)\n",
        "    c = min(max(minindex[2] + cols/2, cen[2]), maxindex[2]- cols/2-1)\n",
        "    cropp_img = img[a - deps / 2:a + deps / 2, b - rows / 2:b + rows / 2,\n",
        "                c - cols / 2: c + cols / 2 + 1].copy()\n",
        "    cropp_tumor = tumor[a - deps / 2:a + deps / 2, b - rows / 2:b + rows / 2,\n",
        "                  c - cols / 2:c + cols / 2 + 1].copy()\n",
        "\n",
        "    cropp_img -= mean\n",
        "     # randomly flipping\n",
        "    flip_num = np.random.randint(0,3)\n",
        "    if flip_num == 1:\n",
        "        cropp_img = np.flipud(cropp_img)\n",
        "        cropp_tumor = np.flipud(cropp_tumor)\n",
        "    elif flip_num == 2:\n",
        "        cropp_img = np.fliplr(cropp_img)\n",
        "        cropp_tumor = np.fliplr(cropp_tumor)\n",
        "    #\n",
        "    cropp_tumor = resize(cropp_tumor, (img_deps,img_rows,img_cols), order=0, mode='edge', cval=0, clip=True, preserve_range=True)\n",
        "    cropp_img   = resize(cropp_img, (img_deps,img_rows,img_cols), order=3, mode='constant', cval=0, clip=True, preserve_range=True)\n",
        "    return cropp_img, cropp_tumor[:,:,1]\n",
        "\n",
        "def generate_arrays_from_file(batch_size, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list):\n",
        "    while 1:\n",
        "        X = np.zeros((batch_size, img_deps, img_rows, img_cols), dtype='float32')\n",
        "        Y = np.zeros((batch_size, img_deps, img_rows, 1), dtype='int16')\n",
        "        Parameter_List = []\n",
        "        for idx in range(batch_size):\n",
        "            count = random.choice(trainidx)\n",
        "            img = img_list[count]\n",
        "            tumor = tumor_list[count]\n",
        "            minindex = minindex_list[count]\n",
        "            maxindex = maxindex_list[count]\n",
        "            num = np.random.randint(0,6)\n",
        "            if num < 3 or (count in liverlist):\n",
        "                lines = liverlines[count]\n",
        "                numid = liveridx[count]\n",
        "            else:\n",
        "                lines = tumorlines[count]\n",
        "                numid = tumoridx[count]\n",
        "            Parameter_List.append([img, tumor, lines, numid, minindex, maxindex])\n",
        "        pool = ThreadPool(thread_num)\n",
        "        result_list = pool.map(load_seq_crop_data_masktumor_try, Parameter_List)\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "        for idx in range(len(result_list)):\n",
        "            X[idx, :, :, :] = result_list[idx][0]\n",
        "            Y[idx, :, :, 0] = result_list[idx][1]\n",
        "        yield (X,Y)\n",
        "\n",
        "def weighted_crossentropy(y_true, y_pred):\n",
        "\n",
        "    y_pred_f = K.reshape(y_pred, (batch_size*img_deps*img_rows,3))\n",
        "    y_true_f = K.reshape(y_true, (batch_size*img_deps*img_rows,))\n",
        "\n",
        "    soft_pred_f = K.softmax(y_pred_f)\n",
        "    soft_pred_f = K.log(tf.clip_by_value(soft_pred_f, 1e-10, 1.0))\n",
        "\n",
        "    neg = K.equal(y_true_f, K.zeros_like(y_true_f))\n",
        "    neg_calculoss = tf.gather(soft_pred_f[:,0], tf.where(neg))\n",
        "\n",
        "    pos1 = K.equal(y_true_f, K.ones_like(y_true_f))\n",
        "    pos1_calculoss = tf.gather(soft_pred_f[:,1], tf.where(pos1))\n",
        "\n",
        "    pos2 = K.equal(y_true_f, 2*K.ones_like(y_true_f))\n",
        "    pos2_calculoss = tf.gather(soft_pred_f[:,2], tf.where(pos2))\n",
        "\n",
        "    loss = -K.mean(tf.concat([0.78*neg_calculoss, 0.65*pos1_calculoss, 8.57*pos2_calculoss], 0))\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def DenseUNet(nb_dense_block=4, growth_rate=48, nb_filter=96, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, classes=1000, weights_path=None):\n",
        "    '''Instantiate the DenseNet 161 architecture,\n",
        "        # Arguments\n",
        "            nb_dense_block: number of dense blocks to add to end\n",
        "            growth_rate: number of filters to add per dense block\n",
        "            nb_filter: initial number of filters\n",
        "            reduction: reduction factor of transition blocks.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            classes: optional number of classes to classify images\n",
        "            weights_path: path to pre-trained weights\n",
        "        # Returns\n",
        "            A Keras model instance.\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "\n",
        "    # compute compression factor\n",
        "    compression = 1.0 - reduction\n",
        "\n",
        "    # Handle Dimension Ordering for different backends\n",
        "    global concat_axis\n",
        "    if K.image_dim_ordering() == 'tf':\n",
        "      concat_axis = 3\n",
        "      img_input = Input(batch_shape=(batch_size, img_deps, img_rows, 3), name='data')\n",
        "    else:\n",
        "      concat_axis = 1\n",
        "      img_input = Input(shape=(3, 512,512), name='data')\n",
        "\n",
        "    # From architecture for ImageNet (Table 1 in the paper)\n",
        "    nb_filter = 96\n",
        "    nb_layers = [6,12,36,24] # For DenseNet-161\n",
        "    box = []\n",
        "    # Initial convolution\n",
        "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n",
        "    x = Conv2D(nb_filter, (7, 7), strides=(2, 2), name='conv1', use_bias=False)(x)\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name='conv1_bn')(x)\n",
        "    x = Scale(axis=concat_axis, name='conv1_scale')(x)\n",
        "    x = Activation('relu', name='relu1')(x)\n",
        "    box.append(x)\n",
        "    x = ZeroPadding2D((1, 1), name='pool1_zeropadding')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n",
        "\n",
        "    # Add dense blocks\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        stage = block_idx+2\n",
        "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        box.append(x)\n",
        "        # Add transition_block\n",
        "        x = transition_block(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "    final_stage = stage + 1\n",
        "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name='conv'+str(final_stage)+'_blk_bn')(x)\n",
        "    x = Scale(axis=concat_axis, name='conv'+str(final_stage)+'_blk_scale')(x)\n",
        "    x = Activation('relu', name='relu'+str(final_stage)+'_blk')(x)\n",
        "    box.append(x)\n",
        "\n",
        "    up0 = UpSampling2D(size=(2,2))(x)\n",
        "    line0 = Conv2D(2208, (1, 1), padding=\"same\", kernel_initializer=\"normal\", name=\"line0\")(box[3])\n",
        "    up0_sum = add([line0, up0])\n",
        "    conv_up0 = Conv2D(768, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up0\")(up0_sum)\n",
        "    bn_up0 = BatchNormalization(name = \"bn_up0\")(conv_up0)\n",
        "    ac_up0 = Activation('relu', name='ac_up0')(bn_up0)\n",
        "\n",
        "    up1 = UpSampling2D(size=(2,2))(ac_up0)\n",
        "    up1_sum = add([box[2], up1])\n",
        "    conv_up1 = Conv2D(384, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up1\")(up1_sum)\n",
        "    bn_up1 = BatchNormalization(name = \"bn_up1\")(conv_up1)\n",
        "    ac_up1 = Activation('relu', name='ac_up1')(bn_up1)\n",
        "\n",
        "    up2 = UpSampling2D(size=(2,2))(ac_up1)\n",
        "    up2_sum = add([box[1], up2])\n",
        "    conv_up2 = Conv2D(96, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up2\")(up2_sum)\n",
        "    bn_up2 = BatchNormalization(name = \"bn_up2\")(conv_up2)\n",
        "    ac_up2 = Activation('relu', name='ac_up2')(bn_up2)\n",
        "\n",
        "    up3 = UpSampling2D(size=(2,2))(ac_up2)\n",
        "    up3_sum = add([box[0], up3])\n",
        "    conv_up3 = Conv2D(96, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up3\")(up3_sum)\n",
        "    bn_up3 = BatchNormalization(name = \"bn_up3\")(conv_up3)\n",
        "    ac_up3 = Activation('relu', name='ac_up3')(bn_up3)\n",
        "\n",
        "    up4 = UpSampling2D(size=(2, 2))(ac_up3)\n",
        "    conv_up4 = Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name=\"conv_up4\")(up4)\n",
        "    conv_up4 = Dropout(rate=0.3)(conv_up4)\n",
        "    bn_up4 = BatchNormalization(name=\"bn_up4\")(conv_up4)\n",
        "    ac_up4 = Activation('relu', name='ac_up4')(bn_up4)\n",
        "\n",
        "    x = Conv2D(3, (1,1), padding=\"same\", kernel_initializer=\"normal\", name=\"dense167classifer\")(ac_up4)\n",
        "\n",
        "    model = Model(img_input, x, name='denseu161')\n",
        "\n",
        "    if weights_path is not None:\n",
        "      model.load_weights(weights_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "def conv_block(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
        "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv2D, 3x3 Conv2D, and option dropout\n",
        "        # Arguments\n",
        "            x: input tensor \n",
        "            stage: index for dense block\n",
        "            branch: layer index within each dense block\n",
        "            nb_filter: number of filters\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = 'conv' + str(stage) + '_' + str(branch)\n",
        "    relu_name_base = 'relu' + str(stage) + '_' + str(branch)\n",
        "\n",
        "    # 1x1 Convolution (Bottleneck layer)\n",
        "    inter_channel = nb_filter * 4\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_x1_bn')(x)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_x1_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x1')(x)\n",
        "    x = Conv2D(inter_channel, (1, 1), name=conv_name_base+'_x1', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # 3x3 Convolution\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_x2_bn')(x)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_x2_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x2')(x)\n",
        "    x = ZeroPadding2D((1, 1), name=conv_name_base+'_x2_zeropadding')(x)\n",
        "    x = Conv2D(nb_filter, (3, 3), name=conv_name_base+'_x2', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def transition_block(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
        "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout \n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_filter: number of filters\n",
        "            compression: calculated as 1 - reduction. Reduces the number of feature maps in the transition block.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = 'conv' + str(stage) + '_blk'\n",
        "    relu_name_base = 'relu' + str(stage) + '_blk'\n",
        "    pool_name_base = 'pool' + str(stage)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_bn')(x)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base)(x)\n",
        "    x = Conv2D(int(nb_filter * compression), (1, 1), name=conv_name_base, use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = AveragePooling2D((2, 2), strides=(2, 2), name=pool_name_base)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True):\n",
        "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_layers: the number of layers of conv_block to append to the model.\n",
        "            nb_filter: number of filters\n",
        "            growth_rate: growth rate\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    concat_feat = x\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        branch = i+1\n",
        "        x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
        "        concat_feat = concatenate([concat_feat, x], axis=concat_axis, name='concat_'+str(stage)+'_'+str(branch))\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    return concat_feat, nb_filter\n",
        "\n",
        "\n",
        "\n",
        "def train_and_predict():\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Creating and compiling model...')\n",
        "    print('-'*30)\n",
        "\n",
        "    model = DenseUNet(reduction=0.5, weights_path='./result_train_dense167_fast/model/weights365.04-0.02.hdf5')\n",
        "    sgd = SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
        "    model.compile(optimizer=sgd, loss=[weighted_crossentropy])\n",
        "\n",
        "    trainidx = list(range(131))\n",
        "    img_list = []\n",
        "    tumor_list = []\n",
        "    minindex_list = []\n",
        "    maxindex_list = []\n",
        "    tumorlines = []\n",
        "    tumoridx = []\n",
        "    liveridx = []\n",
        "    liverlines = []\n",
        "    t1=time.time()\n",
        "    for idx in range(131):\n",
        "        img, img_header = load(DataList[0] + 'myTrainingData/volume-' + str(idx) + '.nii' )\n",
        "        tumor, tumor_header = load(DataList[0] + 'myTrainingData/segmentation-' + str(idx) + '.nii')\n",
        "        img_list.append(img)\n",
        "        tumor_list.append(tumor)\n",
        "\n",
        "        maxmin = np.loadtxt(DataList[0] + str(txtfile) + '/LiverBox/box_' + str(idx) + '.txt', delimiter=' ')\n",
        "        minindex = maxmin[0:3]\n",
        "        maxindex = maxmin[3:6]\n",
        "        minindex = np.array(minindex, dtype='int')\n",
        "        maxindex = np.array(maxindex, dtype='int')\n",
        "        minindex[0] = max(minindex[0]-3, 0)\n",
        "        minindex[1] = max(minindex[1]-3, 0)\n",
        "        minindex[2] = max(minindex[2]-3, 0)\n",
        "        maxindex[0] = min(img.shape[0], maxindex[0]+3)\n",
        "        maxindex[1] = min(img.shape[1], maxindex[1]+3)\n",
        "        maxindex[2] = min(img.shape[2], maxindex[2]+3)\n",
        "        minindex_list.append(minindex)\n",
        "        maxindex_list.append(maxindex)\n",
        "\n",
        "        f1 = open(DataList[0] + str(txtfile) + '/TumorPixels/tumor_' + str(idx) + '.txt','r')\n",
        "        tumorline = f1.readlines()\n",
        "        tumorlines.append(tumorline)\n",
        "        tumoridx.append(len(tumorline))\n",
        "        f1.close()\n",
        "\n",
        "        f2 = open(DataList[0] + str(txtfile) + '/LiverPixels/liver_' + str(idx) + '.txt','r')\n",
        "        liverline = f2.readlines()\n",
        "        liverlines.append(liverline)\n",
        "        liveridx.append(len(liverline))\n",
        "        f2.close()\n",
        "    t2=time.time()\n",
        "    print (t2-t1)\n",
        "\n",
        "\n",
        "    # print (model.summary())\n",
        "\n",
        "    if not os.path.exists(path + \"model\"):\n",
        "        os.mkdir(path + 'model')\n",
        "        os.mkdir(path + 'history')\n",
        "    else:\n",
        "        if os.path.exists(path + \"history/lossbatch.txt\"):\n",
        "            os.remove(path + 'history/lossbatch.txt')\n",
        "        if os.path.exists(path + \"history/lossepoch.txt\"):\n",
        "            os.remove(path + 'history/lossepoch.txt')\n",
        "    model_checkpoint = ModelCheckpoint(path + 'model/weights.{epoch:02d}-{loss:.2f}.hdf5', monitor='loss', verbose = 1,\n",
        "                                       save_best_only=False,save_weights_only=False,mode = 'min', period = 2)\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Fitting model......')\n",
        "    print('-'*30)\n",
        "\n",
        "    steps = 27386/batch_size\n",
        "    model.fit_generator(generate_arrays_from_file(batch_size, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list),steps_per_epoch=steps,\n",
        "                        epochs= 6000, verbose = 1, callbacks = [model_checkpoint], max_queue_size=10, workers=3, use_multiprocessing=True)\n",
        "\n",
        "    print ('Finised Training .......')\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnurKIQWPTme"
      },
      "source": [
        "#loss\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def weighted_crossentropy(y_true, y_pred):\n",
        "    y_pred = y_pred[:,:,:,1:7,:]\n",
        "    y_true = y_true[:,:,:,1:7,:]\n",
        "    y_pred_f = K.reshape(y_pred, (-1,3))\n",
        "    y_true_f = K.reshape(y_true, (-1,))\n",
        "\n",
        "    soft_pred_f = K.softmax(y_pred_f)\n",
        "    soft_pred_f = K.log(tf.clip_by_value(soft_pred_f, 1e-10, 1.0))\n",
        "\n",
        "    neg = K.equal(y_true_f, K.zeros_like(y_true_f))\n",
        "    neg_calculoss = tf.gather(soft_pred_f[:,0], tf.where(neg))\n",
        "\n",
        "    pos1 = K.equal(y_true_f, K.ones_like(y_true_f))\n",
        "    pos1_calculoss = tf.gather(soft_pred_f[:,1], tf.where(pos1))\n",
        "\n",
        "    pos2 = K.equal(y_true_f, 2*K.ones_like(y_true_f))\n",
        "    pos2_calculoss = tf.gather(soft_pred_f[:,2], tf.where(pos2))\n",
        "\n",
        "    loss = -K.mean(tf.concat([0.78*neg_calculoss, 0.65*pos1_calculoss, 8.57*pos2_calculoss], 0))\n",
        "\n",
        "    return loss\n",
        "\n",
        "def weighted_crossentropy_2ddense(y_true, y_pred):\n",
        "\n",
        "    y_pred_f = K.reshape(y_pred, (-1,3))\n",
        "    y_true_f = K.reshape(y_true, (-1,))\n",
        "\n",
        "    soft_pred_f = K.softmax(y_pred_f)\n",
        "    soft_pred_f = K.log(tf.clip_by_value(soft_pred_f, 1e-10, 1.0))\n",
        "\n",
        "    neg = K.equal(y_true_f, K.zeros_like(y_true_f))\n",
        "    neg_calculoss = tf.gather(soft_pred_f[:,0], tf.where(neg))\n",
        "\n",
        "    pos1 = K.equal(y_true_f, K.ones_like(y_true_f))\n",
        "    pos1_calculoss = tf.gather(soft_pred_f[:,1], tf.where(pos1))\n",
        "\n",
        "    pos2 = K.equal(y_true_f, 2*K.ones_like(y_true_f))\n",
        "    pos2_calculoss = tf.gather(soft_pred_f[:,2], tf.where(pos2))\n",
        "\n",
        "    loss = -K.mean(tf.concat([0.78*neg_calculoss, 0.65*pos1_calculoss, 8.57*pos2_calculoss], 0))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79arC5XHPTjM"
      },
      "source": [
        "#changed xrange to range\n",
        "\n",
        "#train2ddense\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "#causing problems\n",
        "# import sys\n",
        "# sys.path.insert(0,'Keras-2.0.8')\n",
        "\n",
        "\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "import random\n",
        "from medpy.io import load\n",
        "import numpy as np\n",
        "import argparse\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import keras.backend as K\n",
        "import os\n",
        "\n",
        "\n",
        "# Make parallel\n",
        "# from keras.utils2.multi_gpu import make_parallel\n",
        "\n",
        "\n",
        "from skimage.transform import resize\n",
        "\n",
        "#  global parameters\n",
        "# parser = argparse.ArgumentParser(description='Keras 2d denseunet Training')\n",
        "# #  data folder\n",
        "# parser.add_argument('-data', type=str, default='data/', help='test images')\n",
        "# parser.add_argument('-save_path', type=str, default='Experiments/')\n",
        "# #  other paras\n",
        "# parser.add_argument('-b', type=int, default=1)\n",
        "# parser.add_argument('-input_size', type=int, default=224)\n",
        "# parser.add_argument('-model_weight', type=str, default='./model/densenet161_weights_tf.h5')\n",
        "# parser.add_argument('-input_cols', type=int, default=3)\n",
        "\n",
        "# #  data augment\n",
        "# parser.add_argument('-mean', type=int, default=48)\n",
        "# parser.add_argument('-thread_num', type=int, default=14)\n",
        "# args = parser.parse_args()\n",
        "\n",
        "MEAN = 48\n",
        "thread_num = 14\n",
        "\n",
        "liverlist = [32,34,38,41,47,87,89,91,105,106,114,115,119]\n",
        "def load_seq_crop_data_masktumor_try(Parameter_List):\n",
        "    img = Parameter_List[0]\n",
        "    tumor = Parameter_List[1]\n",
        "    lines = Parameter_List[2]\n",
        "    numid = Parameter_List[3]\n",
        "    minindex = Parameter_List[4]\n",
        "    maxindex = Parameter_List[5]\n",
        "    #  randomly scale\n",
        "    scale = np.random.uniform(0.8,1.2)\n",
        "    deps = int(512 * scale)\n",
        "    rows = int(512 * scale)\n",
        "    cols = 3\n",
        "\n",
        "    sed = np.random.randint(1,numid)\n",
        "    cen = lines[sed-1]\n",
        "    cen = np.fromstring(cen, dtype=int, sep=' ')\n",
        "\n",
        "    a = min(max(minindex[0] + deps/2, cen[0]), maxindex[0]- deps/2-1)\n",
        "    b = min(max(minindex[1] + rows/2, cen[1]), maxindex[1]- rows/2-1)\n",
        "    c = min(max(minindex[2] + cols/2, cen[2]), maxindex[2]- cols/2-1)\n",
        "    \n",
        "    \n",
        "    # a1=min(img.shape[0]-int(a - deps / 2),int(a + (deps / 2)))\n",
        "    # a2=max(img.shape[0]-int(a - deps / 2),int(a + (deps / 2)))\n",
        "    # b1=min(img.shape[1]-int(b - (rows / 2)),int(b + (rows / 2)))\n",
        "    # b2=max(img.shape[1]-int(b - (rows / 2)),int(b + (rows / 2)))\n",
        "    # c1=min(img.shape[2]-int(c - (cols / 2)),int(c + (cols / 2)))\n",
        "    # c2=max(img.shape[2]-int(c - (cols / 2)),int(c + (cols / 2)))\n",
        "    \n",
        "    \n",
        "    # cropp_img = img[int(a - (deps / 2)):int(a + (deps / 2)), int(b - (rows / 2)):int(b + (rows / 2)),\n",
        "    #             int(c - (cols / 2)): int(c + (cols / 2) + 1)].copy()\n",
        "    # cropp_tumor = tumor[int(a -( deps / 2)):int(a + (deps / 2)), int(b - (rows / 2)):int(b + (rows / 2)),\n",
        "    #               int(c - (cols / 2)):int(c + (cols / 2) + 1)].copy()\n",
        "    \n",
        "    if a < deps / 2:\n",
        "      a1=int((img.shape[0]/2)-(deps/2))\n",
        "      a2=int((img.shape[0]/2)+(deps/2))\n",
        "    else:\n",
        "      a1=int(a-deps/2)\n",
        "      a2=int(a+deps/2)\n",
        "    if b < rows / 2:\n",
        "      b1=int((img.shape[0]/2)-(rows/2))\n",
        "      b2=int((img.shape[0]/2)+(rows/2))\n",
        "    else:\n",
        "      b1=int(b-rows/2)\n",
        "      b2=int(b+rows/2)\n",
        "    if c < cols / 2:\n",
        "      c1=int((img.shape[0]/2)-(cols/2))\n",
        "      c2=int((img.shape[0]/2)+(cols/2))\n",
        "    else:\n",
        "      c1=int(c-cols/2)\n",
        "      c2=int(c+cols/2)\n",
        "\n",
        "    cropp_img = img[a1:a2, b1:b2,c1: c2+1].copy()\n",
        "\n",
        "    cropp_tumor = tumor[a1:a2, b1:b2,c1: c2+1].copy()\n",
        "\n",
        "    cropp_img -= MEAN\n",
        "     # randomly flipping\n",
        "    flip_num = np.random.randint(0, 8)\n",
        "    if flip_num == 1:\n",
        "        cropp_img = np.flipud(cropp_img)\n",
        "        cropp_tumor = np.flipud(cropp_tumor)\n",
        "    elif flip_num == 2:\n",
        "        cropp_img = np.fliplr(cropp_img)\n",
        "        cropp_tumor = np.fliplr(cropp_tumor)\n",
        "    elif flip_num == 3:\n",
        "        cropp_img = np.rot90(cropp_img, k=1, axes=(1, 0))\n",
        "        cropp_tumor = np.rot90(cropp_tumor, k=1, axes=(1, 0))\n",
        "    elif flip_num == 4:\n",
        "        cropp_img = np.rot90(cropp_img, k=3, axes=(1, 0))\n",
        "        cropp_tumor = np.rot90(cropp_tumor, k=3, axes=(1, 0))\n",
        "    elif flip_num == 5:\n",
        "        cropp_img = np.fliplr(cropp_img)\n",
        "        cropp_tumor = np.fliplr(cropp_tumor)\n",
        "        cropp_img = np.rot90(cropp_img, k=1, axes=(1, 0))\n",
        "        cropp_tumor = np.rot90(cropp_tumor, k=1, axes=(1, 0))\n",
        "    elif flip_num == 6:\n",
        "        cropp_img = np.fliplr(cropp_img)\n",
        "        cropp_tumor = np.fliplr(cropp_tumor)\n",
        "        cropp_img = np.rot90(cropp_img, k=3, axes=(1, 0))\n",
        "        cropp_tumor = np.rot90(cropp_tumor, k=3, axes=(1, 0))\n",
        "    elif flip_num == 7:\n",
        "        cropp_img = np.flipud(cropp_img)\n",
        "        cropp_tumor = np.flipud(cropp_tumor)\n",
        "        cropp_img = np.fliplr(cropp_img)\n",
        "        cropp_tumor = np.fliplr(cropp_tumor)\n",
        "\n",
        "    cropp_tumor = resize(cropp_tumor, (512,512,3), order=0, mode='edge', cval=0, clip=True, preserve_range=True)\n",
        "    cropp_img   = resize(cropp_img, (512,512,3), order=3, mode='constant', cval=0, clip=True, preserve_range=True)\n",
        "    return cropp_img, cropp_tumor[:,:,1]\n",
        "\n",
        "def generate_arrays_from_file(batch_size, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list):\n",
        "    while 1:\n",
        "        X = np.zeros((1,512,512,3), dtype='float32')\n",
        "        Y = np.zeros((1,512,512, 1), dtype='int16')\n",
        "        Parameter_List = []\n",
        "        for idx in range(batch_size):\n",
        "            count = random.choice(trainidx)\n",
        "            img = img_list[count]\n",
        "            tumor = tumor_list[count]\n",
        "            minindex = minindex_list[count]\n",
        "            maxindex = maxindex_list[count]\n",
        "            num = np.random.randint(0,6)\n",
        "            if num < 3 or (count in liverlist):\n",
        "                lines = liverlines[count]\n",
        "                numid = liveridx[count]\n",
        "            else:\n",
        "                lines = tumorlines[count]\n",
        "                numid = tumoridx[count]\n",
        "            Parameter_List.append([img, tumor, lines, numid, minindex, maxindex])\n",
        "        pool = ThreadPool(thread_num)\n",
        "        result_list = pool.map(load_seq_crop_data_masktumor_try, Parameter_List)\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "        for idx in range(len(result_list)):\n",
        "            X[idx, :, :, :] = result_list[idx][0]\n",
        "            Y[idx, :, :, 0] = result_list[idx][1]\n",
        "        yield (X,Y)\n",
        "\n",
        "\n",
        "def load_fast_files():\n",
        "\n",
        "    trainidx = list(range(5))\n",
        "    img_list = []\n",
        "    tumor_list = []\n",
        "    minindex_list = []\n",
        "    maxindex_list = []\n",
        "    tumorlines = []\n",
        "    tumoridx = []\n",
        "    liveridx = []\n",
        "    liverlines = []\n",
        "    for idx in range(5):\n",
        "        img, img_header = load('/content/data'+ '/myTrainingData/volume-' + str(idx) + '.nii')\n",
        "        tumor, tumor_header = load('/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/segmentation-' + str(idx) + '.nii')\n",
        "        img_list.append(img)\n",
        "        tumor_list.append(tumor)\n",
        "\n",
        "        maxmin = np.loadtxt('/content/data' + '/myTrainingDataTxt/LiverBox/box_' + str(idx) + '.txt', delimiter=' ')\n",
        "        minindex = maxmin[0:3]\n",
        "        maxindex = maxmin[3:6]\n",
        "        minindex = np.array(minindex, dtype='int')\n",
        "        maxindex = np.array(maxindex, dtype='int')\n",
        "        minindex[0] = max(minindex[0] - 3, 0)\n",
        "        minindex[1] = max(minindex[1] - 3, 0)\n",
        "        minindex[2] = max(minindex[2] - 3, 0)\n",
        "        maxindex[0] = min(img.shape[0], maxindex[0] + 3)\n",
        "        maxindex[1] = min(img.shape[1], maxindex[1] + 3)\n",
        "        maxindex[2] = min(img.shape[2], maxindex[2] + 3)\n",
        "        minindex_list.append(minindex)\n",
        "        maxindex_list.append(maxindex)\n",
        "        f1 = open('/content/data' + '/myTrainingDataTxt/TumorPixels/tumor_' + str(idx) + '.txt', 'r')\n",
        "        tumorline = f1.readlines()\n",
        "        tumorlines.append(tumorline)\n",
        "        tumoridx.append(len(tumorline))\n",
        "        f1.close()\n",
        "        f2 = open('/content/data'+ '/myTrainingDataTxt/LiverPixels/liver_' + str(idx) + '.txt', 'r')\n",
        "        liverline = f2.readlines()\n",
        "        liverlines.append(liverline)\n",
        "        liveridx.append(len(liverline))\n",
        "        f2.close()\n",
        "\n",
        "    return trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list\n",
        "\n",
        "def train_and_predict():\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Creating and compiling model...')\n",
        "    print('-'*30)\n",
        "\n",
        "    model = DenseUNet(reduction=0.5)\n",
        "    model.load_weights('/content/gdrive/MyDrive/segmentation/model_best.hdf5', by_name=True)\n",
        "    \n",
        "    \n",
        "    # model = make_parallel(model, args.b / 10, mini_batch=10)\n",
        "    \n",
        "    \n",
        "    sgd = SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
        "    model.compile(optimizer=sgd, loss=[weighted_crossentropy_2ddense])\n",
        "\n",
        "    trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list = load_fast_files()\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Fitting model......')\n",
        "    print('-'*30)\n",
        "    save_path='Experiments/'\n",
        "    if not os.path.exists(save_path):\n",
        "        os.mkdir(save_path)\n",
        "\n",
        "    if not os.path.exists(save_path + \"/model\"):\n",
        "        os.mkdir(save_path + '/model')\n",
        "        os.mkdir(save_path + '/history')\n",
        "    else:\n",
        "        if os.path.exists(save_path+ \"/history/lossbatch.txt\"):\n",
        "            os.remove(save_path + '/history/lossbatch.txt')\n",
        "        if os.path.exists(save_path + \"/history/lossepoch.txt\"):\n",
        "            os.remove(save_path + '/history/lossepoch.txt')\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(save_path + '/model/weights.{epoch:02d}-{loss:.2f}.hdf5', monitor='loss', verbose = 1,\n",
        "                                       save_best_only=False,save_weights_only=False,mode = 'min', period = 1)\n",
        "\n",
        "\n",
        "    steps = 5\n",
        "    model.fit_generator(generate_arrays_from_file(1, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx,\n",
        "                                                  liveridx, minindex_list, maxindex_list),steps_per_epoch=steps,\n",
        "                                                    epochs= 5, verbose = 1, callbacks = [model_checkpoint], max_queue_size=10,\n",
        "                                                    workers=3, use_multiprocessing=True)\n",
        "\n",
        "    print ('Finised Training .......')\n",
        "# if __name__ == '__main__':\n",
        "#     train_and_predict()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sS5g9uyXuNQ"
      },
      "source": [
        "#lib funcs\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from skimage import measure\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict_tumor_inwindow(model, imgs_test, num, mini, maxi):\n",
        "\n",
        "    batch = 1\n",
        "    img_deps = 512\n",
        "    img_rows = 512\n",
        "    img_cols = 8\n",
        "\n",
        "\n",
        "    window_cols = int(img_cols/4)\n",
        "    count = 0\n",
        "    box_test = np.zeros((batch,img_deps,img_rows,img_cols, 1), dtype=\"float32\")\n",
        "\n",
        "    x = imgs_test.shape[0]\n",
        "    y = imgs_test.shape[1]\n",
        "    z = imgs_test.shape[2]\n",
        "    right_cols = int(min(z,maxi[2]+10)-img_cols)\n",
        "    left_cols  =int( max(0,min(mini[2]-5, right_cols)))\n",
        "    score = np.zeros((x, y, z, num), dtype= 'float32')\n",
        "    score_num = np.zeros((x, y, z, num), dtype= 'int16')\n",
        "\n",
        "    for cols in tqdm(range(left_cols,right_cols+window_cols,window_cols)):\n",
        "        # print ('and', z-img_cols,z)\n",
        "        if cols > z - img_cols:\n",
        "            patch_test = imgs_test[0:img_deps, 0:img_rows, z-img_cols:z]\n",
        "            box_test[count, :, :, :, 0] = patch_test\n",
        "            # print ('final', img_cols-window_cols, img_cols)\n",
        "            patch_test_mask = model.predict(box_test, batch_size=batch, verbose=0)\n",
        "            patch_test_mask = K.softmax(patch_test_mask)\n",
        "            patch_test_mask = K.eval(patch_test_mask)\n",
        "            patch_test_mask = patch_test_mask[:,:,:,1:-1,:]\n",
        "\n",
        "            for i in range(batch):\n",
        "                score[0:img_deps, 0:img_rows,  z-img_cols+1:z-1, :] += patch_test_mask[i]\n",
        "                score_num[0:img_deps, 0:img_rows,  z-img_cols+1:z-1, :] += 1\n",
        "        else:\n",
        "            patch_test = imgs_test[0:img_deps, 0:img_rows, cols:cols + img_cols]\n",
        "            box_test[count, :, :, :, 0] = patch_test\n",
        "            patch_test_mask = model.predict(box_test, batch_size=batch, verbose=0)\n",
        "            patch_test_mask = K.softmax(patch_test_mask)\n",
        "            patch_test_mask = K.eval(patch_test_mask)\n",
        "            patch_test_mask = patch_test_mask[:,:,:,1:-1,:]\n",
        "            for i in range(batch):\n",
        "                score[0:img_deps, 0:img_rows, cols+1:cols+img_cols-1, :] += patch_test_mask[i]\n",
        "                score_num[0:img_deps, 0:img_rows, cols+1:cols+img_cols-1, :] += 1\n",
        "    score = score/(score_num+1e-4)\n",
        "    score1 = score[:,:,:,num-2]\n",
        "    score2 = score[:,:,:,num-1]\n",
        "    return score1, score2\n",
        "\n",
        "\n",
        "def predict_window_mulgpu(model,batch, imgs_test, img_deps, img_rows, img_cols, multiloss):\n",
        "\n",
        "    window_deps = (img_deps/3)*2\n",
        "    window_rows = (img_rows/3)*2\n",
        "    window_cols = (img_cols/3)*2\n",
        "\n",
        "    current_test = imgs_test\n",
        "    x = current_test.shape[0]\n",
        "    y = current_test.shape[1]\n",
        "    z = current_test.shape[2]\n",
        "    score = np.zeros((x,y,z,2), dtype= 'float32')\n",
        "    score_num = np.zeros((x,y,z,2), dtype= 'int16')\n",
        "\n",
        "    count = 0\n",
        "    deplist = []\n",
        "    rowlist = []\n",
        "    collist = []\n",
        "    num = 0\n",
        "\n",
        "    box_test = np.zeros((batch,img_deps,img_rows,img_cols,1), dtype=\"float32\")\n",
        "    for deps in range(0,x-img_deps+window_deps,window_deps):\n",
        "        print (deps)\n",
        "        for rows in range(0, y-img_rows+window_rows, window_rows):\n",
        "            for cols in range(0,z-img_cols+window_cols,window_cols):\n",
        "                if deps>x-img_deps:\n",
        "                    deps = x-img_deps\n",
        "                elif rows > y-img_rows:\n",
        "                    rows = y-img_rows\n",
        "                elif cols>z-img_cols:\n",
        "                    cols = z-img_cols\n",
        "                elif deps>x-img_deps and rows > y - img_rows:\n",
        "                    deps = x - img_deps\n",
        "                    rows = y - img_rows\n",
        "                elif deps>x-img_deps and cols > z - img_cols:\n",
        "                    deps = x - img_deps\n",
        "                    cols = z - img_cols\n",
        "                elif rows>y-img_rows and cols > z-img_cols:\n",
        "                    rows = y - img_rows\n",
        "                    cols = z - img_cols\n",
        "                elif rows>y-img_rows and cols > z-img_cols and deps > x-img_deps:\n",
        "                    deps = x - img_deps\n",
        "                    rows = y - img_rows\n",
        "                    cols = z - img_cols\n",
        "                if count == batch:\n",
        "                    count = 0\n",
        "                    deplist = []\n",
        "                    rowlist = []\n",
        "                    collist = []\n",
        "                    box_test = np.zeros((batch, img_deps, img_rows, img_cols, 1), dtype=\"float32\")\n",
        "                patch_test = current_test[deps:deps+img_deps, rows:rows+img_rows, cols:cols+img_cols]\n",
        "                deplist.append(deps)\n",
        "                rowlist.append(rows)\n",
        "                collist.append(cols)\n",
        "                box_test[count,:,:,:,0] = patch_test\n",
        "                count += 1\n",
        "                del patch_test\n",
        "                if count == batch:\n",
        "                    num = num+1\n",
        "                    print ('num: ',num)\n",
        "                    print ('box:', box_test.shape)\n",
        "\n",
        "                    patch_test_mask = model.predict(box_test, verbose=0)\n",
        "\n",
        "                    if multiloss:\n",
        "                        patch_test_mask = patch_test_mask[2]\n",
        "                    patch_test_mask = K.softmax(patch_test_mask)\n",
        "                    patch_test_mask = K.eval(patch_test_mask)\n",
        "                    print ('predict finish')\n",
        "                    for i in range(batch):\n",
        "                        score[deplist[i]:deplist[i]+img_deps, rowlist[i]:rowlist[i]+img_rows, collist[i]:collist[i]+img_cols,:] += patch_test_mask[i]\n",
        "                        score_num[deplist[i]:deplist[i]+img_deps, rowlist[i]:rowlist[i]+img_rows, collist[i]:collist[i]+img_cols,:] += 1\n",
        "                    # print ('queue finish')\n",
        "                    del box_test, patch_test_mask, deplist, rowlist, collist\n",
        "    score = score / (score_num)\n",
        "    score2 = score[:,:,:,1]\n",
        "    return score2\n",
        "\n",
        "def get_binary_mask(score, id):\n",
        "    ## load affine\n",
        "    # label, header = load('/home/xmli/Data_gpu7/NewThresTestData/test-volume-' + str(id) + '.nii')\n",
        "    Segmask = GeneSeglivertumor(score)\n",
        "    Segmask = np.int16(Segmask)\n",
        "    return Segmask\n",
        "\n",
        "def GeneSeglivertumor(score):\n",
        "\n",
        "    score[score>=0.5] = 1\n",
        "    score[score<0.5] = 0\n",
        "    box = []\n",
        "    [liver_labels, num] = measure.label(score, return_num = True)\n",
        "    region = measure.regionprops(liver_labels)\n",
        "    for i in range(num):\n",
        "        box.append(region[i].area)\n",
        "    label_num = box.index(max(box))+1\n",
        "    liver_labels[liver_labels!=label_num] = 0\n",
        "    liver_labels[liver_labels==label_num] = 1\n",
        "\n",
        "    # labels = ndimage.binary_fill_holes(score).astype(int)\n",
        "    # labels = score\n",
        "    return liver_labels"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-rR5zaZYECB"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, ZeroPadding2D, concatenate, Lambda, ZeroPadding3D, add\n",
        "from keras.layers.core import Dropout, Activation\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv3D, UpSampling3D, AveragePooling3D\n",
        "from keras.layers.pooling import AveragePooling2D, MaxPooling2D, MaxPooling3D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def conv_block3d(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
        "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv3D, 3x3 Conv3D, and option dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            branch: layer index within each dense block\n",
        "            nb_filter: number of filters\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = '3dconv' + str(stage) + '_' + str(branch)\n",
        "    relu_name_base = '3drelu' + str(stage) + '_' + str(branch)\n",
        "\n",
        "    # 1x1 Convolution (Bottleneck layer)\n",
        "    inter_channel = nb_filter * 4\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name=conv_name_base+'_x1_bn', momentum=1.0, trainable=False)(x, training=False)\n",
        "    x = Scale(axis=4, name=conv_name_base+'_x1_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x1')(x)\n",
        "    x = Conv3D(inter_channel, (1, 1, 1), name=conv_name_base+'_x1', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # 3x3 Convolution\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name=conv_name_base+'_x2_bn', momentum=1.0, trainable=False)(x, training=False)\n",
        "    x = Scale(axis=4, name=conv_name_base+'_x2_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x2')(x)\n",
        "    x = ZeroPadding3D((1, 1, 1), name=conv_name_base+'_x2_zeropadding')(x)\n",
        "    x = Conv3D(nb_filter, (3, 3, 3), name=conv_name_base+'_x2', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "def dense_block3d(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True):\n",
        "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_layers: the number of layers of conv_block to append to the model.\n",
        "            nb_filter: number of filters\n",
        "            growth_rate: growth rate\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    concat_feat = x\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        branch = i+1\n",
        "        x = conv_block3d(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
        "        concat_feat = concatenate([concat_feat, x], axis=4, name='3dconcat_'+str(stage)+'_'+str(branch))\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    return concat_feat, nb_filter\n",
        "def transition_block3d(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
        "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_filter: number of filters\n",
        "            compression: calculated as 1 - reduction. Reduces the number of feature maps in the transition block.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = '3dconv' + str(stage) + '_blk'\n",
        "    relu_name_base = '3drelu' + str(stage) + '_blk'\n",
        "    pool_name_base = '3dpool' + str(stage)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name=conv_name_base+'_bn', momentum=1.0)(x, training=False)\n",
        "    x = Scale(axis=4, name=conv_name_base+'_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base)(x)\n",
        "    x = Conv3D(int(nb_filter * compression), (1, 1, 1), name=conv_name_base, use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = AveragePooling3D((2, 2, 1), strides=(2, 2, 1), name=pool_name_base)(x)\n",
        "\n",
        "    return x\n",
        "def DenseNet3D(img_input, nb_dense_block=4, growth_rate=32, nb_filter=96, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, classes=1000, weights_path=None):\n",
        "    '''Instantiate the DenseNet 161 architecture,\n",
        "        # Arguments\n",
        "            nb_dense_block: number of dense blocks to add to end\n",
        "            growth_rate: number of filters to add per dense block\n",
        "            nb_filter: initial number of filters\n",
        "            reduction: reduction factor of transition blocks.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            classes: optional number of classes to classify images\n",
        "            weights_path: path to pre-trained weights\n",
        "        # Returns\n",
        "            A Keras model instance.\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "\n",
        "    # compute compression factor\n",
        "    compression = 1.0 - reduction\n",
        "\n",
        "    # From architecture for ImageNet (Table 1 in the paper)\n",
        "    nb_filter = 96\n",
        "    nb_layers = [3, 4, 12, 8]  # For DenseNet-161\n",
        "    box = []\n",
        "    # Initial convolution\n",
        "    x = ZeroPadding3D((3, 3, 3), name='3dconv1_zeropadding')(img_input)\n",
        "    x = Conv3D(nb_filter, (7, 7, 7), strides=(2, 2, 2), name='3dconv1', use_bias=False)(x)\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name='3dconv1_bn')(x)\n",
        "    x = Scale(axis=4, name='3dconv1_scale')(x)\n",
        "    x = Activation('relu', name='3drelu1')(x)\n",
        "    box.append(x)\n",
        "    x = ZeroPadding3D((1, 1, 1), name='3dpool1_zeropadding')(x)\n",
        "    x = MaxPooling3D((3, 3, 3), strides=(2, 2, 2), name='3dpool1')(x)\n",
        "\n",
        "    # Add dense blocks\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        stage = block_idx + 2\n",
        "        x, nb_filter = dense_block3d(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
        "                                   weight_decay=weight_decay)\n",
        "        box.append(x)\n",
        "        # Add transition_block\n",
        "        x = transition_block3d(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate,\n",
        "                             weight_decay=weight_decay)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "    final_stage = stage + 1\n",
        "    x, nb_filter = dense_block3d(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
        "                               weight_decay=weight_decay)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name='3dconv' + str(final_stage) + '_blk_bn')(x)\n",
        "    x = Scale(axis=4, name='3dconv' + str(final_stage) + '_blk_scale')(x)\n",
        "    x = Activation('relu', name='3drelu' + str(final_stage) + '_blk')(x)\n",
        "    box.append(x)\n",
        "\n",
        "    up0 = UpSampling3D(size=(2, 2, 1))(x)\n",
        "    conv_up0 = Conv3D(504, (3, 3, 3), padding=\"same\", name=\"3dconv_up0\")(up0)\n",
        "    bn_up0 = BatchNormalization(name=\"3dbn_up0\")(conv_up0)\n",
        "    ac_up0 = Activation('relu', name='3dac_up0')(bn_up0)\n",
        "\n",
        "    up1 = UpSampling3D(size=(2, 2, 1))(ac_up0)\n",
        "    conv_up1 = Conv3D(224, (3, 3, 3), padding=\"same\", name=\"3dconv_up1\")(up1)\n",
        "    bn_up1 = BatchNormalization(name=\"3dbn_up1\")(conv_up1)\n",
        "    ac_up1 = Activation('relu', name='3dac_up1')(bn_up1)\n",
        "\n",
        "    up2 = UpSampling3D(size=(2, 2, 1))(ac_up1)\n",
        "    conv_up2 = Conv3D(192, (3, 3, 3), padding=\"same\", name=\"3dconv_up2\")(up2)\n",
        "    bn_up2 = BatchNormalization(name=\"3dbn_up2\")(conv_up2)\n",
        "    ac_up2 = Activation('relu', name='3dac_up2')(bn_up2)\n",
        "\n",
        "    up3 = UpSampling3D(size=(2, 2, 2))(ac_up2)\n",
        "    conv_up3 = Conv3D(96, (3, 3, 3), padding=\"same\", name=\"3dconv_up3\")(up3)\n",
        "    bn_up3 = BatchNormalization(name=\"3dbn_up3\")(conv_up3)\n",
        "    ac_up3 = Activation('relu', name='3dac_up3')(bn_up3)\n",
        "\n",
        "    up4 = UpSampling3D(size=(2, 2, 2))(ac_up3)\n",
        "    conv_up4 = Conv3D(64, (3, 3, 3), padding=\"same\", name=\"3dconv_up4\")(up4)\n",
        "    bn_up4 = BatchNormalization(name=\"3dbn_up4\")(conv_up4)\n",
        "    ac_up4 = Activation('relu', name='3dac_up4')(bn_up4)\n",
        "\n",
        "    x = Conv3D(3, (1, 1, 1), padding=\"same\", name='3dclassifer')(ac_up4)\n",
        "\n",
        "    return ac_up4, x\n",
        "\n",
        "\n",
        "\n",
        "def DenseUNet(img_input, nb_dense_block=4, growth_rate=48, nb_filter=96, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, classes=1000, weights_path=None):\n",
        "    '''Instantiate the DenseNet 161 architecture,\n",
        "        # Arguments\n",
        "            nb_dense_block: number of dense blocks to add to end\n",
        "            growth_rate: number of filters to add per dense block\n",
        "            nb_filter: initial number of filters\n",
        "            reduction: reduction factor of transition blocks.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            classes: optional number of classes to classify images\n",
        "            weights_path: path to pre-trained weights\n",
        "        # Returns\n",
        "            A Keras model instance.\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "    # compute compression factor\n",
        "    compression = 1.0 - reduction\n",
        "\n",
        "    # Handle Dimension Ordering for different backends\n",
        "    global concat_axis\n",
        "    concat_axis = 3\n",
        "\n",
        "    # From architecture for ImageNet (Table 1 in the paper)\n",
        "    nb_filter = 96\n",
        "    nb_layers = [6,12,36,24] # For DenseNet-161\n",
        "    box = []\n",
        "    # Initial convolution\n",
        "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n",
        "    x = Conv2D(nb_filter, (7, 7), strides=(2, 2), name='conv1', use_bias=False, trainable=True)(x)\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name='conv1_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name='conv1_scale')(x)\n",
        "    x = Activation('relu', name='relu1')(x)\n",
        "    box.append(x)\n",
        "    x = ZeroPadding2D((1, 1), name='pool1_zeropadding')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n",
        "\n",
        "    # Add dense blocks\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        stage = block_idx+2\n",
        "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        box.append(x)\n",
        "        # Add transition_block\n",
        "        x = transition_block(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "    final_stage = stage + 1\n",
        "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name='conv'+str(final_stage)+'_blk_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name='conv'+str(final_stage)+'_blk_scale')(x)\n",
        "    x = Activation('relu', name='relu'+str(final_stage)+'_blk')(x)\n",
        "    box.append(x)\n",
        "\n",
        "    up0 = UpSampling2D(size=(2,2))(x)\n",
        "    conv_up0 = Conv2D(768, (3, 3), padding=\"same\", name = \"conv_up0\", trainable=True)(up0)\n",
        "    bn_up0 = BatchNormalization(name = \"bn_up0\", momentum = 1, trainable=False)(conv_up0, training=False)\n",
        "    ac_up0 = Activation('relu', name='ac_up0')(bn_up0)\n",
        "\n",
        "    up1 = UpSampling2D(size=(2,2))(ac_up0)\n",
        "    conv_up1 = Conv2D(384, (3, 3), padding=\"same\", name = \"conv_up1\", trainable=True)(up1)\n",
        "    bn_up1 = BatchNormalization(name = \"bn_up1\", momentum = 1, trainable=False)(conv_up1, training=False)\n",
        "    ac_up1 = Activation('relu', name='ac_up1')(bn_up1)\n",
        "\n",
        "    up2 = UpSampling2D(size=(2,2))(ac_up1)\n",
        "    conv_up2 = Conv2D(96, (3, 3), padding=\"same\", name = \"conv_up2\", trainable=True)(up2)\n",
        "    bn_up2 = BatchNormalization(name = \"bn_up2\", momentum = 1, trainable=False)(conv_up2, training=False)\n",
        "    ac_up2 = Activation('relu', name='ac_up2')(bn_up2)\n",
        "\n",
        "    up3 = UpSampling2D(size=(2,2))(ac_up2)\n",
        "    conv_up3 = Conv2D(96, (3, 3), padding=\"same\", name = \"conv_up3\", trainable=True)(up3)\n",
        "    bn_up3 = BatchNormalization(name = \"bn_up3\", momentum = 1, trainable=False)(conv_up3, training=False)\n",
        "    ac_up3 = Activation('relu', name='ac_up3')(bn_up3)\n",
        "\n",
        "    up4 = UpSampling2D(size=(2, 2))(ac_up3)\n",
        "    conv_up4 = Conv2D(64, (3, 3), padding=\"same\", name=\"conv_up4\", trainable=True)(up4)\n",
        "    bn_up4 = BatchNormalization(name=\"bn_up4\", momentum = 1, trainable=False)(conv_up4, training=False)\n",
        "    ac_up4 = Activation('relu', name='ac_up4')(bn_up4)\n",
        "\n",
        "    x = Conv2D(3, (1,1), padding=\"same\", name='dense167classifer', trainable=True)(ac_up4)\n",
        "\n",
        "    return ac_up4, x\n",
        "\n",
        "def conv_block(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
        "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv2D, 3x3 Conv2D, and option dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            branch: layer index within each dense block\n",
        "            nb_filter: number of filters\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = 'conv' + str(stage) + '_' + str(branch)\n",
        "    relu_name_base = 'relu' + str(stage) + '_' + str(branch)\n",
        "\n",
        "    # 1x1 Convolution (Bottleneck layer)\n",
        "    inter_channel = nb_filter * 4\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name=conv_name_base+'_x1_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_x1_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x1')(x)\n",
        "    x = Conv2D(inter_channel, (1, 1), name=conv_name_base+'_x1', use_bias=False, trainable=True)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # 3x3 Convolution\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name=conv_name_base+'_x2_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_x2_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x2')(x)\n",
        "    x = ZeroPadding2D((1, 1), name=conv_name_base+'_x2_zeropadding')(x)\n",
        "    x = Conv2D(nb_filter, (3, 3), name=conv_name_base+'_x2', use_bias=False, trainable=True)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def transition_block(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
        "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_filter: number of filters\n",
        "            compression: calculated as 1 - reduction. Reduces the number of feature maps in the transition block.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = 'conv' + str(stage) + '_blk'\n",
        "    relu_name_base = 'relu' + str(stage) + '_blk'\n",
        "    pool_name_base = 'pool' + str(stage)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name=conv_name_base+'_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base)(x)\n",
        "    x = Conv2D(int(nb_filter * compression), (1, 1), name=conv_name_base, use_bias=False, trainable=True)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = AveragePooling2D((2, 2), strides=(2, 2), name=pool_name_base)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True):\n",
        "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_layers: the number of layers of conv_block to append to the model.\n",
        "            nb_filter: number of filters\n",
        "            growth_rate: growth rate\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    concat_feat = x\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        branch = i+1\n",
        "        x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
        "        concat_feat = concatenate([concat_feat, x], axis=concat_axis, name='concat_'+str(stage)+'_'+str(branch))\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    return concat_feat, nb_filter\n",
        "def slice(x, h1, h2):\n",
        "    \"\"\" Define a tensor slice function\n",
        "    \"\"\"\n",
        "    return x[:, :, :, h1:h2,:]\n",
        "def slice2d(x, h1, h2):\n",
        "\n",
        "    tmp = x[h1:h2,:,:,:]\n",
        "    tmp = tf.transpose(tmp, perm=[1, 2, 0, 3])\n",
        "    tmp = tf.expand_dims(tmp, 0)\n",
        "    return tmp\n",
        "\n",
        "def slice_last(x):\n",
        "\n",
        "    x = x[:,:,:,:,0]\n",
        "    return x\n",
        "def trans(x):\n",
        "\n",
        "    x = tf.transpose(x, perm=[0,3,1,2,4])\n",
        "    return x\n",
        "def trans_back(x):\n",
        "\n",
        "    x = tf.transpose(x, perm=[0,2,3,1,4])\n",
        "\n",
        "    return x\n",
        "def dense_rnn_net():\n",
        "\n",
        "    #  ************************3d volume input******************************************************************\n",
        "    img_input = Input(batch_shape=(1,512,512,8, 1), name='volumetric_data')\n",
        "\n",
        "    #  ************************(batch*d3cols)*2dvolume--2D DenseNet branch**************************************\n",
        "    input2d = Lambda(slice, arguments={'h1': 0, 'h2': 2})(img_input)\n",
        "    single = Lambda(slice, arguments={'h1':0, 'h2':1})(img_input)\n",
        "    input2d = concatenate([single, input2d], axis=3)\n",
        "    for i in range(8 - 2):\n",
        "        input2d_tmp = Lambda(slice, arguments={'h1': i, 'h2': i + 3})(img_input)\n",
        "        input2d = concatenate([input2d, input2d_tmp], axis=0)\n",
        "        if i == 8 - 3:\n",
        "            final1 = Lambda(slice, arguments={'h1': 8-2, 'h2': 8})(img_input)\n",
        "            final2 = Lambda(slice, arguments={'h1': 8-1, 'h2': 8})(img_input)\n",
        "            final = concatenate([final1, final2], axis=3)\n",
        "            input2d = concatenate([input2d, final], axis=0)\n",
        "    input2d = Lambda(slice_last)(input2d)\n",
        "\n",
        "    #  ******************************stack to 3D volumes *******************************************************\n",
        "    feature2d, classifer2d = DenseUNet(input2d, reduction=0.5)\n",
        "    res2d = Lambda(slice2d, arguments={'h1': 0, 'h2': 1})(classifer2d)\n",
        "    fea2d = Lambda(slice2d, arguments={'h1':0, 'h2':1})(feature2d)\n",
        "    for j in range(8 - 1):\n",
        "        score = Lambda(slice2d, arguments={'h1': j + 1, 'h2': j + 2})(classifer2d)\n",
        "        fea2d_slice = Lambda(slice2d, arguments={'h1': j + 1, 'h2': j + 2})(feature2d)\n",
        "        res2d = concatenate([res2d, score], axis=3)\n",
        "        fea2d = concatenate([fea2d, fea2d_slice], axis=3)\n",
        "\n",
        "    #  *************************** 3d DenseNet on 3D volume (concate with feature map )*********************************\n",
        "    res2d_input = Lambda(lambda x: x * 250)(res2d)\n",
        "    input3d_ori = Lambda(slice, arguments={'h1': 0, 'h2': 8})(img_input)\n",
        "    input3d = concatenate([input3d_ori, res2d_input], axis=4)\n",
        "    feature3d, classifer3d = DenseNet3D(input3d, reduction=0.5)\n",
        "\n",
        "    final = add([feature3d, fea2d])\n",
        "    final_conv = Conv3D(64, (3, 3, 3), padding=\"same\", name='fianl_conv')(final)\n",
        "    final_conv = Dropout(rate=0.3)(final_conv)\n",
        "    final_bn = BatchNormalization(name=\"final_bn\")(final_conv)\n",
        "    final_ac = Activation('relu', name='final_ac')(final_bn)\n",
        "    classifer = Conv3D(3, (1, 1, 1), padding=\"same\", name='2d3dclassifer')(final_ac)\n",
        "\n",
        "    model = Model( inputs = img_input,outputs = classifer, name='auto3d_residual_conv')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def dilated_resnet():\n",
        "    inputs = Input(batch_shape = (1, 512,512,8, 1))\n",
        "    conv1 = Conv3D(64, (3, 3, 3), padding = \"same\",kernel_initializer=\"normal\")(inputs)\n",
        "    bn0 = BatchNormalization()(conv1)\n",
        "    ac0 = Activation('relu')(bn0)\n",
        "    pool1 = MaxPooling3D(pool_size=(2, 2, 1))(ac0)\n",
        "\n",
        "    #  resudial block\n",
        "    conv2 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool1)\n",
        "    bn1 = BatchNormalization()(conv2)\n",
        "    ac1 = Activation('relu')(bn1)\n",
        "    conv3 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac1)\n",
        "    bn2 = BatchNormalization()(conv3)\n",
        "    pad1 = Conv3D(128, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool1)\n",
        "    BN1 = BatchNormalization()(pad1)\n",
        "    sumb1 = add([BN1, bn2])\n",
        "    res1  = Activation('relu')(sumb1)\n",
        "\n",
        "    pool2 = MaxPooling3D(pool_size=(2, 2, 1))(res1)\n",
        "\n",
        "    #  resudial block\n",
        "    conv4 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool2)\n",
        "    bn3 = BatchNormalization()(conv4)\n",
        "    ac2 = Activation('relu')(bn3)\n",
        "    conv5 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac2)\n",
        "    bn4 = BatchNormalization()(conv5)\n",
        "    pad2 = Conv3D(256, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool2)\n",
        "    BN2 = BatchNormalization()(pad2)\n",
        "    sumb2 = add([BN2, bn4])\n",
        "    res2  = Activation('relu')(sumb2)\n",
        "\n",
        "\n",
        "    pool3 = MaxPooling3D(pool_size=(2, 2, 1))(res2)\n",
        "\n",
        "    #  resudial block\n",
        "    conv6 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool3)\n",
        "    bn5 = BatchNormalization()(conv6)\n",
        "    ac3 = Activation('relu')(bn5)\n",
        "    conv7 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac3)\n",
        "    bn6 = BatchNormalization()(conv7)\n",
        "    pad3 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool3)\n",
        "    BN3 = BatchNormalization()(pad3)\n",
        "    sumb3 = add([BN3, bn6])\n",
        "    res3  = Activation('relu')(sumb3)\n",
        "\n",
        "    #  resudial deliated block\n",
        "    del1 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(res3)\n",
        "    delbn1 = BatchNormalization()(del1)\n",
        "    delac1 = Activation('relu')(delbn1)\n",
        "    del2 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(delac1)\n",
        "    delbn2 = BatchNormalization()(del2)\n",
        "    deladd1 = add([res3, delbn2])\n",
        "    delres  = Activation('relu')(deladd1)\n",
        "\n",
        "    pool4 = MaxPooling3D(pool_size=(2, 2, 1))(delres)\n",
        "\n",
        "    #  resudial block\n",
        "    conv6_4 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool4)\n",
        "    bn5_4 = BatchNormalization()(conv6_4)\n",
        "    ac3_4 = Activation('relu')(bn5_4)\n",
        "    conv7_4 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac3_4)\n",
        "    bn6_4 = BatchNormalization()(conv7_4)\n",
        "    pad3_4 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool4)\n",
        "    BN3_4 = BatchNormalization()(pad3_4)\n",
        "    sumb3_4 = add([BN3_4, bn6_4])\n",
        "    res3_4  = Activation('relu')(sumb3_4)\n",
        "\n",
        "    #  resudial deliated block\n",
        "    del3 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(res3_4)\n",
        "    delbn3 = BatchNormalization()(del3)\n",
        "    delac3 = Activation('relu')(delbn3)\n",
        "    del4 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(delac3)\n",
        "    delbn4 = BatchNormalization()(del4)\n",
        "    deladd2 = add([res3_4, delbn4])\n",
        "    delres2  = Activation('relu')(deladd2)\n",
        "\n",
        "\n",
        "    up0 = UpSampling3D(size=(2,2,1))(delres2)\n",
        "    pad4 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(delres)\n",
        "    BN4 = BatchNormalization()(pad4)\n",
        "    sumb4 = add([BN4, up0])\n",
        "\n",
        "    #  resudial block\n",
        "    conv8_1 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb4)\n",
        "    bn7_1 = BatchNormalization()(conv8_1)\n",
        "    ac4_1 = Activation('relu')(bn7_1)\n",
        "    conv9_1 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac4_1)\n",
        "    bn8_1 = BatchNormalization()(conv9_1)\n",
        "    pad5_1 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb4)\n",
        "    BN5_1 = BatchNormalization()(pad5_1)\n",
        "    sumb5_1 = add([BN5_1, bn8_1])\n",
        "    res4_1  = Activation('relu')(sumb5_1)\n",
        "\n",
        "    #  resudial deliated block\n",
        "    del5 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(res4_1)\n",
        "    delbn5 = BatchNormalization()(del5)\n",
        "    delac5 = Activation('relu')(delbn5)\n",
        "    del6 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(delac5)\n",
        "    delbn6 = BatchNormalization()(del6)\n",
        "    deladd3 = add([res4_1, delbn6])\n",
        "    delres3  = Activation('relu')(deladd3)\n",
        "\n",
        "    up0_1 = UpSampling3D(size=(2,2,1))(delres3)\n",
        "    pad4_1 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(res2)\n",
        "    BN4_1 = BatchNormalization()(pad4_1)\n",
        "    sumb4_1 = add([BN4_1, up0_1])\n",
        "\n",
        "    #  resudial block\n",
        "    conv8 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb4_1)\n",
        "    bn7 = BatchNormalization()(conv8)\n",
        "    ac4 = Activation('relu')(bn7)\n",
        "    conv9 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac4)\n",
        "    bn8 = BatchNormalization()(conv9)\n",
        "    pad5 = Conv3D(256, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb4_1)\n",
        "    BN5 = BatchNormalization()(pad5)\n",
        "    sumb5 = add([BN5, bn8])\n",
        "    res4  = Activation('relu')(sumb5)\n",
        "\n",
        "    up1 = UpSampling3D(size=(2, 2, 1))(res4)\n",
        "    pad6 = Conv3D(256, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(res1)\n",
        "    BN6 = BatchNormalization()(pad6)\n",
        "    sumb6 = add([BN6, up1])\n",
        "\n",
        "    #  resudial block\n",
        "    conv10 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb6)\n",
        "    bn9 = BatchNormalization()(conv10)\n",
        "    ac5 = Activation('relu')(bn9)\n",
        "    conv11 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac5)\n",
        "    bn10 = BatchNormalization()(conv11)\n",
        "    pad7 = Conv3D(128, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb6)\n",
        "    BN7 = BatchNormalization()(pad7)\n",
        "    sumb7 = add([BN7, bn10])\n",
        "    res5  = Activation('relu')(sumb7)\n",
        "\n",
        "    up2 = UpSampling3D(size=(2, 2, 1))(res5)\n",
        "    pad8 = Conv3D(128, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(ac0)\n",
        "    BN8 = BatchNormalization()(pad8)\n",
        "    sumb8 = add([BN8, up2])\n",
        "\n",
        "    #  resudial block\n",
        "    conv12 = Conv3D(64, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb8)\n",
        "    bn11= BatchNormalization()(conv12)\n",
        "    ac6 = Activation('relu')(bn11)\n",
        "    conv13 = Conv3D(64, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac6)\n",
        "    bn12 = BatchNormalization()(conv13)\n",
        "    pad9 = Conv3D(64, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb8)\n",
        "    BN9 = BatchNormalization()(pad9)\n",
        "    sumb9 = add([BN9, bn12])\n",
        "    res6 = Activation('relu')(sumb9)\n",
        "\n",
        "    output3 = Conv3D(2, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(res6)\n",
        "\n",
        "    # print (output3)\n",
        "\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[output3])\n",
        "\n",
        "\n",
        "\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kNmKoRmqj2S"
      },
      "source": [
        "\n",
        "#test.py\n",
        "\n",
        "from __future__ import print_function\n",
        "# import sys\n",
        "# sys.path.insert(0,'Keras-2.0.8')\n",
        "from keras import backend as K\n",
        "import os\n",
        "import numpy as np\n",
        "from medpy.io import load,save\n",
        "from keras.optimizers import SGD\n",
        "from scipy import ndimage\n",
        "from skimage import measure\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_bnFaRCDGSn"
      },
      "source": [
        "def dice(p,t):\n",
        "    i=np.sum(t*p)\n",
        "    u=np.sum(t)+np.sum(p)\n",
        "    d=(2*i+1)/(u+1)\n",
        "    return d"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5czjtZprFQ2q"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFMoEH4ZqSMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc415f5b-267c-4940-e966-c001c6c59fae"
      },
      "source": [
        "total=[]\n",
        "for i in glob.glob('/content/*_livertumor.npy'):\n",
        "        number=i.split('/')[-1].split('_')[0]\n",
        "        path='/content/'+number+'_liver.npy'\n",
        "        id =  path.split('/')[-1].split('_')[0][8:]\n",
        "        print('-' * 30)\n",
        "        print('Loading model and preprocessing test data...' + str(id))\n",
        "        print('-' * 30)\n",
        "        model = dense_rnn_net()\n",
        "        model.load_weights('/content/gdrive/MyDrive/0_TL.hdf5')\n",
        "\n",
        "        #  load data\n",
        "        img_test = np.load(path.replace('_liver','_input'))\n",
        "        \n",
        "        \n",
        "        img_test=np.swapaxes(img_test,0,1)\n",
        "        img_test=np.swapaxes(img_test,1,2)\n",
        "        \n",
        "        \n",
        "        img_test -= 48\n",
        "\n",
        "        #  load liver mask\n",
        "        mask = np.load(path)\n",
        "        \n",
        "        \n",
        "        mask=np.swapaxes(mask,0,1)\n",
        "        mask=np.swapaxes(mask,1,2)\n",
        "\n",
        "\n",
        "        mask[mask==255]=1\n",
        "        mask = ndimage.binary_dilation(mask, iterations=1).astype(mask.dtype)\n",
        "        index = np.where(mask==1)\n",
        "        mini = np.min(index, axis = -1)\n",
        "        maxi = np.max(index, axis = -1)\n",
        "\n",
        "        print('-' * 30)\n",
        "        print('Predicting masks on test data...' + str(id))\n",
        "        print('-' * 30)\n",
        "        score1, score2 =  predict_tumor_inwindow(model, img_test, 3, mini, maxi)\n",
        "\n",
        "        result1 = score1\n",
        "        result2 = score2\n",
        "        result1[result1>=0.5]=1\n",
        "        result1[result1<0.5]=0\n",
        "        result2[result2>=0.9]=1\n",
        "        result2[result2<0.9]=0\n",
        "        result1[result2==1]=1\n",
        "\n",
        "        print('-' * 30)\n",
        "        print('Postprocessing on mask ...' + str(id))\n",
        "        print('-' * 30)\n",
        "\n",
        "        #  preserve the largest liver\n",
        "        Segmask = result2\n",
        "        box=[]\n",
        "        [liver_res, num] = measure.label(result1, return_num=True)\n",
        "        region = measure.regionprops(liver_res)\n",
        "        for i in range(num):\n",
        "            box.append(region[i].area)\n",
        "        label_num = box.index(max(box)) + 1\n",
        "        liver_res[liver_res != label_num] = 0\n",
        "        liver_res[liver_res == label_num] = 1\n",
        "\n",
        "        #  preserve the largest liver\n",
        "        mask = ndimage.binary_dilation(mask, iterations=1).astype(mask.dtype)\n",
        "        box = []\n",
        "        [liver_labels, num] = measure.label(mask, return_num=True)\n",
        "        region = measure.regionprops(liver_labels)\n",
        "        for i in range(num):\n",
        "            box.append(region[i].area)\n",
        "        label_num = box.index(max(box)) + 1\n",
        "        liver_labels[liver_labels != label_num] = 0\n",
        "        liver_labels[liver_labels == label_num] = 1\n",
        "        liver_labels = ndimage.binary_fill_holes(liver_labels).astype(int)\n",
        "\n",
        "\n",
        "        #  preserve tumor within ' largest liver' only\n",
        "        Segmask = Segmask * liver_labels\n",
        "        Segmask = ndimage.binary_fill_holes(Segmask).astype(int)\n",
        "        Segmask = np.array(Segmask,dtype='uint8')\n",
        "        liver_res = np.array(liver_res, dtype='uint8')\n",
        "        liver_res = ndimage.binary_fill_holes(liver_res).astype(int)\n",
        "        liver_res[Segmask == 1] = 2\n",
        "        a= np.array(liver_res, dtype='uint8')\n",
        "        b=np.load( '/content/'+number+'_livertumor.npy')\n",
        "        a[a==1]=0\n",
        "        a[a==2]=1\n",
        "        b=np.swapaxes(b,0,1)\n",
        "        b=np.swapaxes(b,1,2)\n",
        "        b[b==255]=1\n",
        "        total.append(dice(a.astype('float64'),b.astype('float64')))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Loading model and preprocessing test data...1.18\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/31 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Predicting masks on test data...1.18\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31/31 [01:23<00:00,  2.68s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Postprocessing on mask ...1.18\n",
            "------------------------------\n",
            "------------------------------\n",
            "Loading model and preprocessing test data...1.10\n",
            "------------------------------\n",
            "------------------------------\n",
            "Predicting masks on test data...1.10\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:08<00:00,  1.37s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Postprocessing on mask ...1.10\n",
            "------------------------------\n",
            "------------------------------\n",
            "Loading model and preprocessing test data...1.9\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Predicting masks on test data...1.9\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:10<00:00,  1.40s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Postprocessing on mask ...1.9\n",
            "------------------------------\n",
            "------------------------------\n",
            "Loading model and preprocessing test data...1.2\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/62 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Predicting masks on test data...1.2\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 62/62 [01:19<00:00,  1.29s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Postprocessing on mask ...1.2\n",
            "------------------------------\n",
            "------------------------------\n",
            "Loading model and preprocessing test data...1.16\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/58 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Predicting masks on test data...1.16\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 58/58 [01:15<00:00,  1.30s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Postprocessing on mask ...1.16\n",
            "------------------------------\n",
            "------------------------------\n",
            "Loading model and preprocessing test data...1.13\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Predicting masks on test data...1.13\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [01:08<00:00,  1.43s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Postprocessing on mask ...1.13\n",
            "------------------------------\n",
            "------------------------------\n",
            "Loading model and preprocessing test data...1.12\n",
            "------------------------------\n",
            "------------------------------\n",
            "Predicting masks on test data...1.12\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [02:09<00:00,  1.04s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Postprocessing on mask ...1.12\n",
            "------------------------------\n",
            "------------------------------\n",
            "Loading model and preprocessing test data...1.15\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/60 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Predicting masks on test data...1.15\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 60/60 [01:17<00:00,  1.29s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Postprocessing on mask ...1.15\n",
            "------------------------------\n",
            "------------------------------\n",
            "Loading model and preprocessing test data...1.3\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/67 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Predicting masks on test data...1.3\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:24<00:00,  1.26s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Postprocessing on mask ...1.3\n",
            "------------------------------\n",
            "------------------------------\n",
            "Loading model and preprocessing test data...1.6\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/49 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Predicting masks on test data...1.6\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 49/49 [01:08<00:00,  1.40s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Postprocessing on mask ...1.6\n",
            "------------------------------\n",
            "------------------------------\n",
            "Loading model and preprocessing test data...1.4\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/36 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Predicting masks on test data...1.4\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 36/36 [00:59<00:00,  1.65s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Postprocessing on mask ...1.4\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJpM7nv8Vojk",
        "outputId": "d9f8fb2f-4acf-49e9-ab6a-ac7a94fdcaa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.mean(total)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46846312506024107"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR2qHm7yMxIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1af0c5-ba54-4b39-8b9f-2636aaf04b18"
      },
      "source": [
        "#6\n",
        "np.mean([0.76,0.808,0.223,0.804,0.067,0.789,0.623,0.403,0.713,0.762,0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5410909090909091"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAkaaiwcFs3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc627135-6f42-4eaf-e6b5-7fa30ca1ab91"
      },
      "source": [
        "        path='/content/3Dircadb1.10_liver.npy'\n",
        "        id =  path.split('/')[-1].split('_')[0][8:]\n",
        "        print('-' * 30)\n",
        "        print('Loading model and preprocessing test data...' + str(id))\n",
        "        print('-' * 30)\n",
        "        model = dense_rnn_net()\n",
        "        model.load_weights('/content/gdrive/MyDrive/0_TL.hdf5')\n",
        "\n",
        "        #  load data\n",
        "        img_test = np.load(path.replace('_liver','_input'))\n",
        "        \n",
        "        \n",
        "        img_test=np.swapaxes(img_test,0,1)\n",
        "        img_test=np.swapaxes(img_test,1,2)\n",
        "        \n",
        "        \n",
        "        img_test -= 48\n",
        "\n",
        "        #  load liver mask\n",
        "        mask = np.load(path)\n",
        "        \n",
        "        \n",
        "        mask=np.swapaxes(mask,0,1)\n",
        "        mask=np.swapaxes(mask,1,2)\n",
        "\n",
        "\n",
        "        mask[mask==255]=1\n",
        "        mask = ndimage.binary_dilation(mask, iterations=1).astype(mask.dtype)\n",
        "        index = np.where(mask==1)\n",
        "        mini = np.min(index, axis = -1)\n",
        "        maxi = np.max(index, axis = -1)\n",
        "\n",
        "        print('-' * 30)\n",
        "        print('Predicting masks on test data...' + str(id))\n",
        "        print('-' * 30)\n",
        "        score1, score2 =  predict_tumor_inwindow(model, img_test, 3, mini, maxi)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Loading model and preprocessing test data...1.10\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Predicting masks on test data...1.10\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [01:18<00:00,  1.56s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1eIHYt9FuHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d4d760-a424-4e8b-bb90-2fe16b9c3966"
      },
      "source": [
        "        K.clear_session()\n",
        "\n",
        "        result1 = score1\n",
        "        result2 = score2\n",
        "        result1[result1>=0.5]=1\n",
        "        result1[result1<0.5]=0\n",
        "        result2[result2>=0.9]=1\n",
        "        result2[result2<0.9]=0\n",
        "        result1[result2==1]=1\n",
        "\n",
        "        print('-' * 30)\n",
        "        print('Postprocessing on mask ...' + str(id))\n",
        "        print('-' * 30)\n",
        "\n",
        "        #  preserve the largest liver\n",
        "        Segmask = result2\n",
        "        box=[]\n",
        "        [liver_res, num] = measure.label(result1, return_num=True)\n",
        "        region = measure.regionprops(liver_res)\n",
        "        for i in range(num):\n",
        "            box.append(region[i].area)\n",
        "        label_num = box.index(max(box)) + 1\n",
        "        liver_res[liver_res != label_num] = 0\n",
        "        liver_res[liver_res == label_num] = 1\n",
        "\n",
        "        #  preserve the largest liver\n",
        "        mask = ndimage.binary_dilation(mask, iterations=1).astype(mask.dtype)\n",
        "        box = []\n",
        "        [liver_labels, num] = measure.label(mask, return_num=True)\n",
        "        region = measure.regionprops(liver_labels)\n",
        "        for i in range(num):\n",
        "            box.append(region[i].area)\n",
        "        label_num = box.index(max(box)) + 1\n",
        "        liver_labels[liver_labels != label_num] = 0\n",
        "        liver_labels[liver_labels == label_num] = 1\n",
        "        liver_labels = ndimage.binary_fill_holes(liver_labels).astype(int)\n",
        "\n",
        "\n",
        "        #  preserve tumor within ' largest liver' only\n",
        "        Segmask = Segmask * liver_labels\n",
        "        Segmask = ndimage.binary_fill_holes(Segmask).astype(int)\n",
        "        Segmask = np.array(Segmask,dtype='uint8')\n",
        "        liver_res = np.array(liver_res, dtype='uint8')\n",
        "        liver_res = ndimage.binary_fill_holes(liver_res).astype(int)\n",
        "        liver_res[Segmask == 1] = 2\n",
        "        liver_res = np.array(liver_res, dtype='uint8')\n",
        "        np.save('/content/' + 'test-segmentation-' + str(id) + '.npy',liver_res,)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Postprocessing on mask ...1.10\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d02FiDbTFuFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c76c0b-b278-41b2-ec04-9edcdd54ef83"
      },
      "source": [
        "a=np.load('/content/test-segmentation-1.10.npy')\n",
        "\n",
        "b=np.load( '/content/3Dircadb1.10_livertumor.npy')\n",
        "\n",
        "b=np.swapaxes(b,0,1)\n",
        "b=np.swapaxes(b,1,2)\n",
        "\n",
        "a[a==1]=0\n",
        "a[a==2]=1\n",
        "b[b==255]=1\n",
        "dice(a.astype('float64'),b.astype('float64'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7622755140795399"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFUe6jN_RLD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db96fb18-d225-4d84-c1da-95837ae04af6"
      },
      "source": [
        "#5\n",
        "dice(a.astype('float64'),b.astype('float64'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8192903995229577"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXxeQqhRFijr"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKqCjToJF7Ew",
        "outputId": "8e0a2c16-6567-4709-f171-048f746d2b36"
      },
      "source": [
        "#4\n",
        "dice(a.astype('float64'),b.astype('float64'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8170236098567927"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3R1f6EC0EpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f8bbbd8-d882-4f53-fcdc-3f87482d0be4"
      },
      "source": [
        "#3\n",
        "dice(a.astype('float64'),b.astype('float64'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7973809523809524"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9viC6bNlMhoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d2b65e-6a42-49ca-8118-a9ac5ef18f35"
      },
      "source": [
        "#2\n",
        "dice(a.astype('float64'),b.astype('float64'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7910407453667758"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmQKidrDvI-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168d657f-3231-4dde-f241-0415154e6c1d"
      },
      "source": [
        "#best\n",
        "dice(a.astype('float64'),b.astype('float64'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8519299864851502"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO5d0Sg7uviy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebbb2b72-6165-4fb4-864c-28dd8dc1426a"
      },
      "source": [
        "#1\n",
        "dice(a.astype('float64'),b.astype('float64'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.173867999268694"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-4Wn4TLAtoy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}