{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hybrid_end2end.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/gan_segmentation_FE/blob/main/hybrid_end2end.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26e933e-44a3-4cae-a845-bf76e047a0f0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/gdrive/My Drive/Training_Batch1.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNo_tSo_g1Cx"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "for i in range(10,28):\n",
        "    os.remove( '/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/segmentation-'+str(i)+'.nii')\n",
        "    os.remove( '/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/volume-'+str(i)+'.nii')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "584885c3-57e6-49ba-d93e-7c6ab1b32b65"
      },
      "source": [
        "pip install medpy"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting medpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/70/c1fd5dd60242eee81774696ea7ba4caafac2bad8f028bba94b1af83777d7/MedPy-0.4.0.tar.gz (151kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 20.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 51kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 71kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 81kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 102kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 112kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 122kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 133kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 143kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from medpy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from medpy) (1.19.5)\n",
            "Collecting SimpleITK>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/6b/85df5eb3a8059b23a53a9f224476e75473f9bcc0a8583ed1a9c34619f372/SimpleITK-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (47.4MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4MB 111kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: medpy\n",
            "  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for medpy: filename=MedPy-0.4.0-cp37-cp37m-linux_x86_64.whl size=754479 sha256=6d60eca7d2bafc3451fdc205a280b849423f015d63ee8f8187d80bfa7d0075d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/c9/9c/2c6281c7a72b9fb1570862a4f028af7ce38405008354fbf870\n",
            "Successfully built medpy\n",
            "Installing collected packages: SimpleITK, medpy\n",
            "Successfully installed SimpleITK-2.0.2 medpy-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ1viiyHgWeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "554345a6-2df1-4413-8794-63993e36a47e"
      },
      "source": [
        "!git clone https://github.com/xmengli999/H-DenseUNet"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'H-DenseUNet'...\n",
            "remote: Enumerating objects: 712, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 712 (delta 16), reused 0 (delta 0), pack-reused 687\u001b[K\n",
            "Receiving objects: 100% (712/712), 13.77 MiB | 24.62 MiB/s, done.\n",
            "Resolving deltas: 100% (245/245), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJU3Lj-n-TRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461637b5-09ff-493b-e900-9ad2a60277c0"
      },
      "source": [
        "cd /content/H-DenseUNet"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/H-DenseUNet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a4c1b6a-f09d-4645-f88d-19cd81adf30b"
      },
      "source": [
        "pip install -r requirements.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting absl-py==0.1.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/b8/3dafc45f20a817ab9f042302646bcbe6f7e26e8a760871a85637e53a35ec/absl-py-0.1.10.tar.gz (79kB)\n",
            "\r\u001b[K     |████                            | 10kB 20.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 20kB 27.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 30kB 20.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 40kB 16.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 51kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 61kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 4.9MB/s \n",
            "\u001b[?25hCollecting backports-abc==0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/56/6f3ac1b816d0cd8994e83d0c4e55bc64567532f7dc543378bd87f81cebc7/backports_abc-0.5-py2.py3-none-any.whl\n",
            "Collecting backports.functools-lru-cache==1.5\n",
            "  Downloading https://files.pythonhosted.org/packages/03/8e/2424c0e65c4a066e28f539364deee49b6451f8fcd4f718fefa50cc3dcf48/backports.functools_lru_cache-1.5-py2.py3-none-any.whl\n",
            "Collecting backports.weakref==1.0rc1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/f7/ae34b6818b603e264f26fe7db2bd07850ce331ce2fde74b266d61f4a2d87/backports.weakref-1.0rc1-py3-none-any.whl\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting bokeh==0.12.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/67/82f17df7d1f4b9e81c9263c1a1dc3897c43cf5a9461872f9054517331f77/bokeh-0.12.15.tar.gz (13.6MB)\n",
            "\u001b[K     |████████████████████████████████| 13.6MB 22.1MB/s \n",
            "\u001b[?25hCollecting certifi==2018.1.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/53/0a5562e2b96749e99a3d55d8c7df91c9e4d8c39a9da1f1a49ac9e4f4b39f/certifi-2018.1.18-py2.py3-none-any.whl (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 61.8MB/s \n",
            "\u001b[?25hCollecting cffi==1.11.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/7b/d1014289d0578c3522b2798b9cb87c65e5b36798bd3ae68a75fa1fe09e78/cffi-1.11.5-cp37-cp37m-manylinux1_x86_64.whl (421kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 63.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (3.0.4)\n",
            "Collecting click==6.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.5MB/s \n",
            "\u001b[?25hCollecting cloudpickle==0.5.2\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/18/514b557c4d8d4ada1f0454ad06c845454ad438fd5c5e0039ba51d6b032fe/cloudpickle-0.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (0.10.0)\n",
            "Collecting cytoolz==0.9.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/f4/9728ba01ccb2f55df9a5af029b48ba0aaca1081bbd7823ea2ee223ba7a42/cytoolz-0.9.0.1.tar.gz (443kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 40.5MB/s \n",
            "\u001b[?25hCollecting dask==0.17.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/f1/700c604af030d9b256a6590adf56cadd174c30c8ac6f555daf0e3023d294/dask-0.17.2-py2.py3-none-any.whl (582kB)\n",
            "\u001b[K     |████████████████████████████████| 583kB 40.4MB/s \n",
            "\u001b[?25hCollecting decorator==4.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/bb/a24838832ba35baf52f32ab1a49b906b5f82fb7c76b2f6a7e35e140bac30/decorator-4.3.0-py2.py3-none-any.whl\n",
            "Collecting distributed==1.21.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/e8/7453e61bbee910aa91936743d6782a2108c28d9945f5f61cf801b485b5fa/distributed-1.21.6-py2.py3-none-any.whl (458kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 54.5MB/s \n",
            "\u001b[?25hCollecting dominate==2.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/43/b2/3b7d67dd59dab93ae08569384b254323516e8868b453eea5614a53835baf/dominate-2.3.1.tar.gz\n",
            "Collecting easydict==1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/a1/dfe10522accfc2f6f27bee6144ac20f4852d6d177ec9dce1152b989d5228/easydict-1.4.tar.gz\n",
            "Collecting enum34==1.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/af/42/cb9355df32c69b553e72a2e28daee25d1611d2c0d9c272aa1d34204205b2/enum34-1.1.6-py3-none-any.whl\n",
            "Collecting funcsigs==1.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Collecting functools32==3.2.3.post2\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/60/6ac26ad05857c601308d8fb9e87fa36d0ebf889423f47c3502ef034365db/functools32-3.2.3-2.tar.gz\n",
            "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW_cCm5qClpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf3f584-4215-4aa4-d1d2-1827b74f812f"
      },
      "source": [
        "cd .."
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6DXNLU4mLei"
      },
      "source": [
        "os.mkdir('data')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm4CwPSqp7ru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769071d7-3853-4a46-ff0d-c7848a1aede9"
      },
      "source": [
        "\n",
        "#Preprocessing.py changed image paths and took 5 images from both train and test also indentation error\n",
        "\n",
        "from medpy.io import load, save\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def proprecessing(image_path, save_folder):\n",
        "\n",
        "    if not os.path.exists(\"data/\"+save_folder):\n",
        "        os.mkdir(\"data/\"+save_folder)\n",
        "    filelist = os.listdir(image_path)\n",
        "    filelist = [item for item in filelist if 'volume' in item]\n",
        "    for file in filelist:\n",
        "      img, img_header = load(image_path+file)\n",
        "      img[img < -200] = -200\n",
        "      img[img > 250] = 250\n",
        "      img = np.array(img, dtype='float32')\n",
        "      print (\"Saving image \"+file)\n",
        "      save(img, \"./data/\" + save_folder + file)\n",
        "\n",
        "def generate_livertxt(image_path, save_folder):\n",
        "    if not os.path.exists(\"data/\"+save_folder):\n",
        "        os.mkdir(\"data/\"+save_folder)\n",
        "\n",
        "    # Generate Livertxt\n",
        "    if not os.path.exists(\"data/\"+save_folder+'LiverPixels'):\n",
        "        os.mkdir(\"data/\"+save_folder+'LiverPixels')\n",
        "\n",
        "    for i in range(0,10):\n",
        "        livertumor, header = load(image_path+'segmentation-'+str(i)+'.nii')\n",
        "        f = open('data/' +save_folder+'/LiverPixels/liver_' + str(i) + '.txt', 'w')\n",
        "        index = np.where(livertumor==1)\n",
        "        x = index[0]\n",
        "        y = index[1]\n",
        "        z = index[2]\n",
        "        np.savetxt(f, np.c_[x,y,z], fmt=\"%d\")\n",
        "\t\n",
        "        f.write(\"\\n\")\n",
        "        f.close()\n",
        "\n",
        "def generate_tumortxt(image_path, save_folder):\n",
        "    if not os.path.exists(\"data/\"+save_folder):\n",
        "        os.mkdir(\"data/\"+save_folder)\n",
        "\n",
        "    # Generate Livertxt\n",
        "    if not os.path.exists(\"data/\"+save_folder+'TumorPixels'):\n",
        "        os.mkdir(\"data/\"+save_folder+'TumorPixels')\n",
        "\n",
        "    for i in range(0,10):\n",
        "        livertumor, header = load(image_path+'segmentation-'+str(i)+'.nii')\n",
        "        f = open(\"data/\"+save_folder+\"/TumorPixels/tumor_\"+str(i)+'.txt','w')\n",
        "        index = np.where(livertumor==2)\n",
        "\n",
        "        x = index[0]\n",
        "        y = index[1]\n",
        "        z = index[2]\n",
        "\n",
        "        np.savetxt(f,np.c_[x,y,z],fmt=\"%d\")\n",
        "\n",
        "        f.write(\"\\n\")\n",
        "        f.close()\n",
        "\n",
        "def generate_txt(image_path, save_folder):\n",
        "    if not os.path.exists(\"data/\"+save_folder):\n",
        "        os.mkdir(\"data/\"+save_folder)\n",
        "\n",
        "    # Generate Livertxt\n",
        "    if not os.path.exists(\"data/\"+save_folder+'LiverBox'):\n",
        "        os.mkdir(\"data/\"+save_folder+'LiverBox')\n",
        "    for i in range(0,10):\n",
        "        values = np.loadtxt('data/myTrainingDataTxt/LiverPixels/liver_' + str(i) + '.txt', delimiter=' ', usecols=[0, 1, 2])\n",
        "        a = np.min(values, axis=0)\n",
        "        b = np.max(values, axis=0)\n",
        "        box = np.append(a,b, axis=0)\n",
        "        np.savetxt('data/myTrainingDataTxt/LiverBox/box_'+str(i)+'.txt', box,fmt='%d')\n",
        "\n",
        "proprecessing(image_path='/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/', save_folder='myTrainingData/')\n",
        "\n",
        "# proprecessing(image_path='/content/', save_folder='myTestData/')\n",
        "print (\"Generate liver txt \")\n",
        "\n",
        "generate_livertxt(image_path='/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/', save_folder='myTrainingDataTxt/')\n",
        "print (\"Generate tumor txt\")\n",
        "\n",
        "generate_tumortxt(image_path='/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/', save_folder='myTrainingDataTxt/')\n",
        "print (\"Generate liver box \")\n",
        "\n",
        "generate_txt(image_path='/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/', save_folder='myTrainingDataTxt/')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving image volume-8.nii\n",
            "Saving image volume-3.nii\n",
            "Saving image volume-4.nii\n",
            "Saving image volume-2.nii\n",
            "Saving image volume-7.nii\n",
            "Saving image volume-0.nii\n",
            "Saving image volume-1.nii\n",
            "Saving image volume-6.nii\n",
            "Saving image volume-9.nii\n",
            "Saving image volume-5.nii\n",
            "Generate liver txt \n",
            "Generate tumor txt\n",
            "Generate liver box \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0PuY2ltp9TB"
      },
      "source": [
        "#custom layers\n",
        "\n",
        "from keras.layers import Layer, InputSpec\n",
        "try:\n",
        "    from keras import initializations\n",
        "except ImportError:\n",
        "    from keras import initializers as initializations\n",
        "import keras.backend as K\n",
        "\n",
        "class Scale(Layer):\n",
        "    '''Custom Layer for DenseNet used for BatchNormalization.\n",
        "    \n",
        "    Learns a set of weights and biases used for scaling the input data.\n",
        "    the output consists simply in an element-wise multiplication of the input\n",
        "    and a sum of a set of constants:\n",
        "        out = in * gamma + beta,\n",
        "    where 'gamma' and 'beta' are the weights and biases larned.\n",
        "    # Arguments\n",
        "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
        "            if your input tensor has shape (samples, channels, rows, cols),\n",
        "            set axis to 1 to normalize per feature map (channels axis).\n",
        "        momentum: momentum in the computation of the\n",
        "            exponential average of the mean and standard deviation\n",
        "            of the data, for feature-wise normalization.\n",
        "        weights: Initialization weights.\n",
        "            List of 2 Numpy arrays, with shapes:\n",
        "            `[(input_shape,), (input_shape,)]`\n",
        "        beta_init: name of initialization function for shift parameter\n",
        "            (see [initializations](../initializations.md)), or alternatively,\n",
        "            Theano/TensorFlow function to use for weights initialization.\n",
        "            This parameter is only relevant if you don't pass a `weights` argument.\n",
        "        gamma_init: name of initialization function for scale parameter (see\n",
        "            [initializations](../initializations.md)), or alternatively,\n",
        "            Theano/TensorFlow function to use for weights initialization.\n",
        "            This parameter is only relevant if you don't pass a `weights` argument.\n",
        "    '''\n",
        "    def __init__(self, weights=None, axis=-1, momentum = 0.9, beta_init='zero', gamma_init='one', **kwargs):\n",
        "        self.momentum = momentum\n",
        "        self.axis = axis\n",
        "        self.beta_init = initializations.get(beta_init)\n",
        "        self.gamma_init = initializations.get(gamma_init)\n",
        "        self.initial_weights = weights\n",
        "        super(Scale, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        shape = (int(input_shape[self.axis]),)\n",
        "\n",
        "        # Tensorflow >= 1.0.0 compatibility\n",
        "        self.gamma = K.variable(self.gamma_init(shape), name='{}_gamma'.format(self.name))\n",
        "        self.beta = K.variable(self.beta_init(shape), name='{}_beta'.format(self.name))\n",
        "        #self.gamma = self.gamma_init(shape, name='{}_gamma'.format(self.name))\n",
        "        #self.beta = self.beta_init(shape, name='{}_beta'.format(self.name))\n",
        "        self.trainable_weights_ = [self.gamma, self.beta]\n",
        "\n",
        "        if self.initial_weights is not None:\n",
        "            self.set_weights(self.initial_weights)\n",
        "            del self.initial_weights\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        input_shape = self.input_spec[0].shape\n",
        "        broadcast_shape = [1] * len(input_shape)\n",
        "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
        "\n",
        "        out = K.reshape(self.gamma, broadcast_shape) * x + K.reshape(self.beta, broadcast_shape)\n",
        "        return out\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\"momentum\": self.momentum, \"axis\": self.axis}\n",
        "        base_config = super(Scale, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOdQm2sOWzCv"
      },
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "# sys.path.insert(0,'/home/xmli/livertumor_xmli/Keras-2.0.8')\n",
        "# sys.path.insert(0,'/home/xmli/livertumor_xmli/mylib')\n",
        "# sys.path.insert(0,'/research/pheng/xmli/livertumor/Keras-2.0.8')\n",
        "# sys.path.insert(0,'/research/pheng/xmli/livertumor/mylib')\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "import random\n",
        "from medpy.io import load\n",
        "import numpy as np\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, ZeroPadding2D, concatenate, add\n",
        "from keras.layers.core import Dropout, Activation\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.layers.pooling import AveragePooling2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import keras.backend as K\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from skimage.transform import resize\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "path = './result_train_denseU167_fast_new/'\n",
        "batch_size = 1\n",
        "img_deps = 512\n",
        "img_rows = 512\n",
        "img_cols = 3\n",
        "std = 37\n",
        "thread_num = 14\n",
        "txtfile = 'myTrainingDataTxt'\n",
        "mean = 48\n",
        "\n",
        "liverlist = [32,34,38,41,47,87,89,91,105,106,114,115,119]\n",
        "DataList = [\"/home/xmli/gpu7_xmli/\"]\n",
        "def load_seq_crop_data_masktumor_try(Parameter_List):\n",
        "    img = Parameter_List[0]\n",
        "    tumor = Parameter_List[1]\n",
        "    lines = Parameter_List[2]\n",
        "    numid = Parameter_List[3]\n",
        "    minindex = Parameter_List[4]\n",
        "    maxindex = Parameter_List[5]\n",
        "    #  randomly scale\n",
        "    scale = np.random.uniform(0.8,1.2)\n",
        "    deps = int(img_deps * scale)\n",
        "    rows = int(img_rows * scale)\n",
        "    cols = 3\n",
        "\n",
        "    sed = np.random.randint(1,numid)\n",
        "    cen = lines[sed-1]\n",
        "    cen = np.fromstring(cen, dtype=int, sep=' ')\n",
        "    # print (cen)\n",
        "    a = min(max(minindex[0] + deps/2, cen[0]), maxindex[0]- deps/2-1)\n",
        "    b = min(max(minindex[1] + rows/2, cen[1]), maxindex[1]- rows/2-1)\n",
        "    c = min(max(minindex[2] + cols/2, cen[2]), maxindex[2]- cols/2-1)\n",
        "    cropp_img = img[a - deps / 2:a + deps / 2, b - rows / 2:b + rows / 2,\n",
        "                c - cols / 2: c + cols / 2 + 1].copy()\n",
        "    cropp_tumor = tumor[a - deps / 2:a + deps / 2, b - rows / 2:b + rows / 2,\n",
        "                  c - cols / 2:c + cols / 2 + 1].copy()\n",
        "\n",
        "    cropp_img -= mean\n",
        "     # randomly flipping\n",
        "    flip_num = np.random.randint(0,3)\n",
        "    if flip_num == 1:\n",
        "        cropp_img = np.flipud(cropp_img)\n",
        "        cropp_tumor = np.flipud(cropp_tumor)\n",
        "    elif flip_num == 2:\n",
        "        cropp_img = np.fliplr(cropp_img)\n",
        "        cropp_tumor = np.fliplr(cropp_tumor)\n",
        "    #\n",
        "    cropp_tumor = resize(cropp_tumor, (img_deps,img_rows,img_cols), order=0, mode='edge', cval=0, clip=True, preserve_range=True)\n",
        "    cropp_img   = resize(cropp_img, (img_deps,img_rows,img_cols), order=3, mode='constant', cval=0, clip=True, preserve_range=True)\n",
        "    return cropp_img, cropp_tumor[:,:,1]\n",
        "\n",
        "def generate_arrays_from_file(batch_size, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list):\n",
        "    while 1:\n",
        "        X = np.zeros((batch_size, img_deps, img_rows, img_cols), dtype='float32')\n",
        "        Y = np.zeros((batch_size, img_deps, img_rows, 1), dtype='int16')\n",
        "        Parameter_List = []\n",
        "        for idx in tqdm(range(batch_size)):\n",
        "            count = random.choice(trainidx)\n",
        "            img = img_list[count]\n",
        "            tumor = tumor_list[count]\n",
        "            minindex = minindex_list[count]\n",
        "            maxindex = maxindex_list[count]\n",
        "            num = np.random.randint(0,6)\n",
        "            if num < 3 or (count in liverlist):\n",
        "                lines = liverlines[count]\n",
        "                numid = liveridx[count]\n",
        "            else:\n",
        "                lines = tumorlines[count]\n",
        "                numid = tumoridx[count]\n",
        "            Parameter_List.append([img, tumor, lines, numid, minindex, maxindex])\n",
        "        \n",
        "        pool = ThreadPool(thread_num)\n",
        "        result_list = pool.map(load_seq_crop_data_masktumor_try, Parameter_List)\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "        for idx in range(len(result_list)):\n",
        "            X[idx, :, :, :] = result_list[idx][0]\n",
        "            Y[idx, :, :, 0] = result_list[idx][1]\n",
        "        yield (X,Y)\n",
        "\n",
        "def weighted_crossentropy(y_true, y_pred):\n",
        "\n",
        "    y_pred_f = K.reshape(y_pred, (batch_size*img_deps*img_rows,3))\n",
        "    y_true_f = K.reshape(y_true, (batch_size*img_deps*img_rows,))\n",
        "\n",
        "    soft_pred_f = K.softmax(y_pred_f)\n",
        "    soft_pred_f = K.log(tf.clip_by_value(soft_pred_f, 1e-10, 1.0))\n",
        "\n",
        "    neg = K.equal(y_true_f, K.zeros_like(y_true_f))\n",
        "    neg_calculoss = tf.gather(soft_pred_f[:,0], tf.where(neg))\n",
        "\n",
        "    pos1 = K.equal(y_true_f, K.ones_like(y_true_f))\n",
        "    pos1_calculoss = tf.gather(soft_pred_f[:,1], tf.where(pos1))\n",
        "\n",
        "    pos2 = K.equal(y_true_f, 2*K.ones_like(y_true_f))\n",
        "    pos2_calculoss = tf.gather(soft_pred_f[:,2], tf.where(pos2))\n",
        "\n",
        "    loss = -K.mean(tf.concat([0.78*neg_calculoss, 0.65*pos1_calculoss, 8.57*pos2_calculoss], 0))\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def DenseUNet(nb_dense_block=4, growth_rate=48, nb_filter=96, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, classes=1000, weights_path=None):\n",
        "    '''Instantiate the DenseNet 161 architecture,\n",
        "        # Arguments\n",
        "            nb_dense_block: number of dense blocks to add to end\n",
        "            growth_rate: number of filters to add per dense block\n",
        "            nb_filter: initial number of filters\n",
        "            reduction: reduction factor of transition blocks.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            classes: optional number of classes to classify images\n",
        "            weights_path: path to pre-trained weights\n",
        "        # Returns\n",
        "            A Keras model instance.\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "\n",
        "    # compute compression factor\n",
        "    compression = 1.0 - reduction\n",
        "\n",
        "    # Handle Dimension Ordering for different backends\n",
        "    global concat_axis\n",
        "    if K.image_dim_ordering() == 'tf':\n",
        "      concat_axis = 3\n",
        "      img_input = Input(batch_shape=(batch_size, img_deps, img_rows, 3), name='data')\n",
        "    else:\n",
        "      concat_axis = 1\n",
        "      img_input = Input(shape=(3, 512,512), name='data')\n",
        "\n",
        "    # From architecture for ImageNet (Table 1 in the paper)\n",
        "    nb_filter = 96\n",
        "    nb_layers = [6,12,36,24] # For DenseNet-161\n",
        "    box = []\n",
        "    # Initial convolution\n",
        "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n",
        "    x = Conv2D(nb_filter, (7, 7), strides=(2, 2), name='conv1', use_bias=False)(x)\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name='conv1_bn')(x)\n",
        "    x = Scale(axis=concat_axis, name='conv1_scale')(x)\n",
        "    x = Activation('relu', name='relu1')(x)\n",
        "    box.append(x)\n",
        "    x = ZeroPadding2D((1, 1), name='pool1_zeropadding')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n",
        "\n",
        "    # Add dense blocks\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        stage = block_idx+2\n",
        "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        box.append(x)\n",
        "        # Add transition_block\n",
        "        x = transition_block(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "    final_stage = stage + 1\n",
        "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name='conv'+str(final_stage)+'_blk_bn')(x)\n",
        "    x = Scale(axis=concat_axis, name='conv'+str(final_stage)+'_blk_scale')(x)\n",
        "    x = Activation('relu', name='relu'+str(final_stage)+'_blk')(x)\n",
        "    box.append(x)\n",
        "\n",
        "    up0 = UpSampling2D(size=(2,2))(x)\n",
        "    line0 = Conv2D(2208, (1, 1), padding=\"same\", kernel_initializer=\"normal\", name=\"line0\")(box[3])\n",
        "    up0_sum = add([line0, up0])\n",
        "    conv_up0 = Conv2D(768, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up0\")(up0_sum)\n",
        "    bn_up0 = BatchNormalization(name = \"bn_up0\")(conv_up0)\n",
        "    ac_up0 = Activation('relu', name='ac_up0')(bn_up0)\n",
        "\n",
        "    up1 = UpSampling2D(size=(2,2))(ac_up0)\n",
        "    up1_sum = add([box[2], up1])\n",
        "    conv_up1 = Conv2D(384, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up1\")(up1_sum)\n",
        "    bn_up1 = BatchNormalization(name = \"bn_up1\")(conv_up1)\n",
        "    ac_up1 = Activation('relu', name='ac_up1')(bn_up1)\n",
        "\n",
        "    up2 = UpSampling2D(size=(2,2))(ac_up1)\n",
        "    up2_sum = add([box[1], up2])\n",
        "    conv_up2 = Conv2D(96, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up2\")(up2_sum)\n",
        "    bn_up2 = BatchNormalization(name = \"bn_up2\")(conv_up2)\n",
        "    ac_up2 = Activation('relu', name='ac_up2')(bn_up2)\n",
        "\n",
        "    up3 = UpSampling2D(size=(2,2))(ac_up2)\n",
        "    up3_sum = add([box[0], up3])\n",
        "    conv_up3 = Conv2D(96, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up3\")(up3_sum)\n",
        "    bn_up3 = BatchNormalization(name = \"bn_up3\")(conv_up3)\n",
        "    ac_up3 = Activation('relu', name='ac_up3')(bn_up3)\n",
        "\n",
        "    up4 = UpSampling2D(size=(2, 2))(ac_up3)\n",
        "    conv_up4 = Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name=\"conv_up4\")(up4)\n",
        "    conv_up4 = Dropout(rate=0.3)(conv_up4)\n",
        "    bn_up4 = BatchNormalization(name=\"bn_up4\")(conv_up4)\n",
        "    ac_up4 = Activation('relu', name='ac_up4')(bn_up4)\n",
        "\n",
        "    x = Conv2D(3, (1,1), padding=\"same\", kernel_initializer=\"normal\", name=\"dense167classifer\")(ac_up4)\n",
        "\n",
        "    model = Model(img_input, x, name='denseu161')\n",
        "\n",
        "    if weights_path is not None:\n",
        "      model.load_weights(weights_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "def conv_block(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
        "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv2D, 3x3 Conv2D, and option dropout\n",
        "        # Arguments\n",
        "            x: input tensor \n",
        "            stage: index for dense block\n",
        "            branch: layer index within each dense block\n",
        "            nb_filter: number of filters\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = 'conv' + str(stage) + '_' + str(branch)\n",
        "    relu_name_base = 'relu' + str(stage) + '_' + str(branch)\n",
        "\n",
        "    # 1x1 Convolution (Bottleneck layer)\n",
        "    inter_channel = nb_filter * 4\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_x1_bn')(x)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_x1_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x1')(x)\n",
        "    x = Conv2D(inter_channel, (1, 1), name=conv_name_base+'_x1', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # 3x3 Convolution\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_x2_bn')(x)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_x2_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x2')(x)\n",
        "    x = ZeroPadding2D((1, 1), name=conv_name_base+'_x2_zeropadding')(x)\n",
        "    x = Conv2D(nb_filter, (3, 3), name=conv_name_base+'_x2', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def transition_block(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
        "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout \n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_filter: number of filters\n",
        "            compression: calculated as 1 - reduction. Reduces the number of feature maps in the transition block.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = 'conv' + str(stage) + '_blk'\n",
        "    relu_name_base = 'relu' + str(stage) + '_blk'\n",
        "    pool_name_base = 'pool' + str(stage)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_bn')(x)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base)(x)\n",
        "    x = Conv2D(int(nb_filter * compression), (1, 1), name=conv_name_base, use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = AveragePooling2D((2, 2), strides=(2, 2), name=pool_name_base)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True):\n",
        "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_layers: the number of layers of conv_block to append to the model.\n",
        "            nb_filter: number of filters\n",
        "            growth_rate: growth rate\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    concat_feat = x\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        branch = i+1\n",
        "        x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
        "        concat_feat = concatenate([concat_feat, x], axis=concat_axis, name='concat_'+str(stage)+'_'+str(branch))\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    return concat_feat, nb_filter\n",
        "\n",
        "\n",
        "\n",
        "def train_and_predict():\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Creating and compiling model...')\n",
        "    print('-'*30)\n",
        "\n",
        "    model = DenseUNet(reduction=0.5, weights_path='./result_train_dense167_fast/model/weights365.04-0.02.hdf5')\n",
        "    sgd = SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
        "    model.compile(optimizer=sgd, loss=[weighted_crossentropy])\n",
        "\n",
        "    trainidx = list(range(131))\n",
        "    img_list = []\n",
        "    tumor_list = []\n",
        "    minindex_list = []\n",
        "    maxindex_list = []\n",
        "    tumorlines = []\n",
        "    tumoridx = []\n",
        "    liveridx = []\n",
        "    liverlines = []\n",
        "    t1=time.time()\n",
        "    for idx in range(131):\n",
        "        img, img_header = load(DataList[0] + 'myTrainingData/volume-' + str(idx) + '.nii' )\n",
        "        tumor, tumor_header = load(DataList[0] + 'myTrainingData/segmentation-' + str(idx) + '.nii')\n",
        "        img_list.append(img)\n",
        "        tumor_list.append(tumor)\n",
        "\n",
        "        maxmin = np.loadtxt(DataList[0] + str(txtfile) + '/LiverBox/box_' + str(idx) + '.txt', delimiter=' ')\n",
        "        minindex = maxmin[0:3]\n",
        "        maxindex = maxmin[3:6]\n",
        "        minindex = np.array(minindex, dtype='int')\n",
        "        maxindex = np.array(maxindex, dtype='int')\n",
        "        minindex[0] = max(minindex[0]-3, 0)\n",
        "        minindex[1] = max(minindex[1]-3, 0)\n",
        "        minindex[2] = max(minindex[2]-3, 0)\n",
        "        maxindex[0] = min(img.shape[0], maxindex[0]+3)\n",
        "        maxindex[1] = min(img.shape[1], maxindex[1]+3)\n",
        "        maxindex[2] = min(img.shape[2], maxindex[2]+3)\n",
        "        minindex_list.append(minindex)\n",
        "        maxindex_list.append(maxindex)\n",
        "\n",
        "        f1 = open(DataList[0] + str(txtfile) + '/TumorPixels/tumor_' + str(idx) + '.txt','r')\n",
        "        tumorline = f1.readlines()\n",
        "        tumorlines.append(tumorline)\n",
        "        tumoridx.append(len(tumorline))\n",
        "        f1.close()\n",
        "\n",
        "        f2 = open(DataList[0] + str(txtfile) + '/LiverPixels/liver_' + str(idx) + '.txt','r')\n",
        "        liverline = f2.readlines()\n",
        "        liverlines.append(liverline)\n",
        "        liveridx.append(len(liverline))\n",
        "        f2.close()\n",
        "    t2=time.time()\n",
        "    print (t2-t1)\n",
        "\n",
        "\n",
        "    # print (model.summary())\n",
        "\n",
        "    if not os.path.exists(path + \"model\"):\n",
        "        os.mkdir(path + 'model')\n",
        "        os.mkdir(path + 'history')\n",
        "    else:\n",
        "        if os.path.exists(path + \"history/lossbatch.txt\"):\n",
        "            os.remove(path + 'history/lossbatch.txt')\n",
        "        if os.path.exists(path + \"history/lossepoch.txt\"):\n",
        "            os.remove(path + 'history/lossepoch.txt')\n",
        "    model_checkpoint = ModelCheckpoint(path + 'model/weights.{epoch:02d}-{loss:.2f}.hdf5', monitor='loss', verbose = 1,\n",
        "                                       save_best_only=False,save_weights_only=False,mode = 'min', period = 2)\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Fitting model......')\n",
        "    print('-'*30)\n",
        "\n",
        "    steps = 27386/batch_size\n",
        "    model.fit_generator(generate_arrays_from_file(batch_size, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list),steps_per_epoch=steps,\n",
        "                        epochs= 6000, verbose = 1, callbacks = [model_checkpoint], max_queue_size=10, workers=3, use_multiprocessing=True)\n",
        "\n",
        "    print ('Finised Training .......')\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42U2QDGDtCAG"
      },
      "source": [
        "#loss\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def weighted_crossentropy(y_true, y_pred):\n",
        "    y_pred = y_pred[:,:,:,1:7,:]\n",
        "    y_true = y_true[:,:,:,1:7,:]\n",
        "    y_pred_f = K.reshape(y_pred, (-1,3))\n",
        "    y_true_f = K.reshape(y_true, (-1,))\n",
        "\n",
        "    soft_pred_f = K.softmax(y_pred_f)\n",
        "    soft_pred_f = K.log(tf.clip_by_value(soft_pred_f, 1e-10, 1.0))\n",
        "\n",
        "    neg = K.equal(y_true_f, K.zeros_like(y_true_f))\n",
        "    neg_calculoss = tf.gather(soft_pred_f[:,0], tf.where(neg))\n",
        "\n",
        "    pos1 = K.equal(y_true_f, K.ones_like(y_true_f))\n",
        "    pos1_calculoss = tf.gather(soft_pred_f[:,1], tf.where(pos1))\n",
        "\n",
        "    pos2 = K.equal(y_true_f, 2*K.ones_like(y_true_f))\n",
        "    pos2_calculoss = tf.gather(soft_pred_f[:,2], tf.where(pos2))\n",
        "\n",
        "    loss = -K.mean(tf.concat([0.78*neg_calculoss, 0.65*pos1_calculoss, 8.57*pos2_calculoss], 0))\n",
        "\n",
        "    return loss\n",
        "\n",
        "def weighted_crossentropy_2ddense(y_true, y_pred):\n",
        "\n",
        "    y_pred_f = K.reshape(y_pred, (-1,3))\n",
        "    y_true_f = K.reshape(y_true, (-1,))\n",
        "\n",
        "    soft_pred_f = K.softmax(y_pred_f)\n",
        "    soft_pred_f = K.log(tf.clip_by_value(soft_pred_f, 1e-10, 1.0))\n",
        "\n",
        "    neg = K.equal(y_true_f, K.zeros_like(y_true_f))\n",
        "    neg_calculoss = tf.gather(soft_pred_f[:,0], tf.where(neg))\n",
        "\n",
        "    pos1 = K.equal(y_true_f, K.ones_like(y_true_f))\n",
        "    pos1_calculoss = tf.gather(soft_pred_f[:,1], tf.where(pos1))\n",
        "\n",
        "    pos2 = K.equal(y_true_f, 2*K.ones_like(y_true_f))\n",
        "    pos2_calculoss = tf.gather(soft_pred_f[:,2], tf.where(pos2))\n",
        "\n",
        "    loss = -K.mean(tf.concat([0.78*neg_calculoss, 0.65*pos1_calculoss, 8.57*pos2_calculoss], 0))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF0KbcZ-vGwz"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, ZeroPadding2D, concatenate, Lambda, ZeroPadding3D, add\n",
        "from keras.layers.core import Dropout, Activation\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv3D, UpSampling3D, AveragePooling3D\n",
        "from keras.layers.pooling import AveragePooling2D, MaxPooling2D, MaxPooling3D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def conv_block3d(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
        "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv3D, 3x3 Conv3D, and option dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            branch: layer index within each dense block\n",
        "            nb_filter: number of filters\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = '3dconv' + str(stage) + '_' + str(branch)\n",
        "    relu_name_base = '3drelu' + str(stage) + '_' + str(branch)\n",
        "\n",
        "    # 1x1 Convolution (Bottleneck layer)\n",
        "    inter_channel = nb_filter * 4\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name=conv_name_base+'_x1_bn', momentum=1.0, trainable=False)(x, training=False)\n",
        "    x = Scale(axis=4, name=conv_name_base+'_x1_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x1')(x)\n",
        "    x = Conv3D(inter_channel, (1, 1, 1), name=conv_name_base+'_x1', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # 3x3 Convolution\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name=conv_name_base+'_x2_bn', momentum=1.0, trainable=False)(x, training=False)\n",
        "    x = Scale(axis=4, name=conv_name_base+'_x2_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x2')(x)\n",
        "    x = ZeroPadding3D((1, 1, 1), name=conv_name_base+'_x2_zeropadding')(x)\n",
        "    x = Conv3D(nb_filter, (3, 3, 3), name=conv_name_base+'_x2', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "def dense_block3d(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True):\n",
        "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_layers: the number of layers of conv_block to append to the model.\n",
        "            nb_filter: number of filters\n",
        "            growth_rate: growth rate\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    concat_feat = x\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        branch = i+1\n",
        "        x = conv_block3d(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
        "        concat_feat = concatenate([concat_feat, x], axis=4, name='3dconcat_'+str(stage)+'_'+str(branch))\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    return concat_feat, nb_filter\n",
        "def transition_block3d(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
        "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_filter: number of filters\n",
        "            compression: calculated as 1 - reduction. Reduces the number of feature maps in the transition block.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = '3dconv' + str(stage) + '_blk'\n",
        "    relu_name_base = '3drelu' + str(stage) + '_blk'\n",
        "    pool_name_base = '3dpool' + str(stage)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name=conv_name_base+'_bn', momentum=1.0)(x, training=False)\n",
        "    x = Scale(axis=4, name=conv_name_base+'_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base)(x)\n",
        "    x = Conv3D(int(nb_filter * compression), (1, 1, 1), name=conv_name_base, use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = AveragePooling3D((2, 2, 1), strides=(2, 2, 1), name=pool_name_base)(x)\n",
        "\n",
        "    return x\n",
        "def DenseNet3D(img_input, nb_dense_block=4, growth_rate=32, nb_filter=96, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, classes=1000, weights_path=None):\n",
        "    '''Instantiate the DenseNet 161 architecture,\n",
        "        # Arguments\n",
        "            nb_dense_block: number of dense blocks to add to end\n",
        "            growth_rate: number of filters to add per dense block\n",
        "            nb_filter: initial number of filters\n",
        "            reduction: reduction factor of transition blocks.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            classes: optional number of classes to classify images\n",
        "            weights_path: path to pre-trained weights\n",
        "        # Returns\n",
        "            A Keras model instance.\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "\n",
        "    # compute compression factor\n",
        "    compression = 1.0 - reduction\n",
        "\n",
        "    # From architecture for ImageNet (Table 1 in the paper)\n",
        "    nb_filter = 96\n",
        "    nb_layers = [3, 4, 12, 8]  # For DenseNet-161\n",
        "    box = []\n",
        "    # Initial convolution\n",
        "    x = ZeroPadding3D((3, 3, 3), name='3dconv1_zeropadding')(img_input)\n",
        "    x = Conv3D(nb_filter, (7, 7, 7), strides=(2, 2, 2), name='3dconv1', use_bias=False)(x)\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name='3dconv1_bn')(x)\n",
        "    x = Scale(axis=4, name='3dconv1_scale')(x)\n",
        "    x = Activation('relu', name='3drelu1')(x)\n",
        "    box.append(x)\n",
        "    x = ZeroPadding3D((1, 1, 1), name='3dpool1_zeropadding')(x)\n",
        "    x = MaxPooling3D((3, 3, 3), strides=(2, 2, 2), name='3dpool1')(x)\n",
        "\n",
        "    # Add dense blocks\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        stage = block_idx + 2\n",
        "        x, nb_filter = dense_block3d(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
        "                                   weight_decay=weight_decay)\n",
        "        box.append(x)\n",
        "        # Add transition_block\n",
        "        x = transition_block3d(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate,\n",
        "                             weight_decay=weight_decay)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "    final_stage = stage + 1\n",
        "    x, nb_filter = dense_block3d(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
        "                               weight_decay=weight_decay)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name='3dconv' + str(final_stage) + '_blk_bn')(x)\n",
        "    x = Scale(axis=4, name='3dconv' + str(final_stage) + '_blk_scale')(x)\n",
        "    x = Activation('relu', name='3drelu' + str(final_stage) + '_blk')(x)\n",
        "    box.append(x)\n",
        "\n",
        "    up0 = UpSampling3D(size=(2, 2, 1))(x)\n",
        "    conv_up0 = Conv3D(504, (3, 3, 3), padding=\"same\", name=\"3dconv_up0\")(up0)\n",
        "    bn_up0 = BatchNormalization(name=\"3dbn_up0\")(conv_up0)\n",
        "    ac_up0 = Activation('relu', name='3dac_up0')(bn_up0)\n",
        "\n",
        "    up1 = UpSampling3D(size=(2, 2, 1))(ac_up0)\n",
        "    conv_up1 = Conv3D(224, (3, 3, 3), padding=\"same\", name=\"3dconv_up1\")(up1)\n",
        "    bn_up1 = BatchNormalization(name=\"3dbn_up1\")(conv_up1)\n",
        "    ac_up1 = Activation('relu', name='3dac_up1')(bn_up1)\n",
        "\n",
        "    up2 = UpSampling3D(size=(2, 2, 1))(ac_up1)\n",
        "    conv_up2 = Conv3D(192, (3, 3, 3), padding=\"same\", name=\"3dconv_up2\")(up2)\n",
        "    bn_up2 = BatchNormalization(name=\"3dbn_up2\")(conv_up2)\n",
        "    ac_up2 = Activation('relu', name='3dac_up2')(bn_up2)\n",
        "\n",
        "    up3 = UpSampling3D(size=(2, 2, 2))(ac_up2)\n",
        "    conv_up3 = Conv3D(96, (3, 3, 3), padding=\"same\", name=\"3dconv_up3\")(up3)\n",
        "    bn_up3 = BatchNormalization(name=\"3dbn_up3\")(conv_up3)\n",
        "    ac_up3 = Activation('relu', name='3dac_up3')(bn_up3)\n",
        "\n",
        "    up4 = UpSampling3D(size=(2, 2, 2))(ac_up3)\n",
        "    conv_up4 = Conv3D(64, (3, 3, 3), padding=\"same\", name=\"3dconv_up4\")(up4)\n",
        "    bn_up4 = BatchNormalization(name=\"3dbn_up4\")(conv_up4)\n",
        "    ac_up4 = Activation('relu', name='3dac_up4')(bn_up4)\n",
        "\n",
        "    x = Conv3D(3, (1, 1, 1), padding=\"same\", name='3dclassifer')(ac_up4)\n",
        "\n",
        "    return ac_up4, x\n",
        "\n",
        "\n",
        "\n",
        "def DenseUNet(img_input, nb_dense_block=4, growth_rate=48, nb_filter=96, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, classes=1000, weights_path=None):\n",
        "    '''Instantiate the DenseNet 161 architecture,\n",
        "        # Arguments\n",
        "            nb_dense_block: number of dense blocks to add to end\n",
        "            growth_rate: number of filters to add per dense block\n",
        "            nb_filter: initial number of filters\n",
        "            reduction: reduction factor of transition blocks.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            classes: optional number of classes to classify images\n",
        "            weights_path: path to pre-trained weights\n",
        "        # Returns\n",
        "            A Keras model instance.\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "    # compute compression factor\n",
        "    compression = 1.0 - reduction\n",
        "\n",
        "    # Handle Dimension Ordering for different backends\n",
        "    global concat_axis\n",
        "    concat_axis = 3\n",
        "\n",
        "    # From architecture for ImageNet (Table 1 in the paper)\n",
        "    nb_filter = 96\n",
        "    nb_layers = [6,12,36,24] # For DenseNet-161\n",
        "    box = []\n",
        "    # Initial convolution\n",
        "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n",
        "    x = Conv2D(nb_filter, (7, 7), strides=(2, 2), name='conv1', use_bias=False, trainable=True)(x)\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name='conv1_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name='conv1_scale')(x)\n",
        "    x = Activation('relu', name='relu1')(x)\n",
        "    box.append(x)\n",
        "    x = ZeroPadding2D((1, 1), name='pool1_zeropadding')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n",
        "\n",
        "    # Add dense blocks\n",
        "    for block_idx in range(int(nb_dense_block) - 1):\n",
        "        stage = block_idx+2\n",
        "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        box.append(x)\n",
        "        # Add transition_block\n",
        "        x = transition_block(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "    final_stage = stage + 1\n",
        "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name='conv'+str(final_stage)+'_blk_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name='conv'+str(final_stage)+'_blk_scale')(x)\n",
        "    x = Activation('relu', name='relu'+str(final_stage)+'_blk')(x)\n",
        "    box.append(x)\n",
        "\n",
        "    up0 = UpSampling2D(size=(2,2))(x)\n",
        "    conv_up0 = Conv2D(768, (3, 3), padding=\"same\", name = \"conv_up0\", trainable=True)(up0)\n",
        "    bn_up0 = BatchNormalization(name = \"bn_up0\", momentum = 1, trainable=False)(conv_up0, training=False)\n",
        "    ac_up0 = Activation('relu', name='ac_up0')(bn_up0)\n",
        "\n",
        "    up1 = UpSampling2D(size=(2,2))(ac_up0)\n",
        "    conv_up1 = Conv2D(384, (3, 3), padding=\"same\", name = \"conv_up1\", trainable=True)(up1)\n",
        "    bn_up1 = BatchNormalization(name = \"bn_up1\", momentum = 1, trainable=False)(conv_up1, training=False)\n",
        "    ac_up1 = Activation('relu', name='ac_up1')(bn_up1)\n",
        "\n",
        "    up2 = UpSampling2D(size=(2,2))(ac_up1)\n",
        "    conv_up2 = Conv2D(96, (3, 3), padding=\"same\", name = \"conv_up2\", trainable=True)(up2)\n",
        "    bn_up2 = BatchNormalization(name = \"bn_up2\", momentum = 1, trainable=False)(conv_up2, training=False)\n",
        "    ac_up2 = Activation('relu', name='ac_up2')(bn_up2)\n",
        "\n",
        "    up3 = UpSampling2D(size=(2,2))(ac_up2)\n",
        "    conv_up3 = Conv2D(96, (3, 3), padding=\"same\", name = \"conv_up3\", trainable=True)(up3)\n",
        "    bn_up3 = BatchNormalization(name = \"bn_up3\", momentum = 1, trainable=False)(conv_up3, training=False)\n",
        "    ac_up3 = Activation('relu', name='ac_up3')(bn_up3)\n",
        "\n",
        "    up4 = UpSampling2D(size=(2, 2))(ac_up3)\n",
        "    conv_up4 = Conv2D(64, (3, 3), padding=\"same\", name=\"conv_up4\", trainable=True)(up4)\n",
        "    bn_up4 = BatchNormalization(name=\"bn_up4\", momentum = 1, trainable=False)(conv_up4, training=False)\n",
        "    ac_up4 = Activation('relu', name='ac_up4')(bn_up4)\n",
        "\n",
        "    x = Conv2D(3, (1,1), padding=\"same\", name='dense167classifer', trainable=True)(ac_up4)\n",
        "\n",
        "    return ac_up4, x\n",
        "\n",
        "def conv_block(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
        "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv2D, 3x3 Conv2D, and option dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            branch: layer index within each dense block\n",
        "            nb_filter: number of filters\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = 'conv' + str(stage) + '_' + str(branch)\n",
        "    relu_name_base = 'relu' + str(stage) + '_' + str(branch)\n",
        "\n",
        "    # 1x1 Convolution (Bottleneck layer)\n",
        "    inter_channel = nb_filter * 4\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name=conv_name_base+'_x1_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_x1_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x1')(x)\n",
        "    x = Conv2D(inter_channel, (1, 1), name=conv_name_base+'_x1', use_bias=False, trainable=True)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # 3x3 Convolution\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name=conv_name_base+'_x2_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_x2_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x2')(x)\n",
        "    x = ZeroPadding2D((1, 1), name=conv_name_base+'_x2_zeropadding')(x)\n",
        "    x = Conv2D(nb_filter, (3, 3), name=conv_name_base+'_x2', use_bias=False, trainable=True)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def transition_block(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
        "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_filter: number of filters\n",
        "            compression: calculated as 1 - reduction. Reduces the number of feature maps in the transition block.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = 'conv' + str(stage) + '_blk'\n",
        "    relu_name_base = 'relu' + str(stage) + '_blk'\n",
        "    pool_name_base = 'pool' + str(stage)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name=conv_name_base+'_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base)(x)\n",
        "    x = Conv2D(int(nb_filter * compression), (1, 1), name=conv_name_base, use_bias=False, trainable=True)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = AveragePooling2D((2, 2), strides=(2, 2), name=pool_name_base)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True):\n",
        "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_layers: the number of layers of conv_block to append to the model.\n",
        "            nb_filter: number of filters\n",
        "            growth_rate: growth rate\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    concat_feat = x\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        branch = i+1\n",
        "        x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
        "        concat_feat = concatenate([concat_feat, x], axis=concat_axis, name='concat_'+str(stage)+'_'+str(branch))\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    return concat_feat, nb_filter\n",
        "def slice(x, h1, h2):\n",
        "    \"\"\" Define a tensor slice function\n",
        "    \"\"\"\n",
        "    return x[:, :, :, h1:h2,:]\n",
        "def slice2d(x, h1, h2):\n",
        "\n",
        "    tmp = x[h1:h2,:,:,:]\n",
        "    tmp = tf.transpose(tmp, perm=[1, 2, 0, 3])\n",
        "    tmp = tf.expand_dims(tmp, 0)\n",
        "    return tmp\n",
        "\n",
        "def slice_last(x):\n",
        "\n",
        "    x = x[:,:,:,:,0]\n",
        "    return x\n",
        "def trans(x):\n",
        "\n",
        "    x = tf.transpose(x, perm=[0,3,1,2,4])\n",
        "    return x\n",
        "def trans_back(x):\n",
        "\n",
        "    x = tf.transpose(x, perm=[0,2,3,1,4])\n",
        "\n",
        "    return x\n",
        "def dense_rnn_net():\n",
        "\n",
        "    #  ************************3d volume input******************************************************************\n",
        "    img_input = Input(batch_shape=(1, 224, 224,8, 1), name='volumetric_data')\n",
        "\n",
        "    #  ************************(batch*d3cols)*2dvolume--2D DenseNet branch**************************************\n",
        "    input2d = Lambda(slice, arguments={'h1': 0, 'h2': 2})(img_input)\n",
        "    single = Lambda(slice, arguments={'h1':0, 'h2':1})(img_input)\n",
        "    input2d = concatenate([single, input2d], axis=3)\n",
        "    for i in range(8 - 2):\n",
        "        input2d_tmp = Lambda(slice, arguments={'h1': i, 'h2': i + 3})(img_input)\n",
        "        input2d = concatenate([input2d, input2d_tmp], axis=0)\n",
        "        if i == 8 - 3:\n",
        "            final1 = Lambda(slice, arguments={'h1': 8-2, 'h2': 8})(img_input)\n",
        "            final2 = Lambda(slice, arguments={'h1': 8-1, 'h2': 8})(img_input)\n",
        "            final = concatenate([final1, final2], axis=3)\n",
        "            input2d = concatenate([input2d, final], axis=0)\n",
        "    input2d = Lambda(slice_last)(input2d)\n",
        "\n",
        "    #  ******************************stack to 3D volumes *******************************************************\n",
        "    feature2d, classifer2d = DenseUNet(input2d, reduction=0.5)\n",
        "    res2d = Lambda(slice2d, arguments={'h1': 0, 'h2': 1})(classifer2d)\n",
        "    fea2d = Lambda(slice2d, arguments={'h1':0, 'h2':1})(feature2d)\n",
        "    for j in range(8 - 1):\n",
        "        score = Lambda(slice2d, arguments={'h1': j + 1, 'h2': j + 2})(classifer2d)\n",
        "        fea2d_slice = Lambda(slice2d, arguments={'h1': j + 1, 'h2': j + 2})(feature2d)\n",
        "        res2d = concatenate([res2d, score], axis=3)\n",
        "        fea2d = concatenate([fea2d, fea2d_slice], axis=3)\n",
        "\n",
        "    #  *************************** 3d DenseNet on 3D volume (concate with feature map )*********************************\n",
        "    res2d_input = Lambda(lambda x: x * 250)(res2d)\n",
        "    input3d_ori = Lambda(slice, arguments={'h1': 0, 'h2': 8})(img_input)\n",
        "    input3d = concatenate([input3d_ori, res2d_input], axis=4)\n",
        "    feature3d, classifer3d = DenseNet3D(input3d, reduction=0.5)\n",
        "\n",
        "    final = add([feature3d, fea2d])\n",
        "    final_conv = Conv3D(64, (3, 3, 3), padding=\"same\", name='fianl_conv')(final)\n",
        "    final_conv = Dropout(rate=0.3)(final_conv)\n",
        "    final_bn = BatchNormalization(name=\"final_bn\")(final_conv)\n",
        "    final_ac = Activation('relu', name='final_ac')(final_bn)\n",
        "    classifer = Conv3D(3, (1, 1, 1), padding=\"same\", name='2d3dclassifer')(final_ac)\n",
        "\n",
        "    model = Model( inputs = img_input,outputs = classifer, name='auto3d_residual_conv')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def dilated_resnet(args):\n",
        "    inputs = Input(batch_shape = (args.b, args.input_size, args.input_size, args.input_cols, 1))\n",
        "    conv1 = Conv3D(64, (3, 3, 3), padding = \"same\",kernel_initializer=\"normal\")(inputs)\n",
        "    bn0 = BatchNormalization()(conv1)\n",
        "    ac0 = Activation('relu')(bn0)\n",
        "    pool1 = MaxPooling3D(pool_size=(2, 2, 1))(ac0)\n",
        "\n",
        "    #  resudial block\n",
        "    conv2 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool1)\n",
        "    bn1 = BatchNormalization()(conv2)\n",
        "    ac1 = Activation('relu')(bn1)\n",
        "    conv3 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac1)\n",
        "    bn2 = BatchNormalization()(conv3)\n",
        "    pad1 = Conv3D(128, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool1)\n",
        "    BN1 = BatchNormalization()(pad1)\n",
        "    sumb1 = add([BN1, bn2])\n",
        "    res1  = Activation('relu')(sumb1)\n",
        "\n",
        "    pool2 = MaxPooling3D(pool_size=(2, 2, 1))(res1)\n",
        "\n",
        "    #  resudial block\n",
        "    conv4 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool2)\n",
        "    bn3 = BatchNormalization()(conv4)\n",
        "    ac2 = Activation('relu')(bn3)\n",
        "    conv5 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac2)\n",
        "    bn4 = BatchNormalization()(conv5)\n",
        "    pad2 = Conv3D(256, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool2)\n",
        "    BN2 = BatchNormalization()(pad2)\n",
        "    sumb2 = add([BN2, bn4])\n",
        "    res2  = Activation('relu')(sumb2)\n",
        "\n",
        "\n",
        "    pool3 = MaxPooling3D(pool_size=(2, 2, 1))(res2)\n",
        "\n",
        "    #  resudial block\n",
        "    conv6 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool3)\n",
        "    bn5 = BatchNormalization()(conv6)\n",
        "    ac3 = Activation('relu')(bn5)\n",
        "    conv7 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac3)\n",
        "    bn6 = BatchNormalization()(conv7)\n",
        "    pad3 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool3)\n",
        "    BN3 = BatchNormalization()(pad3)\n",
        "    sumb3 = add([BN3, bn6])\n",
        "    res3  = Activation('relu')(sumb3)\n",
        "\n",
        "    #  resudial deliated block\n",
        "    del1 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(res3)\n",
        "    delbn1 = BatchNormalization()(del1)\n",
        "    delac1 = Activation('relu')(delbn1)\n",
        "    del2 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(delac1)\n",
        "    delbn2 = BatchNormalization()(del2)\n",
        "    deladd1 = add([res3, delbn2])\n",
        "    delres  = Activation('relu')(deladd1)\n",
        "\n",
        "    pool4 = MaxPooling3D(pool_size=(2, 2, 1))(delres)\n",
        "\n",
        "    #  resudial block\n",
        "    conv6_4 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool4)\n",
        "    bn5_4 = BatchNormalization()(conv6_4)\n",
        "    ac3_4 = Activation('relu')(bn5_4)\n",
        "    conv7_4 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac3_4)\n",
        "    bn6_4 = BatchNormalization()(conv7_4)\n",
        "    pad3_4 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool4)\n",
        "    BN3_4 = BatchNormalization()(pad3_4)\n",
        "    sumb3_4 = add([BN3_4, bn6_4])\n",
        "    res3_4  = Activation('relu')(sumb3_4)\n",
        "\n",
        "    #  resudial deliated block\n",
        "    del3 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(res3_4)\n",
        "    delbn3 = BatchNormalization()(del3)\n",
        "    delac3 = Activation('relu')(delbn3)\n",
        "    del4 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(delac3)\n",
        "    delbn4 = BatchNormalization()(del4)\n",
        "    deladd2 = add([res3_4, delbn4])\n",
        "    delres2  = Activation('relu')(deladd2)\n",
        "\n",
        "\n",
        "    up0 = UpSampling3D(size=(2,2,1))(delres2)\n",
        "    pad4 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(delres)\n",
        "    BN4 = BatchNormalization()(pad4)\n",
        "    sumb4 = add([BN4, up0])\n",
        "\n",
        "    #  resudial block\n",
        "    conv8_1 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb4)\n",
        "    bn7_1 = BatchNormalization()(conv8_1)\n",
        "    ac4_1 = Activation('relu')(bn7_1)\n",
        "    conv9_1 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac4_1)\n",
        "    bn8_1 = BatchNormalization()(conv9_1)\n",
        "    pad5_1 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb4)\n",
        "    BN5_1 = BatchNormalization()(pad5_1)\n",
        "    sumb5_1 = add([BN5_1, bn8_1])\n",
        "    res4_1  = Activation('relu')(sumb5_1)\n",
        "\n",
        "    #  resudial deliated block\n",
        "    del5 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(res4_1)\n",
        "    delbn5 = BatchNormalization()(del5)\n",
        "    delac5 = Activation('relu')(delbn5)\n",
        "    del6 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(delac5)\n",
        "    delbn6 = BatchNormalization()(del6)\n",
        "    deladd3 = add([res4_1, delbn6])\n",
        "    delres3  = Activation('relu')(deladd3)\n",
        "\n",
        "    up0_1 = UpSampling3D(size=(2,2,1))(delres3)\n",
        "    pad4_1 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(res2)\n",
        "    BN4_1 = BatchNormalization()(pad4_1)\n",
        "    sumb4_1 = add([BN4_1, up0_1])\n",
        "\n",
        "    #  resudial block\n",
        "    conv8 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb4_1)\n",
        "    bn7 = BatchNormalization()(conv8)\n",
        "    ac4 = Activation('relu')(bn7)\n",
        "    conv9 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac4)\n",
        "    bn8 = BatchNormalization()(conv9)\n",
        "    pad5 = Conv3D(256, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb4_1)\n",
        "    BN5 = BatchNormalization()(pad5)\n",
        "    sumb5 = add([BN5, bn8])\n",
        "    res4  = Activation('relu')(sumb5)\n",
        "\n",
        "    up1 = UpSampling3D(size=(2, 2, 1))(res4)\n",
        "    pad6 = Conv3D(256, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(res1)\n",
        "    BN6 = BatchNormalization()(pad6)\n",
        "    sumb6 = add([BN6, up1])\n",
        "\n",
        "    #  resudial block\n",
        "    conv10 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb6)\n",
        "    bn9 = BatchNormalization()(conv10)\n",
        "    ac5 = Activation('relu')(bn9)\n",
        "    conv11 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac5)\n",
        "    bn10 = BatchNormalization()(conv11)\n",
        "    pad7 = Conv3D(128, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb6)\n",
        "    BN7 = BatchNormalization()(pad7)\n",
        "    sumb7 = add([BN7, bn10])\n",
        "    res5  = Activation('relu')(sumb7)\n",
        "\n",
        "    up2 = UpSampling3D(size=(2, 2, 1))(res5)\n",
        "    pad8 = Conv3D(128, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(ac0)\n",
        "    BN8 = BatchNormalization()(pad8)\n",
        "    sumb8 = add([BN8, up2])\n",
        "\n",
        "    #  resudial block\n",
        "    conv12 = Conv3D(64, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb8)\n",
        "    bn11= BatchNormalization()(conv12)\n",
        "    ac6 = Activation('relu')(bn11)\n",
        "    conv13 = Conv3D(64, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac6)\n",
        "    bn12 = BatchNormalization()(conv13)\n",
        "    pad9 = Conv3D(64, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb8)\n",
        "    BN9 = BatchNormalization()(pad9)\n",
        "    sumb9 = add([BN9, bn12])\n",
        "    res6 = Activation('relu')(sumb9)\n",
        "\n",
        "    output3 = Conv3D(2, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(res6)\n",
        "\n",
        "    # print (output3)\n",
        "\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[output3])\n",
        "\n",
        "\n",
        "\n",
        "    return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMPP9KGjn7eM"
      },
      "source": [
        "os.mkdir('/content/Experiments')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnbjQsYmtF_R",
        "outputId": "027211ad-6435-4aa4-cc05-13ea78ccbafa"
      },
      "source": [
        "\n",
        "# train_hybrid\n",
        "\n",
        "from __future__ import print_function\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "from medpy.io import load\n",
        "import numpy as np\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import keras.backend as K\n",
        "import os\n",
        "import time\n",
        "from skimage.transform import resize\n",
        "import argparse\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# K.set_image_dim_ordering('tf')\n",
        "\n",
        "# #  global parameters\n",
        "# parser = argparse.ArgumentParser(description='Keras DenseUnet Training')\n",
        "# #  data folder\n",
        "# parser.add_argument('-data', type=str, default='data/', help='test images')\n",
        "# parser.add_argument('-save_path', type=str, default='Experiments/')\n",
        "# #  other paras\n",
        "# parser.add_argument('-b', type=int, default=1)\n",
        "# parser.add_argument('-input_size', type=int, default=224)\n",
        "# parser.add_argument('-model_weight', type=str, default='./model/model_best.hdf5')\n",
        "# parser.add_argument('-input_cols', type=int, default=8)\n",
        "# parser.add_argument('-arch', type=str, default='')\n",
        "\n",
        "# #  data augment\n",
        "# parser.add_argument('-mean', type=int, default=48)\n",
        "# args = parser.parse_args()\n",
        "\n",
        "thread_num = 14\n",
        "liverlist = [32,34,38,41,47,87,89,91,105,106,114,115,119]\n",
        "\n",
        "def load_seq_crop_data_masktumor_try(Parameter_List):\n",
        "    img = Parameter_List[0]\n",
        "    tumor = Parameter_List[1]\n",
        "    lines = Parameter_List[2]\n",
        "    numid = Parameter_List[3]\n",
        "    minindex = Parameter_List[4]\n",
        "    maxindex = Parameter_List[5]\n",
        "    #  randomly scale\n",
        "    scale = np.random.uniform(0.8,1.2)\n",
        "    deps = int(224 * scale)\n",
        "    rows = int(224 * scale)\n",
        "    cols = 8\n",
        "\n",
        "    sed = np.random.randint(1,numid)\n",
        "    cen = lines[sed-1]\n",
        "    cen = np.fromstring(cen, dtype=int, sep=' ')\n",
        "    # print (cen)\n",
        "    a = min(max(minindex[0] + deps/2, cen[0]), maxindex[0]- deps/2-1)\n",
        "    b = min(max(minindex[1] + rows/2, cen[1]), maxindex[1]- rows/2-1)\n",
        "    c = min(max(minindex[2] + cols/2, cen[2]), maxindex[2]- cols/2-1)\n",
        "\n",
        "    if a < deps / 2:\n",
        "      a1=int((img.shape[0]/2)-(deps/2))\n",
        "      a2=int((img.shape[0]/2)+(deps/2))\n",
        "    else:\n",
        "      a1=int(a-deps/2)\n",
        "      a2=int(a+deps/2)\n",
        "    if b < rows / 2:\n",
        "      b1=int((img.shape[0]/2)-(rows/2))\n",
        "      b2=int((img.shape[0]/2)+(rows/2))\n",
        "    else:\n",
        "      b1=int(b-rows/2)\n",
        "      b2=int(b+rows/2)\n",
        "    if c < cols / 2:\n",
        "      c1=int((img.shape[0]/2)-(cols/2))\n",
        "      c2=int((img.shape[0]/2)+(cols/2))\n",
        "    else:\n",
        "      c1=int(c-cols/2)\n",
        "      c2=int(c+cols/2)\n",
        "\n",
        "    cropp_img = img[a1:a2, b1:b2,c1: c2+1].copy()\n",
        "\n",
        "    cropp_tumor = tumor[a1:a2, b1:b2,c1: c2+1].copy()\n",
        "    \n",
        "\n",
        "    cropp_img -= 48\n",
        "    cropp_tumor = resize(cropp_tumor, (224,224,8), order=0, mode='edge', cval=0, clip=True, preserve_range=True)\n",
        "    cropp_img   = resize(cropp_img, (224,224,8), order=3, mode='constant', cval=0, clip=True, preserve_range=True)\n",
        "    return cropp_img, cropp_tumor\n",
        "   \n",
        "\n",
        "\n",
        "def generate_arrays_from_file(batch_size, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list):\n",
        "    while 1:\n",
        "        X = np.zeros((batch_size, 224, 224,8,1), dtype='float32')\n",
        "        Y = np.zeros((batch_size, 224,224, 8,1), dtype='int16')\n",
        "        Parameter_List = []\n",
        "        for idx in range(batch_size):\n",
        "            count = np.random.choice(trainidx)\n",
        "            img = img_list[count]\n",
        "            tumor = tumor_list[count]\n",
        "            minindex = minindex_list[count]\n",
        "            maxindex = maxindex_list[count]\n",
        "            num = np.random.randint(0,6)\n",
        "            if num < 3 or (count in liverlist):\n",
        "                lines = liverlines[count]\n",
        "                numid = liveridx[count]\n",
        "            else:\n",
        "                lines = tumorlines[count]\n",
        "                numid = tumoridx[count]\n",
        "            Parameter_List.append([img, tumor, lines, numid, minindex, maxindex])\n",
        "                \n",
        "        pool = ThreadPool(thread_num)\n",
        "        result_list = pool.map(load_seq_crop_data_masktumor_try, Parameter_List)\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "\n",
        "\n",
        "        for idx in range(len(result_list)):\n",
        "            X[idx, :, :, :, 0] = result_list[idx][0]\n",
        "            Y[idx, :, :, :, 0] = result_list[idx][1]\n",
        "        # if np.sum(Y==0)==0:\n",
        "        #     print(np.sum(Y==0)==0)\n",
        "        #     continue\n",
        "        # if np.sum(Y==1)==0:\n",
        "        #     print(np.sum(Y==1)==0)\n",
        "        #     continue\n",
        "        # if np.sum(Y==2)==0:\n",
        "        #     print(np.sum(Y==2)==0)\n",
        "        #     continue\n",
        "        yield (X,Y)\n",
        "\n",
        "def train_and_predict():\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Creating and compiling model...')\n",
        "    print('-'*30)\n",
        "\n",
        "    # model = denseunet_3d()\n",
        "    # model_path = \"/3dpart_model\"\n",
        "    # sgd = SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
        "    # model.compile(optimizer=sgd, loss=[weighted_crossentropy])\n",
        "    # model.load_weights('/content/gdrive/MyDrive/segmentation/model_best.hdf5', by_name=True, by_gpu=True, two_model=True, by_flag=True)\n",
        "    # model.load_weights('/content/gdrive/MyDrive/segmentation/model_best.hdf5')\n",
        "    # else:\n",
        "    model = dense_rnn_net()\n",
        "    model_path = \"/hybrid_model\"\n",
        "    sgd = SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
        "    model.compile(optimizer=sgd, loss=[weighted_crossentropy])\n",
        "    model.load_weights('/content/gdrive/MyDrive/model_best.hdf5')\n",
        "\n",
        "    #  liver tumor LITS\n",
        "    trainidx = list(range(10))\n",
        "    img_list = []\n",
        "    tumor_list = []\n",
        "    minindex_list = []\n",
        "    maxindex_list = []\n",
        "    tumorlines = []\n",
        "    tumoridx = []\n",
        "    liveridx = []\n",
        "    liverlines = []\n",
        "    for idx in range(10):\n",
        "        img, img_header = load('/content/data/myTrainingData/volume-' + str(idx) + '.nii' )\n",
        "        tumor, tumor_header = load('/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/segmentation-' + str(idx) + '.nii')\n",
        "        img_list.append(img)\n",
        "        tumor_list.append(tumor)\n",
        "\n",
        "        maxmin = np.loadtxt('/content/data/myTrainingDataTxt/LiverBox/box_' + str(idx) + '.txt', delimiter=' ')\n",
        "        minindex = maxmin[0:3]\n",
        "        maxindex = maxmin[3:6]\n",
        "        minindex = np.array(minindex, dtype='int')\n",
        "        maxindex = np.array(maxindex, dtype='int')\n",
        "        minindex[0] = max(minindex[0]-3, 0)\n",
        "        minindex[1] = max(minindex[1]-3, 0)\n",
        "        minindex[2] = max(minindex[2]-3, 0)\n",
        "        maxindex[0] = min(img.shape[0], maxindex[0]+3)\n",
        "        maxindex[1] = min(img.shape[1], maxindex[1]+3)\n",
        "        maxindex[2] = min(img.shape[2], maxindex[2]+3)\n",
        "        minindex_list.append(minindex)\n",
        "        maxindex_list.append(maxindex)\n",
        "\n",
        "        f1 = open('/content/data/myTrainingDataTxt/TumorPixels/tumor_' + str(idx) + '.txt','r')\n",
        "        tumorline = f1.readlines()\n",
        "        tumorlines.append(tumorline)\n",
        "        tumoridx.append(len(tumorline))\n",
        "        f1.close()\n",
        "\n",
        "        f2 = open('/content/data/myTrainingDataTxt/LiverPixels/liver_' + str(idx) + '.txt','r')\n",
        "        liverline = f2.readlines()\n",
        "        liverlines.append(liverline)\n",
        "        liveridx.append(len(liverline))\n",
        "        f2.close()\n",
        "    save_path='Experiments'\n",
        "    if not os.path.exists(save_path +model_path):\n",
        "        os.mkdir(save_path + model_path)\n",
        "    if not os.path.exists(save_path + \"/history\"):\n",
        "        os.mkdir(save_path + '/history')\n",
        "    else:\n",
        "        if os.path.exists(save_path + \"/history/lossbatch.txt\"):\n",
        "            os.remove(save_path + '/history/lossbatch.txt')\n",
        "        if os.path.exists(save_path + \"/history/lossepoch.txt\"):\n",
        "            os.remove(save_path + '/history/lossepoch.txt')\n",
        "    print('-'*30)\n",
        "    print('Fitting model......')\n",
        "    print('-'*30)\n",
        "    steps = 5\n",
        "    model.fit_generator(generate_arrays_from_file(1, trainidx, img_list, tumor_list, tumorlines, liverlines,\n",
        "                                                  tumoridx, liveridx, minindex_list, maxindex_list),\n",
        "                        steps_per_epoch=steps,\n",
        "                        epochs= 50, verbose = 1, use_multiprocessing=False)\n",
        "    print ('Finised Training .......')\n",
        "\n",
        "train_and_predict()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Creating and compiling model...\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Fitting model......\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1915: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "5/5 [==============================] - 116s 671ms/step - loss: 0.0236\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 3s 668ms/step - loss: 0.0165\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 3s 670ms/step - loss: 0.0371\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0149\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0263\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0464\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 3s 668ms/step - loss: 0.0128\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 3s 670ms/step - loss: 0.0180\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 3s 670ms/step - loss: 0.0628\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0180\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 3s 668ms/step - loss: 0.0207\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 3s 668ms/step - loss: 0.0277\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0398\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 3s 670ms/step - loss: 0.0262\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 3s 668ms/step - loss: 0.0266\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0221\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0163\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 3s 668ms/step - loss: 0.0266\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0181\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0424\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0294\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 3s 670ms/step - loss: 0.0327\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0297\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0337\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 3s 670ms/step - loss: 0.0195\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 3s 670ms/step - loss: 0.0410\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0201\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0261\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0273\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 3s 668ms/step - loss: 0.0480\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 3s 670ms/step - loss: 0.0175\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0231\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 3s 668ms/step - loss: 0.0455\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 3s 671ms/step - loss: 0.0300\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 3s 673ms/step - loss: 0.0261\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 3s 670ms/step - loss: 0.0195\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0236\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0388\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 3s 668ms/step - loss: 0.0294\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 3s 668ms/step - loss: 0.0176\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 3s 670ms/step - loss: 0.0223\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0169\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0223\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0296\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 3s 670ms/step - loss: 0.0258\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0236\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0139\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 3s 669ms/step - loss: 0.0367\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 3s 668ms/step - loss: 0.0356\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 3s 670ms/step - loss: 0.0425\n",
            "Finised Training .......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k14xx0XDPSVw"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, ZeroPadding2D, concatenate, Lambda, ZeroPadding3D, add\n",
        "from keras.layers.core import Dropout, Activation\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv3D, UpSampling3D, AveragePooling3D\n",
        "from keras.layers.pooling import AveragePooling2D, MaxPooling2D, MaxPooling3D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def conv_block3d(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
        "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv3D, 3x3 Conv3D, and option dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            branch: layer index within each dense block\n",
        "            nb_filter: number of filters\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = '3dconv' + str(stage) + '_' + str(branch)\n",
        "    relu_name_base = '3drelu' + str(stage) + '_' + str(branch)\n",
        "\n",
        "    # 1x1 Convolution (Bottleneck layer)\n",
        "    inter_channel = nb_filter * 4\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name=conv_name_base+'_x1_bn', momentum=1.0, trainable=False)(x, training=False)\n",
        "    x = Scale(axis=4, name=conv_name_base+'_x1_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x1')(x)\n",
        "    x = Conv3D(inter_channel, (1, 1, 1), name=conv_name_base+'_x1', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # 3x3 Convolution\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name=conv_name_base+'_x2_bn', momentum=1.0, trainable=False)(x, training=False)\n",
        "    x = Scale(axis=4, name=conv_name_base+'_x2_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x2')(x)\n",
        "    x = ZeroPadding3D((1, 1, 1), name=conv_name_base+'_x2_zeropadding')(x)\n",
        "    x = Conv3D(nb_filter, (3, 3, 3), name=conv_name_base+'_x2', use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "def dense_block3d(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True):\n",
        "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_layers: the number of layers of conv_block to append to the model.\n",
        "            nb_filter: number of filters\n",
        "            growth_rate: growth rate\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    concat_feat = x\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        branch = i+1\n",
        "        x = conv_block3d(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
        "        concat_feat = concatenate([concat_feat, x], axis=4, name='3dconcat_'+str(stage)+'_'+str(branch))\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    return concat_feat, nb_filter\n",
        "def transition_block3d(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
        "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_filter: number of filters\n",
        "            compression: calculated as 1 - reduction. Reduces the number of feature maps in the transition block.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = '3dconv' + str(stage) + '_blk'\n",
        "    relu_name_base = '3drelu' + str(stage) + '_blk'\n",
        "    pool_name_base = '3dpool' + str(stage)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name=conv_name_base+'_bn', momentum=1.0)(x, training=False)\n",
        "    x = Scale(axis=4, name=conv_name_base+'_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base)(x)\n",
        "    x = Conv3D(int(nb_filter * compression), (1, 1, 1), name=conv_name_base, use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = AveragePooling3D((2, 2, 1), strides=(2, 2, 1), name=pool_name_base)(x)\n",
        "\n",
        "    return x\n",
        "def DenseNet3D(img_input, nb_dense_block=4, growth_rate=32, nb_filter=96, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, classes=1000, weights_path=None):\n",
        "    '''Instantiate the DenseNet 161 architecture,\n",
        "        # Arguments\n",
        "            nb_dense_block: number of dense blocks to add to end\n",
        "            growth_rate: number of filters to add per dense block\n",
        "            nb_filter: initial number of filters\n",
        "            reduction: reduction factor of transition blocks.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            classes: optional number of classes to classify images\n",
        "            weights_path: path to pre-trained weights\n",
        "        # Returns\n",
        "            A Keras model instance.\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "\n",
        "    # compute compression factor\n",
        "    compression = 1.0 - reduction\n",
        "\n",
        "    # From architecture for ImageNet (Table 1 in the paper)\n",
        "    nb_filter = 96\n",
        "    nb_layers = [3, 4, 12, 8]  # For DenseNet-161\n",
        "    box = []\n",
        "    # Initial convolution\n",
        "    x = ZeroPadding3D((3, 3, 3), name='3dconv1_zeropadding')(img_input)\n",
        "    x = Conv3D(nb_filter, (7, 7, 7), strides=(2, 2, 2), name='3dconv1', use_bias=False)(x)\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name='3dconv1_bn')(x)\n",
        "    x = Scale(axis=4, name='3dconv1_scale')(x)\n",
        "    x = Activation('relu', name='3drelu1')(x)\n",
        "    box.append(x)\n",
        "    x = ZeroPadding3D((1, 1, 1), name='3dpool1_zeropadding')(x)\n",
        "    x = MaxPooling3D((3, 3, 3), strides=(2, 2, 2), name='3dpool1')(x)\n",
        "\n",
        "    # Add dense blocks\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        stage = block_idx + 2\n",
        "        x, nb_filter = dense_block3d(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
        "                                   weight_decay=weight_decay)\n",
        "        box.append(x)\n",
        "        # Add transition_block\n",
        "        x = transition_block3d(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate,\n",
        "                             weight_decay=weight_decay)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "    final_stage = stage + 1\n",
        "    x, nb_filter = dense_block3d(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
        "                               weight_decay=weight_decay)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=4, name='3dconv' + str(final_stage) + '_blk_bn')(x)\n",
        "    x = Scale(axis=4, name='3dconv' + str(final_stage) + '_blk_scale')(x)\n",
        "    x = Activation('relu', name='3drelu' + str(final_stage) + '_blk')(x)\n",
        "    box.append(x)\n",
        "\n",
        "    up0 = UpSampling3D(size=(2, 2, 1))(x)\n",
        "    conv_up0 = Conv3D(504, (3, 3, 3), padding=\"same\", name=\"3dconv_up0\")(up0)\n",
        "    bn_up0 = BatchNormalization(name=\"3dbn_up0\")(conv_up0)\n",
        "    ac_up0 = Activation('relu', name='3dac_up0')(bn_up0)\n",
        "\n",
        "    up1 = UpSampling3D(size=(2, 2, 1))(ac_up0)\n",
        "    conv_up1 = Conv3D(224, (3, 3, 3), padding=\"same\", name=\"3dconv_up1\")(up1)\n",
        "    bn_up1 = BatchNormalization(name=\"3dbn_up1\")(conv_up1)\n",
        "    ac_up1 = Activation('relu', name='3dac_up1')(bn_up1)\n",
        "\n",
        "    up2 = UpSampling3D(size=(2, 2, 1))(ac_up1)\n",
        "    conv_up2 = Conv3D(192, (3, 3, 3), padding=\"same\", name=\"3dconv_up2\")(up2)\n",
        "    bn_up2 = BatchNormalization(name=\"3dbn_up2\")(conv_up2)\n",
        "    ac_up2 = Activation('relu', name='3dac_up2')(bn_up2)\n",
        "\n",
        "    up3 = UpSampling3D(size=(2, 2, 2))(ac_up2)\n",
        "    conv_up3 = Conv3D(96, (3, 3, 3), padding=\"same\", name=\"3dconv_up3\")(up3)\n",
        "    bn_up3 = BatchNormalization(name=\"3dbn_up3\")(conv_up3)\n",
        "    ac_up3 = Activation('relu', name='3dac_up3')(bn_up3)\n",
        "\n",
        "    up4 = UpSampling3D(size=(2, 2, 2))(ac_up3)\n",
        "    conv_up4 = Conv3D(64, (3, 3, 3), padding=\"same\", name=\"3dconv_up4\")(up4)\n",
        "    bn_up4 = BatchNormalization(name=\"3dbn_up4\")(conv_up4)\n",
        "    ac_up4 = Activation('relu', name='3dac_up4')(bn_up4)\n",
        "\n",
        "    x = Conv3D(3, (1, 1, 1), padding=\"same\", name='3dclassifer')(ac_up4)\n",
        "\n",
        "    return ac_up4, x\n",
        "\n",
        "\n",
        "\n",
        "def DenseUNet(img_input, nb_dense_block=4, growth_rate=48, nb_filter=96, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, classes=1000, weights_path=None):\n",
        "    '''Instantiate the DenseNet 161 architecture,\n",
        "        # Arguments\n",
        "            nb_dense_block: number of dense blocks to add to end\n",
        "            growth_rate: number of filters to add per dense block\n",
        "            nb_filter: initial number of filters\n",
        "            reduction: reduction factor of transition blocks.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            classes: optional number of classes to classify images\n",
        "            weights_path: path to pre-trained weights\n",
        "        # Returns\n",
        "            A Keras model instance.\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "    # compute compression factor\n",
        "    compression = 1.0 - reduction\n",
        "\n",
        "    # Handle Dimension Ordering for different backends\n",
        "    global concat_axis\n",
        "    concat_axis = 3\n",
        "\n",
        "    # From architecture for ImageNet (Table 1 in the paper)\n",
        "    nb_filter = 96\n",
        "    nb_layers = [6,12,36,24] # For DenseNet-161\n",
        "    box = []\n",
        "    # Initial convolution\n",
        "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n",
        "    x = Conv2D(nb_filter, (7, 7), strides=(2, 2), name='conv1', use_bias=False, trainable=True)(x)\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name='conv1_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name='conv1_scale')(x)\n",
        "    x = Activation('relu', name='relu1')(x)\n",
        "    box.append(x)\n",
        "    x = ZeroPadding2D((1, 1), name='pool1_zeropadding')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n",
        "\n",
        "    # Add dense blocks\n",
        "    for block_idx in range(int(nb_dense_block) - 1):\n",
        "        stage = block_idx+2\n",
        "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        box.append(x)\n",
        "        # Add transition_block\n",
        "        x = transition_block(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "        nb_filter = int(nb_filter * compression)\n",
        "\n",
        "    final_stage = stage + 1\n",
        "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name='conv'+str(final_stage)+'_blk_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name='conv'+str(final_stage)+'_blk_scale')(x)\n",
        "    x = Activation('relu', name='relu'+str(final_stage)+'_blk')(x)\n",
        "    box.append(x)\n",
        "\n",
        "    up0 = UpSampling2D(size=(2,2))(x)\n",
        "    conv_up0 = Conv2D(768, (3, 3), padding=\"same\", name = \"conv_up0\", trainable=True)(up0)\n",
        "    bn_up0 = BatchNormalization(name = \"bn_up0\", momentum = 1, trainable=False)(conv_up0, training=False)\n",
        "    ac_up0 = Activation('relu', name='ac_up0')(bn_up0)\n",
        "\n",
        "    up1 = UpSampling2D(size=(2,2))(ac_up0)\n",
        "    conv_up1 = Conv2D(384, (3, 3), padding=\"same\", name = \"conv_up1\", trainable=True)(up1)\n",
        "    bn_up1 = BatchNormalization(name = \"bn_up1\", momentum = 1, trainable=False)(conv_up1, training=False)\n",
        "    ac_up1 = Activation('relu', name='ac_up1')(bn_up1)\n",
        "\n",
        "    up2 = UpSampling2D(size=(2,2))(ac_up1)\n",
        "    conv_up2 = Conv2D(96, (3, 3), padding=\"same\", name = \"conv_up2\", trainable=True)(up2)\n",
        "    bn_up2 = BatchNormalization(name = \"bn_up2\", momentum = 1, trainable=False)(conv_up2, training=False)\n",
        "    ac_up2 = Activation('relu', name='ac_up2')(bn_up2)\n",
        "\n",
        "    up3 = UpSampling2D(size=(2,2))(ac_up2)\n",
        "    conv_up3 = Conv2D(96, (3, 3), padding=\"same\", name = \"conv_up3\", trainable=True)(up3)\n",
        "    bn_up3 = BatchNormalization(name = \"bn_up3\", momentum = 1, trainable=False)(conv_up3, training=False)\n",
        "    ac_up3 = Activation('relu', name='ac_up3')(bn_up3)\n",
        "\n",
        "    up4 = UpSampling2D(size=(2, 2))(ac_up3)\n",
        "    conv_up4 = Conv2D(64, (3, 3), padding=\"same\", name=\"conv_up4\", trainable=True)(up4)\n",
        "    bn_up4 = BatchNormalization(name=\"bn_up4\", momentum = 1, trainable=False)(conv_up4, training=False)\n",
        "    ac_up4 = Activation('relu', name='ac_up4')(bn_up4)\n",
        "\n",
        "    x = Conv2D(3, (1,1), padding=\"same\", name='dense167classifer', trainable=True)(ac_up4)\n",
        "\n",
        "    return ac_up4, x\n",
        "\n",
        "def conv_block(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
        "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv2D, 3x3 Conv2D, and option dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            branch: layer index within each dense block\n",
        "            nb_filter: number of filters\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = 'conv' + str(stage) + '_' + str(branch)\n",
        "    relu_name_base = 'relu' + str(stage) + '_' + str(branch)\n",
        "\n",
        "    # 1x1 Convolution (Bottleneck layer)\n",
        "    inter_channel = nb_filter * 4\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name=conv_name_base+'_x1_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_x1_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x1')(x)\n",
        "    x = Conv2D(inter_channel, (1, 1), name=conv_name_base+'_x1', use_bias=False, trainable=True)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # 3x3 Convolution\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name=conv_name_base+'_x2_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_x2_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base+'_x2')(x)\n",
        "    x = ZeroPadding2D((1, 1), name=conv_name_base+'_x2_zeropadding')(x)\n",
        "    x = Conv2D(nb_filter, (3, 3), name=conv_name_base+'_x2', use_bias=False, trainable=True)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def transition_block(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
        "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_filter: number of filters\n",
        "            compression: calculated as 1 - reduction. Reduces the number of feature maps in the transition block.\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    conv_name_base = 'conv' + str(stage) + '_blk'\n",
        "    relu_name_base = 'relu' + str(stage) + '_blk'\n",
        "    pool_name_base = 'pool' + str(stage)\n",
        "\n",
        "    x = BatchNormalization(epsilon=eps, axis=concat_axis, momentum = 1, name=conv_name_base+'_bn', trainable=False)(x, training=False)\n",
        "    x = Scale(axis=concat_axis, name=conv_name_base+'_scale')(x)\n",
        "    x = Activation('relu', name=relu_name_base)(x)\n",
        "    x = Conv2D(int(nb_filter * compression), (1, 1), name=conv_name_base, use_bias=False, trainable=True)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = AveragePooling2D((2, 2), strides=(2, 2), name=pool_name_base)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True):\n",
        "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
        "        # Arguments\n",
        "            x: input tensor\n",
        "            stage: index for dense block\n",
        "            nb_layers: the number of layers of conv_block to append to the model.\n",
        "            nb_filter: number of filters\n",
        "            growth_rate: growth rate\n",
        "            dropout_rate: dropout rate\n",
        "            weight_decay: weight decay factor\n",
        "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
        "    '''\n",
        "\n",
        "    eps = 1.1e-5\n",
        "    concat_feat = x\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        branch = i+1\n",
        "        x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
        "        concat_feat = concatenate([concat_feat, x], axis=concat_axis, name='concat_'+str(stage)+'_'+str(branch))\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    return concat_feat, nb_filter\n",
        "def slice(x, h1, h2):\n",
        "    \"\"\" Define a tensor slice function\n",
        "    \"\"\"\n",
        "    return x[:, :, :, h1:h2,:]\n",
        "def slice2d(x, h1, h2):\n",
        "\n",
        "    tmp = x[h1:h2,:,:,:]\n",
        "    tmp = tf.transpose(tmp, perm=[1, 2, 0, 3])\n",
        "    tmp = tf.expand_dims(tmp, 0)\n",
        "    return tmp\n",
        "\n",
        "def slice_last(x):\n",
        "\n",
        "    x = x[:,:,:,:,0]\n",
        "    return x\n",
        "def trans(x):\n",
        "\n",
        "    x = tf.transpose(x, perm=[0,3,1,2,4])\n",
        "    return x\n",
        "def trans_back(x):\n",
        "\n",
        "    x = tf.transpose(x, perm=[0,2,3,1,4])\n",
        "\n",
        "    return x\n",
        "def dense_rnn_net():\n",
        "\n",
        "    #  ************************3d volume input******************************************************************\n",
        "    img_input = Input(batch_shape=(1, 224, 224,8, 1), name='volumetric_data')\n",
        "\n",
        "    #  ************************(batch*d3cols)*2dvolume--2D DenseNet branch**************************************\n",
        "    input2d = Lambda(slice, arguments={'h1': 0, 'h2': 2})(img_input)\n",
        "    single = Lambda(slice, arguments={'h1':0, 'h2':1})(img_input)\n",
        "    input2d = concatenate([single, input2d], axis=3)\n",
        "    for i in range(8 - 2):\n",
        "        input2d_tmp = Lambda(slice, arguments={'h1': i, 'h2': i + 3})(img_input)\n",
        "        input2d = concatenate([input2d, input2d_tmp], axis=0)\n",
        "        if i == 8 - 3:\n",
        "            final1 = Lambda(slice, arguments={'h1': 8-2, 'h2': 8})(img_input)\n",
        "            final2 = Lambda(slice, arguments={'h1': 8-1, 'h2': 8})(img_input)\n",
        "            final = concatenate([final1, final2], axis=3)\n",
        "            input2d = concatenate([input2d, final], axis=0)\n",
        "    input2d = Lambda(slice_last)(input2d)\n",
        "\n",
        "    #  ******************************stack to 3D volumes *******************************************************\n",
        "    feature2d, classifer2d = DenseUNet(input2d, reduction=0.5)\n",
        "    res2d = Lambda(slice2d, arguments={'h1': 0, 'h2': 1})(classifer2d)\n",
        "    fea2d = Lambda(slice2d, arguments={'h1':0, 'h2':1})(feature2d)\n",
        "    for j in range(8 - 1):\n",
        "        score = Lambda(slice2d, arguments={'h1': j + 1, 'h2': j + 2})(classifer2d)\n",
        "        fea2d_slice = Lambda(slice2d, arguments={'h1': j + 1, 'h2': j + 2})(feature2d)\n",
        "        res2d = concatenate([res2d, score], axis=3)\n",
        "        fea2d = concatenate([fea2d, fea2d_slice], axis=3)\n",
        "\n",
        "    #  *************************** 3d DenseNet on 3D volume (concate with feature map )*********************************\n",
        "    res2d_input = Lambda(lambda x: x * 250)(res2d)\n",
        "    input3d_ori = Lambda(slice, arguments={'h1': 0, 'h2': 8})(img_input)\n",
        "    input3d = concatenate([input3d_ori, res2d_input], axis=4)\n",
        "    feature3d, classifer3d = DenseNet3D(input3d, reduction=0.5)\n",
        "\n",
        "    final = add([feature3d, fea2d])\n",
        "    final_conv = Conv3D(64, (3, 3, 3), padding=\"same\", name='fianl_conv')(final)\n",
        "    final_conv = Dropout(rate=0.3)(final_conv)\n",
        "    final_bn = BatchNormalization(name=\"final_bn\")(final_conv)\n",
        "    final_ac = Activation('relu', name='final_ac')(final_bn)\n",
        "    classifer = Conv3D(3, (1, 1, 1), padding=\"same\", name='2d3dclassifer')(final_ac)\n",
        "\n",
        "    model = Model( inputs = img_input,outputs = classifer, name='auto3d_residual_conv')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def dilated_resnet(args):\n",
        "    inputs = Input(batch_shape = (args.b, args.input_size, args.input_size, args.input_cols, 1))\n",
        "    conv1 = Conv3D(64, (3, 3, 3), padding = \"same\",kernel_initializer=\"normal\")(inputs)\n",
        "    bn0 = BatchNormalization()(conv1)\n",
        "    ac0 = Activation('relu')(bn0)\n",
        "    pool1 = MaxPooling3D(pool_size=(2, 2, 1))(ac0)\n",
        "\n",
        "    #  resudial block\n",
        "    conv2 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool1)\n",
        "    bn1 = BatchNormalization()(conv2)\n",
        "    ac1 = Activation('relu')(bn1)\n",
        "    conv3 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac1)\n",
        "    bn2 = BatchNormalization()(conv3)\n",
        "    pad1 = Conv3D(128, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool1)\n",
        "    BN1 = BatchNormalization()(pad1)\n",
        "    sumb1 = add([BN1, bn2])\n",
        "    res1  = Activation('relu')(sumb1)\n",
        "\n",
        "    pool2 = MaxPooling3D(pool_size=(2, 2, 1))(res1)\n",
        "\n",
        "    #  resudial block\n",
        "    conv4 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool2)\n",
        "    bn3 = BatchNormalization()(conv4)\n",
        "    ac2 = Activation('relu')(bn3)\n",
        "    conv5 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac2)\n",
        "    bn4 = BatchNormalization()(conv5)\n",
        "    pad2 = Conv3D(256, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool2)\n",
        "    BN2 = BatchNormalization()(pad2)\n",
        "    sumb2 = add([BN2, bn4])\n",
        "    res2  = Activation('relu')(sumb2)\n",
        "\n",
        "\n",
        "    pool3 = MaxPooling3D(pool_size=(2, 2, 1))(res2)\n",
        "\n",
        "    #  resudial block\n",
        "    conv6 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool3)\n",
        "    bn5 = BatchNormalization()(conv6)\n",
        "    ac3 = Activation('relu')(bn5)\n",
        "    conv7 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac3)\n",
        "    bn6 = BatchNormalization()(conv7)\n",
        "    pad3 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool3)\n",
        "    BN3 = BatchNormalization()(pad3)\n",
        "    sumb3 = add([BN3, bn6])\n",
        "    res3  = Activation('relu')(sumb3)\n",
        "\n",
        "    #  resudial deliated block\n",
        "    del1 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(res3)\n",
        "    delbn1 = BatchNormalization()(del1)\n",
        "    delac1 = Activation('relu')(delbn1)\n",
        "    del2 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(delac1)\n",
        "    delbn2 = BatchNormalization()(del2)\n",
        "    deladd1 = add([res3, delbn2])\n",
        "    delres  = Activation('relu')(deladd1)\n",
        "\n",
        "    pool4 = MaxPooling3D(pool_size=(2, 2, 1))(delres)\n",
        "\n",
        "    #  resudial block\n",
        "    conv6_4 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(pool4)\n",
        "    bn5_4 = BatchNormalization()(conv6_4)\n",
        "    ac3_4 = Activation('relu')(bn5_4)\n",
        "    conv7_4 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac3_4)\n",
        "    bn6_4 = BatchNormalization()(conv7_4)\n",
        "    pad3_4 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(pool4)\n",
        "    BN3_4 = BatchNormalization()(pad3_4)\n",
        "    sumb3_4 = add([BN3_4, bn6_4])\n",
        "    res3_4  = Activation('relu')(sumb3_4)\n",
        "\n",
        "    #  resudial deliated block\n",
        "    del3 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(res3_4)\n",
        "    delbn3 = BatchNormalization()(del3)\n",
        "    delac3 = Activation('relu')(delbn3)\n",
        "    del4 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(delac3)\n",
        "    delbn4 = BatchNormalization()(del4)\n",
        "    deladd2 = add([res3_4, delbn4])\n",
        "    delres2  = Activation('relu')(deladd2)\n",
        "\n",
        "\n",
        "    up0 = UpSampling3D(size=(2,2,1))(delres2)\n",
        "    pad4 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(delres)\n",
        "    BN4 = BatchNormalization()(pad4)\n",
        "    sumb4 = add([BN4, up0])\n",
        "\n",
        "    #  resudial block\n",
        "    conv8_1 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb4)\n",
        "    bn7_1 = BatchNormalization()(conv8_1)\n",
        "    ac4_1 = Activation('relu')(bn7_1)\n",
        "    conv9_1 = Conv3D(512, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac4_1)\n",
        "    bn8_1 = BatchNormalization()(conv9_1)\n",
        "    pad5_1 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb4)\n",
        "    BN5_1 = BatchNormalization()(pad5_1)\n",
        "    sumb5_1 = add([BN5_1, bn8_1])\n",
        "    res4_1  = Activation('relu')(sumb5_1)\n",
        "\n",
        "    #  resudial deliated block\n",
        "    del5 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(res4_1)\n",
        "    delbn5 = BatchNormalization()(del5)\n",
        "    delac5 = Activation('relu')(delbn5)\n",
        "    del6 = Conv3D(512, (3, 3, 3), padding=\"same\", dilation_rate=(2, 2, 2), kernel_initializer=\"normal\")(delac5)\n",
        "    delbn6 = BatchNormalization()(del6)\n",
        "    deladd3 = add([res4_1, delbn6])\n",
        "    delres3  = Activation('relu')(deladd3)\n",
        "\n",
        "    up0_1 = UpSampling3D(size=(2,2,1))(delres3)\n",
        "    pad4_1 = Conv3D(512, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(res2)\n",
        "    BN4_1 = BatchNormalization()(pad4_1)\n",
        "    sumb4_1 = add([BN4_1, up0_1])\n",
        "\n",
        "    #  resudial block\n",
        "    conv8 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb4_1)\n",
        "    bn7 = BatchNormalization()(conv8)\n",
        "    ac4 = Activation('relu')(bn7)\n",
        "    conv9 = Conv3D(256, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac4)\n",
        "    bn8 = BatchNormalization()(conv9)\n",
        "    pad5 = Conv3D(256, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb4_1)\n",
        "    BN5 = BatchNormalization()(pad5)\n",
        "    sumb5 = add([BN5, bn8])\n",
        "    res4  = Activation('relu')(sumb5)\n",
        "\n",
        "    up1 = UpSampling3D(size=(2, 2, 1))(res4)\n",
        "    pad6 = Conv3D(256, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(res1)\n",
        "    BN6 = BatchNormalization()(pad6)\n",
        "    sumb6 = add([BN6, up1])\n",
        "\n",
        "    #  resudial block\n",
        "    conv10 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb6)\n",
        "    bn9 = BatchNormalization()(conv10)\n",
        "    ac5 = Activation('relu')(bn9)\n",
        "    conv11 = Conv3D(128, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac5)\n",
        "    bn10 = BatchNormalization()(conv11)\n",
        "    pad7 = Conv3D(128, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb6)\n",
        "    BN7 = BatchNormalization()(pad7)\n",
        "    sumb7 = add([BN7, bn10])\n",
        "    res5  = Activation('relu')(sumb7)\n",
        "\n",
        "    up2 = UpSampling3D(size=(2, 2, 1))(res5)\n",
        "    pad8 = Conv3D(128, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(ac0)\n",
        "    BN8 = BatchNormalization()(pad8)\n",
        "    sumb8 = add([BN8, up2])\n",
        "\n",
        "    #  resudial block\n",
        "    conv12 = Conv3D(64, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(sumb8)\n",
        "    bn11= BatchNormalization()(conv12)\n",
        "    ac6 = Activation('relu')(bn11)\n",
        "    conv13 = Conv3D(64, (3, 3, 3), padding=\"same\", kernel_initializer=\"normal\")(ac6)\n",
        "    bn12 = BatchNormalization()(conv13)\n",
        "    pad9 = Conv3D(64, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(sumb8)\n",
        "    BN9 = BatchNormalization()(pad9)\n",
        "    sumb9 = add([BN9, bn12])\n",
        "    res6 = Activation('relu')(sumb9)\n",
        "\n",
        "    output3 = Conv3D(2, (1, 1, 1), padding=\"same\", kernel_initializer=\"normal\")(res6)\n",
        "\n",
        "    # print (output3)\n",
        "\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[output3])\n",
        "\n",
        "\n",
        "\n",
        "    return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQwP3AoVnwLW"
      },
      "source": [
        "\n",
        "# train_hybrid\n",
        "import gc\n",
        "from __future__ import print_function\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "from medpy.io import load\n",
        "import numpy as np\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import keras.backend as K\n",
        "import os\n",
        "import time\n",
        "from skimage.transform import resize\n",
        "import argparse\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# K.set_image_dim_ordering('tf')\n",
        "\n",
        "# #  global parameters\n",
        "# parser = argparse.ArgumentParser(description='Keras DenseUnet Training')\n",
        "# #  data folder\n",
        "# parser.add_argument('-data', type=str, default='data/', help='test images')\n",
        "# parser.add_argument('-save_path', type=str, default='Experiments/')\n",
        "# #  other paras\n",
        "# parser.add_argument('-b', type=int, default=1)\n",
        "# parser.add_argument('-input_size', type=int, default=224)\n",
        "# parser.add_argument('-model_weight', type=str, default='./model/model_best.hdf5')\n",
        "# parser.add_argument('-input_cols', type=int, default=8)\n",
        "# parser.add_argument('-arch', type=str, default='')\n",
        "\n",
        "# #  data augment\n",
        "# parser.add_argument('-mean', type=int, default=48)\n",
        "# args = parser.parse_args()\n",
        "\n",
        "thread_num = 14\n",
        "liverlist = [32,34,38,41,47,87,89,91,105,106,114,115,119]\n",
        "\n",
        "def load_seq_crop_data_masktumor_try(Parameter_List):\n",
        "    img = Parameter_List[0]\n",
        "    tumor = Parameter_List[1]\n",
        "    lines = Parameter_List[2]\n",
        "    numid = Parameter_List[3]\n",
        "    minindex = Parameter_List[4]\n",
        "    maxindex = Parameter_List[5]\n",
        "    #  randomly scale\n",
        "    scale = np.random.uniform(0.8,1.2)\n",
        "    deps = int(224 * scale)\n",
        "    rows = int(224 * scale)\n",
        "    cols = 8\n",
        "\n",
        "    sed = np.random.randint(1,numid)\n",
        "    cen = lines[sed-1]\n",
        "    cen = np.fromstring(cen, dtype=int, sep=' ')\n",
        "    # print (cen)\n",
        "    a = min(max(minindex[0] + deps/2, cen[0]), maxindex[0]- deps/2-1)\n",
        "    b = min(max(minindex[1] + rows/2, cen[1]), maxindex[1]- rows/2-1)\n",
        "    c = min(max(minindex[2] + cols/2, cen[2]), maxindex[2]- cols/2-1)\n",
        "\n",
        "    if a < deps / 2:\n",
        "      a1=int((img.shape[0]/2)-(deps/2))\n",
        "      a2=int((img.shape[0]/2)+(deps/2))\n",
        "    else:\n",
        "      a1=int(a-deps/2)\n",
        "      a2=int(a+deps/2)\n",
        "    if b < rows / 2:\n",
        "      b1=int((img.shape[0]/2)-(rows/2))\n",
        "      b2=int((img.shape[0]/2)+(rows/2))\n",
        "    else:\n",
        "      b1=int(b-rows/2)\n",
        "      b2=int(b+rows/2)\n",
        "    if c < cols / 2:\n",
        "      c1=int((img.shape[0]/2)-(cols/2))\n",
        "      c2=int((img.shape[0]/2)+(cols/2))\n",
        "    else:\n",
        "      c1=int(c-cols/2)\n",
        "      c2=int(c+cols/2)\n",
        "\n",
        "    cropp_img = img[a1:a2, b1:b2,c1: c2+1].copy()\n",
        "\n",
        "    cropp_tumor = tumor[a1:a2, b1:b2,c1: c2+1].copy()\n",
        "    \n",
        "\n",
        "    cropp_img -= 48\n",
        "    cropp_tumor = resize(cropp_tumor, (224,224,8), order=0, mode='edge', cval=0, clip=True, preserve_range=True)\n",
        "    cropp_img   = resize(cropp_img, (224,224,8), order=3, mode='constant', cval=0, clip=True, preserve_range=True)\n",
        "    return cropp_img, cropp_tumor\n",
        "   \n",
        "\n",
        "\n",
        "def generate_arrays_from_file(batch_size, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list):\n",
        "    while 1:\n",
        "        X = np.zeros((batch_size, 224, 224,8,1), dtype='float32')\n",
        "        Y = np.zeros((batch_size, 224,224, 8,1), dtype='int16')\n",
        "        Parameter_List = []\n",
        "        for idx in range(batch_size):\n",
        "            count = np.random.choice(trainidx)\n",
        "            img = img_list[count]\n",
        "            tumor = tumor_list[count]\n",
        "            minindex = minindex_list[count]\n",
        "            maxindex = maxindex_list[count]\n",
        "            num = np.random.randint(0,6)\n",
        "            if num < 3 or (count in liverlist):\n",
        "                lines = liverlines[count]\n",
        "                numid = liveridx[count]\n",
        "            else:\n",
        "                lines = tumorlines[count]\n",
        "                numid = tumoridx[count]\n",
        "            Parameter_List.append([img, tumor, lines, numid, minindex, maxindex])\n",
        "                \n",
        "        pool = ThreadPool(thread_num)\n",
        "        result_list = pool.map(load_seq_crop_data_masktumor_try, Parameter_List)\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "\n",
        "\n",
        "        for idx in range(len(result_list)):\n",
        "            X[idx, :, :, :, 0] = result_list[idx][0]\n",
        "            Y[idx, :, :, :, 0] = result_list[idx][1]\n",
        "        # if np.sum(Y==0)==0:\n",
        "        #     print(np.sum(Y==0)==0)\n",
        "        #     continue\n",
        "        # if np.sum(Y==1)==0:\n",
        "        #     print(np.sum(Y==1)==0)\n",
        "        #     continue\n",
        "        # if np.sum(Y==2)==0:\n",
        "        #     print(np.sum(Y==2)==0)\n",
        "        #     continue\n",
        "        yield (X,Y)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdSsONK-1Qbz",
        "outputId": "e87c1573-1382-43cd-b9c7-5d650718c2c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "    print('-'*30)\n",
        "    print('Creating and compiling model...')\n",
        "    print('-'*30)\n",
        "\n",
        "    model = dense_rnn_net()\n",
        "    model_path = \"hybrid_model\"\n",
        "    model.load_weights('/content/gdrive/MyDrive/model_best.hdf5')\n",
        "    \n",
        "\n",
        "    model1 = dense_rnn_net()\n",
        "    sgd1 = SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
        "    model1.compile(optimizer=sgd1, loss=[weighted_crossentropy])\n",
        "    model1.load_weights('/content/gdrive/MyDrive/model_best.hdf5')\n",
        "\n",
        "    model2 = dense_rnn_net()\n",
        "    sgd2 = SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
        "    model2.compile(optimizer=sgd2, loss=[weighted_crossentropy])\n",
        "    model2.load_weights('/content/gdrive/MyDrive/model_best.hdf5')\n",
        "    models=[model1,model2]"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Creating and compiling model...\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5hXdYKYzfl0",
        "outputId": "95e8e7e3-5c0e-49d1-e908-849ba9f02b39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "    #  liver tumor LITS\n",
        "    trainidx_1 =list(range(5))\n",
        "        \n",
        "    trainidx_2 =list(range(5))\n",
        "    \n",
        "    img_list_1 = []\n",
        "    tumor_list_1 = []\n",
        "    minindex_list_1 = []\n",
        "    maxindex_list_1 = []\n",
        "    tumorlines_1 = []\n",
        "    tumoridx_1 = []\n",
        "    liveridx_1 = []\n",
        "    liverlines_1 = []\n",
        "    for idx in list(range(5)):\n",
        "        img, img_header = load('/content/data/myTrainingData/volume-' + str(idx) + '.nii' )\n",
        "        tumor, tumor_header = load('/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/segmentation-' + str(idx) + '.nii')\n",
        "        img_list_1.append(img)\n",
        "        tumor_list_1.append(tumor)\n",
        "\n",
        "        maxmin = np.loadtxt('/content/data/myTrainingDataTxt/LiverBox/box_' + str(idx) + '.txt', delimiter=' ')\n",
        "        minindex = maxmin[0:3]\n",
        "        maxindex = maxmin[3:6]\n",
        "        minindex = np.array(minindex, dtype='int')\n",
        "        maxindex = np.array(maxindex, dtype='int')\n",
        "        minindex[0] = max(minindex[0]-3, 0)\n",
        "        minindex[1] = max(minindex[1]-3, 0)\n",
        "        minindex[2] = max(minindex[2]-3, 0)\n",
        "        maxindex[0] = min(img.shape[0], maxindex[0]+3)\n",
        "        maxindex[1] = min(img.shape[1], maxindex[1]+3)\n",
        "        maxindex[2] = min(img.shape[2], maxindex[2]+3)\n",
        "        minindex_list_1.append(minindex)\n",
        "        maxindex_list_1.append(maxindex)\n",
        "\n",
        "        f1 = open('/content/data/myTrainingDataTxt/TumorPixels/tumor_' + str(idx) + '.txt','r')\n",
        "        tumorline = f1.readlines()\n",
        "        tumorlines_1.append(tumorline)\n",
        "        tumoridx_1.append(len(tumorline))\n",
        "        f1.close()\n",
        "\n",
        "        f2 = open('/content/data/myTrainingDataTxt/LiverPixels/liver_' + str(idx) + '.txt','r')\n",
        "        liverline = f2.readlines()\n",
        "        liverlines_1.append(liverline)\n",
        "        liveridx_1.append(len(liverline))\n",
        "        f2.close()\n",
        "\n",
        "\n",
        "    img_list_2 = []\n",
        "    tumor_list_2 = []\n",
        "    minindex_list_2 = []\n",
        "    maxindex_list_2 = []\n",
        "    tumorlines_2 = []\n",
        "    tumoridx_2 = []\n",
        "    liveridx_2 = []\n",
        "    liverlines_2 = []\n",
        "    for idx in list(range(5,10)):\n",
        "        img, img_header = load('/content/data/myTrainingData/volume-' + str(idx) + '.nii' )\n",
        "        tumor, tumor_header = load('/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/segmentation-' + str(idx) + '.nii')\n",
        "        img_list_2.append(img)\n",
        "        tumor_list_2.append(tumor)\n",
        "\n",
        "        maxmin = np.loadtxt('/content/data/myTrainingDataTxt/LiverBox/box_' + str(idx) + '.txt', delimiter=' ')\n",
        "        minindex = maxmin[0:3]\n",
        "        maxindex = maxmin[3:6]\n",
        "        minindex = np.array(minindex, dtype='int')\n",
        "        maxindex = np.array(maxindex, dtype='int')\n",
        "        minindex[0] = max(minindex[0]-3, 0)\n",
        "        minindex[1] = max(minindex[1]-3, 0)\n",
        "        minindex[2] = max(minindex[2]-3, 0)\n",
        "        maxindex[0] = min(img.shape[0], maxindex[0]+3)\n",
        "        maxindex[1] = min(img.shape[1], maxindex[1]+3)\n",
        "        maxindex[2] = min(img.shape[2], maxindex[2]+3)\n",
        "        minindex_list_2.append(minindex)\n",
        "        maxindex_list_2.append(maxindex)\n",
        "\n",
        "        f1 = open('/content/data/myTrainingDataTxt/TumorPixels/tumor_' + str(idx) + '.txt','r')\n",
        "        tumorline = f1.readlines()\n",
        "        tumorlines_2.append(tumorline)\n",
        "        tumoridx_2.append(len(tumorline))\n",
        "        f1.close()\n",
        "\n",
        "        f2 = open('/content/data/myTrainingDataTxt/LiverPixels/liver_' + str(idx) + '.txt','r')\n",
        "        liverline = f2.readlines()\n",
        "        liverlines_2.append(liverline)\n",
        "        liveridx_2.append(len(liverline))\n",
        "        f2.close()\n",
        "\n",
        "    "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Creating and compiling model...\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3XeRWrpzhpc",
        "outputId": "fb7d8190-2ba5-494f-fe8b-eaed1d7dfb16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "    save_path='Experiments'\n",
        "    if not os.path.exists(save_path +model_path):\n",
        "        os.mkdir(save_path + model_path)\n",
        "    if not os.path.exists(save_path + \"/history\"):\n",
        "        os.mkdir(save_path + '/history')\n",
        "    else:\n",
        "        if os.path.exists(save_path + \"/history/lossbatch.txt\"):\n",
        "            os.remove(save_path + '/history/lossbatch.txt')\n",
        "        if os.path.exists(save_path + \"/history/lossepoch.txt\"):\n",
        "            os.remove(save_path + '/history/lossepoch.txt')\n",
        "    print('-'*30)\n",
        "    print('Fitting model......')\n",
        "    print('-'*30)\n",
        "    steps = 5\n",
        "\n",
        "    trainidxs =[trainidx_1,trainidx_2]\n",
        "    img_lists = [img_list_1,img_list_2]\n",
        "    tumor_lists = [tumor_list_1,tumor_list_2]\n",
        "    minindex_lists = [minindex_list_1,minindex_list_2]\n",
        "    maxindex_lists = [maxindex_list_1,maxindex_list_2]\n",
        "    tumorliness = [tumorlines_1,tumorlines_2]\n",
        "    tumoridxs = [tumoridx_1,tumoridx_2]\n",
        "    liveridxs = [liveridx_1,liveridx_2]\n",
        "    liverliness = [liverlines_1,liverlines_2]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Fitting model......\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj_4wxWYziYk",
        "outputId": "1bb01dd5-2ee3-4f88-85c6-9fe709df8cfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "    for epoch in range(100):\n",
        "      for i in range(2):\n",
        "        \n",
        "        trainidx=trainidxs[i]\n",
        "        img_list=img_lists[i]\n",
        "        tumor_list=tumor_lists[i]\n",
        "        minindex_list=minindex_lists[i]\n",
        "        maxindex_list=maxindex_lists[i]\n",
        "        tumorlines=tumorliness[i]\n",
        "        tumoridx=tumoridxs[i]\n",
        "        liveridx=liveridxs[i]\n",
        "        liverlines=liverliness[i]\n",
        "        \n",
        "        models[i].fit_generator(generate_arrays_from_file(1, trainidx, img_list, tumor_list, tumorlines, liverlines,\n",
        "                                                  tumoridx, liveridx, minindex_list, maxindex_list),\n",
        "                        steps_per_epoch=10,\n",
        "                        epochs= 1, verbose = 1, use_multiprocessing=False)\n",
        "      \n",
        "      for num in tqdm(range(len(model.layers))):\n",
        "          wts=model.layers[num].get_weights()\n",
        "          if len(wts)!=0:\n",
        "            ls=[]\n",
        "            for i in range(len(wts)):\n",
        "              temp = np.zeros(wts[i].shape)\n",
        "              for s in range(2):\n",
        "                weights=models[s].layers[num].get_weights()[i]\n",
        "                noise = np.random.normal(0, 1e-15*np.std(weights),weights.shape).astype('float32')\n",
        "                temp += 0.5*(weights+noise)\n",
        "              # print(np.mean(temp-weights))\n",
        "              ls.append(tf.convert_to_tensor(temp)._copy_to_device('/device:GPU:0'))\n",
        "            model.layers[num].set_weights(ls)\n",
        "            for s in range(2):\n",
        "              models[s].layers[num].set_weights(ls)\n",
        "      del([wts,temp,weights,noise,ls])\n",
        "      gc.collect()\n",
        "    print ('Finised Training .......')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1915: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 82s 671ms/step - loss: 0.0397\n",
            "10/10 [==============================] - 81s 670ms/step - loss: 0.0169\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.93it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0378\n",
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0209\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.87it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0475\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0170\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.49it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0351\n",
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0399\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 60.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0247\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0148\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0543\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0147\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.65it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0353\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.73it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0424\n",
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0189\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0264\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0173\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0514\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0169\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.59it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 672ms/step - loss: 0.0540\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0166\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 60.54it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0309\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 60.74it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0473\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0163\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.92it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0460\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0355\n",
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.92it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0339\n",
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0194\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0311\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0169\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.49it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0277\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.64it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0332\n",
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0128\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:20<00:00, 59.63it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0421\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.72it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0362\n",
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0152\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0359\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.14it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0367\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0170\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:20<00:00, 58.75it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0374\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:20<00:00, 58.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0392\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.05it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0415\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 60.38it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0388\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 672ms/step - loss: 0.0527\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.81it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0394\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0148\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.38it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0374\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.28it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0297\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0144\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.48it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 672ms/step - loss: 0.0296\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0383\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.34it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0462\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.69it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0249\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0180\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 60.97it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 672ms/step - loss: 0.0307\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0153\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.29it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0355\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 60.33it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0420\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0447\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0129\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 60.96it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 672ms/step - loss: 0.0465\n",
            "10/10 [==============================] - 7s 672ms/step - loss: 0.0143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.17it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0369\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.75it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0268\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.84it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0293\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0170\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.21it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 673ms/step - loss: 0.0260\n",
            "10/10 [==============================] - 7s 673ms/step - loss: 0.0144\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.88it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0363\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.13it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0370\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0163\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.09it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0319\n",
            "10/10 [==============================] - 7s 672ms/step - loss: 0.0161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.13it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 672ms/step - loss: 0.0473\n",
            "10/10 [==============================] - 7s 673ms/step - loss: 0.0167\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0353\n",
            "10/10 [==============================] - 7s 671ms/step - loss: 0.0156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 62.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0375\n",
            "10/10 [==============================] - 7s 670ms/step - loss: 0.0182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1201/1201 [00:19<00:00, 61.81it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0354\n",
            "10/10 [==============================] - 7s 669ms/step - loss: 0.0134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD7rg8Kg09j-",
        "outputId": "275dcdc2-1c1d-4adf-f7d2-2db860a94c4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "del([model,models])\n",
        "gc.collect()"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "203604"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    }
  ]
}