{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepare.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/gan_segmentation_FE/blob/main/prepare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ab3764-5f6c-4a7d-be64-79f62a2f29d4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/gdrive/My Drive/segmentation/Training_Batch1.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOdQm2sOWzCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa184980-2eae-4262-95cb-27358924e966"
      },
      "source": [
        "pip install medpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting medpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/70/c1fd5dd60242eee81774696ea7ba4caafac2bad8f028bba94b1af83777d7/MedPy-0.4.0.tar.gz (151kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 21.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 23.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 51kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 71kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 81kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 92kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 102kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 112kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 122kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 133kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 143kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from medpy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from medpy) (1.19.5)\n",
            "Collecting SimpleITK>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/85/6a7ce61f07cdaca722dd64f028b5678fb0a9e1bf66f534c2f8dd2eb78490/SimpleITK-2.0.2-cp36-cp36m-manylinux2010_x86_64.whl (47.4MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4MB 67kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: medpy\n",
            "  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for medpy: filename=MedPy-0.4.0-cp36-cp36m-linux_x86_64.whl size=753439 sha256=0d5be5ca9022330813f215866fe2e4ed3dc12790cff6cb7e1f2fc62ecf5ef876\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/c9/9c/2c6281c7a72b9fb1570862a4f028af7ce38405008354fbf870\n",
            "Successfully built medpy\n",
            "Installing collected packages: SimpleITK, medpy\n",
            "Successfully installed SimpleITK-2.0.2 medpy-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG5mEofcR9TZ",
        "outputId": "d89a1249-9e05-47fb-fc3a-82bfce8de43a"
      },
      "source": [
        "pip install tensorflow==1.13.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 33kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 51.0MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 43.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (53.0.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.0)\n",
            "Installing collected packages: mock, tensorflow-estimator, keras-applications, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f__ixCs8SB54",
        "outputId": "81fd6d19-1b8c-4ac2-8cf3-504804514857"
      },
      "source": [
        "pip install keras==2.2.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\r\u001b[K     |█                               | 10kB 17.2MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 20.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 11.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 112kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 133kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 163kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 184kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 225kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 245kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 256kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 266kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 276kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 296kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlagQvogSDMa",
        "outputId": "261a4cc2-cefd-4d1f-bf0a-3aa6b430376b"
      },
      "source": [
        "!git clone https://github.com/xmengli999/H-DenseUNet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'H-DenseUNet'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 706 (delta 5), reused 0 (delta 0), pack-reused 697\u001b[K\n",
            "Receiving objects: 100% (706/706), 13.77 MiB | 17.56 MiB/s, done.\n",
            "Resolving deltas: 100% (241/241), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yPwq4MHSEjW",
        "outputId": "75a14357-01ca-44f0-a4ec-dc8e0d8aad07"
      },
      "source": [
        "from medpy.io import load, save\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def proprecessing(image_path, save_folder):\n",
        "\n",
        "    if not os.path.exists(\"data/\"+save_folder):\n",
        "        os.mkdir(\"data/\"+save_folder)\n",
        "    filelist = os.listdir(image_path)\n",
        "    filelist = [item for item in filelist if 'volume' in item]\n",
        "    for file in filelist:\n",
        "        img, img_header = load(image_path+file)\n",
        "        img[img < -200] = -200\n",
        "        img[img > 250] = 250\n",
        "        img = np.array(img, dtype='float32')\n",
        "        print (\"Saving image \"+file)\n",
        "        save(img, \"./data/\" + save_folder + file)\n",
        "\n",
        "def generate_livertxt(image_path, save_folder):\n",
        "    if not os.path.exists(\"data/\"+save_folder):\n",
        "        os.mkdir(\"data/\"+save_folder)\n",
        "\n",
        "    # Generate Livertxt\n",
        "    if not os.path.exists(\"data/\"+save_folder+'LiverPixels'):\n",
        "        os.mkdir(\"data/\"+save_folder+'LiverPixels')\n",
        "\n",
        "\n",
        "    #change\n",
        "    #131 to 5\n",
        "\n",
        "    for i in range(0,131):\n",
        "        livertumor, header = load(image_path+'segmentation-'+str(i)+'.nii')\n",
        "        f = open('data/' +save_folder+'/LiverPixels/liver_' + str(i) + '.txt', 'w')\n",
        "        index = np.where(livertumor==1)\n",
        "        x = index[0]\n",
        "        y = index[1]\n",
        "        z = index[2]\n",
        "        np.savetxt(f, np.c_[x,y,z], fmt=\"%d\")\n",
        "\t\n",
        "        f.write(\"\\n\")\n",
        "        f.close()\n",
        "\n",
        "def generate_tumortxt(image_path, save_folder):\n",
        "    if not os.path.exists(\"data/\"+save_folder):\n",
        "        os.mkdir(\"data/\"+save_folder)\n",
        "\n",
        "    # Generate Livertxt\n",
        "    if not os.path.exists(\"data/\"+save_folder+'TumorPixels'):\n",
        "        os.mkdir(\"data/\"+save_folder+'TumorPixels')\n",
        "\n",
        "    for i in range(0,131):\n",
        "        livertumor, header = load(image_path+'segmentation-'+str(i)+'.nii')\n",
        "        f = open(\"data/\"+save_folder+\"/TumorPixels/tumor_\"+str(i)+'.txt','w')\n",
        "        index = np.where(livertumor==2)\n",
        "\n",
        "        x = index[0]\n",
        "        y = index[1]\n",
        "        z = index[2]\n",
        "\n",
        "        np.savetxt(f,np.c_[x,y,z],fmt=\"%d\")\n",
        "\n",
        "        f.write(\"\\n\")\n",
        "        f.close()\n",
        "\n",
        "def generate_txt(image_path, save_folder):\n",
        "    if not os.path.exists(\"data/\"+save_folder):\n",
        "        os.mkdir(\"data/\"+save_folder)\n",
        "\n",
        "    # Generate Livertxt\n",
        "    if not os.path.exists(\"data/\"+save_folder+'LiverBox'):\n",
        "        os.mkdir(\"data/\"+save_folder+'LiverBox')\n",
        "    for i in range(0,131):\n",
        "        values = np.loadtxt('data/myTrainingDataTxt/LiverPixels/liver_' + str(i) + '.txt', delimiter=' ', usecols=[0, 1, 2])\n",
        "        a = np.min(values, axis=0)\n",
        "        b = np.max(values, axis=0)\n",
        "        box = np.append(a,b, axis=0)\n",
        "        np.savetxt('data/myTrainingDataTxt/LiverBox/box_'+str(i)+'.txt', box,fmt='%d')\n",
        "\n",
        "#change\n",
        "#create data\n",
        "#change training data path\n",
        "\n",
        "proprecessing(image_path='/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/', save_folder='myTrainingData/')\n",
        "\n",
        "\n",
        "#commenting test\n",
        "# proprecessing(image_path='data/TestData/', save_folder='myTestData/')\n",
        "\n",
        "\n",
        "print (\"Generate liver txt \")\n",
        "generate_livertxt(image_path='/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/', save_folder='myTrainingDataTxt/')\n",
        "print (\"Generate tumor txt\")\n",
        "generate_tumortxt(image_path='/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/', save_folder='myTrainingDataTxt/')\n",
        "print (\"Generate liver box \")\n",
        "generate_txt(image_path='data/TrainingData/', save_folder='myTrainingDataTxt/')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving image volume-4.nii\n",
            "Saving image volume-0.nii\n",
            "Saving image volume-1.nii\n",
            "Saving image volume-3.nii\n",
            "Saving image volume-2.nii\n",
            "Generate liver txt \n",
            "Generate tumor txt\n",
            "Generate liver box \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaIgvh1fS2I8",
        "outputId": "d131c8e6-5ecb-4175-c25a-1784798de48c"
      },
      "source": [
        "#change\n",
        "#keras -2.0.8\n",
        "\n",
        "from __future__ import print_function\n",
        "# import sys\n",
        "# sys.path.insert(0,'Keras-2.0.8')\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "import random\n",
        "from medpy.io import load\n",
        "import numpy as np\n",
        "import argparse\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import keras.backend as K\n",
        "# from loss import weighted_crossentropy_2ddense\n",
        "import os\n",
        "# from keras.utils2.multi_gpu import make_parallel\n",
        "# from denseunet import DenseUNet\n",
        "from skimage.transform import resize\n",
        "K.set_image_dim_ordering('tf')\n",
        "\n",
        "#  global parameters\n",
        "# parser = argparse.ArgumentParser(description='Keras 2d denseunet Training')\n",
        "# #  data folder\n",
        "# parser.add_argument('-data', type=str, default='data/', help='test images')\n",
        "# parser.add_argument('-save_path', type=str, default='Experiments/')\n",
        "# #  other paras\n",
        "# parser.add_argument('-b', type=int, default=40)\n",
        "# parser.add_argument('-input_size', type=int, default=224)\n",
        "# parser.add_argument('-model_weight', type=str, default='./model/densenet161_weights_tf.h5')\n",
        "# parser.add_argument('-input_cols', type=int, default=3)\n",
        "\n",
        "#  data augment\n",
        "# parser.add_argument('-mean', type=int, default=48)\n",
        "# parser.add_argument('-thread_num', type=int, default=14)\n",
        "# args = parser.parse_args()\n",
        "\n",
        "MEAN = 48\n",
        "thread_num = 14\n",
        "input_size=224\n",
        "b=1\n",
        "input_cols=3\n",
        "\n",
        "liverlist = [32,34,38,41,47,87,89,91,105,106,114,115,119]\n",
        "trainidx = list(range(5))\n",
        "img_list = []\n",
        "tumor_list = []\n",
        "minindex_list = []\n",
        "maxindex_list = []\n",
        "tumorlines = []\n",
        "tumoridx = []\n",
        "liveridx = []\n",
        "liverlines = []\n",
        "\n",
        "for idx in range(196):\n",
        "        img, img_header = load('/content/data/myTrainingData/volume-'+str(idx)+'.nii')\n",
        "        tumor, tumor_header = load('/content/media/nas/01_Datasets/CT/LITS/Training Batch 1/segmentation-'+str(idx)+'.nii')\n",
        "        img_list.append(img)\n",
        "        tumor_list.append(tumor)\n",
        "\n",
        "        maxmin = np.loadtxt('/content/data/myTrainingDataTxt/LiverBox/box_'+str(idx)+'.txt')\n",
        "        minindex = maxmin[0:3]\n",
        "        maxindex = maxmin[3:6]\n",
        "        minindex = np.array(minindex, dtype='int')\n",
        "        maxindex = np.array(maxindex, dtype='int')\n",
        "        minindex[0] = max(minindex[0] - 3, 0)\n",
        "        minindex[1] = max(minindex[1] - 3, 0)\n",
        "        minindex[2] = max(minindex[2] - 3, 0)\n",
        "        maxindex[0] = min(img.shape[0], maxindex[0] + 3)\n",
        "        maxindex[1] = min(img.shape[1], maxindex[1] + 3)\n",
        "        maxindex[2] = min(img.shape[2], maxindex[2] + 3)\n",
        "        minindex_list.append(minindex)\n",
        "        maxindex_list.append(maxindex)\n",
        "        f1 = open('/content/data/myTrainingDataTxt/TumorPixels/tumor_'+str(idx)+'.txt')\n",
        "        tumorline = f1.readlines()\n",
        "        tumorlines.append(tumorline)\n",
        "        tumoridx.append(len(tumorline))\n",
        "        f1.close()\n",
        "        f2 = open('/content/data/myTrainingDataTxt/LiverPixels/liver_'+str(idx)+'.txt')\n",
        "        liverline = f2.readlines()\n",
        "        liverlines.append(liverline)\n",
        "        liveridx.append(len(liverline))\n",
        "        f2.close()\n",
        "        np.save('/content/gdrive/MyDrive/'+str(idx)+'.npy',[trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Creating and compiling model...\n",
            "------------------------------\n",
            "------------------------------\n",
            "Fitting model......\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "5/5 [==============================] - 136s 27s/step - loss: 1.8731\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 22s 4s/step - loss: 1.4738\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 22s 4s/step - loss: 0.9403\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 22s 4s/step - loss: 0.7511\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 23s 5s/step - loss: 1.0408\n",
            "Finised Training .......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNbRV15OV3df"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}